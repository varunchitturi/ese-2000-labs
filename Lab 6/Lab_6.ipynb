{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:38.593992Z",
     "start_time": "2024-05-31T06:51:38.458541Z"
    }
   },
   "source": [
    "!git clone https://github.com/Damowerko/ese2000-dynamical-systems.git\n",
    "import sys\n",
    "sys.path.append('./ese2000-dynamical-systems/')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ese2000-dynamical-systems' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:39.072116Z",
     "start_time": "2024-05-31T06:51:39.068260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.style\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.init as init\n",
    "\n",
    "from ese2000_dynamical.config import Config\n",
    "from ese2000_dynamical.track import load_track, Track\n",
    "from ese2000_dynamical.simulator import Simulator, dynamics_ca\n",
    "\n",
    "# matplotlib settings\n",
    "matplotlib.style.use(\"seaborn-v0_8-colorblind\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "data_path = Path(\"./ese2000-dynamical-systems/data\")\n",
    "figure_path = Path(\"figures\")\n",
    "\n",
    "device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\""
   ],
   "id": "b4e2e105454fcdc0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:39.470861Z",
     "start_time": "2024-05-31T06:51:39.468012Z"
    }
   },
   "cell_type": "code",
   "source": "sim = Simulator()",
   "id": "ba064b76a1113aaf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:39.843509Z",
     "start_time": "2024-05-31T06:51:39.838608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_expert = np.load(data_path / \"states.npy\")\n",
    "u_expert = np.load(data_path / \"inputs.npy\")\n",
    "track = load_track(data_path / \"track.npz\")\n",
    "\n",
    "# Choose a trajectory\n",
    "x_trajectory = torch.tensor(x_expert[5]).float().to(device)\n",
    "p = x_trajectory[:, :2]\n",
    "v = x_trajectory[:, 2:]\n",
    "a = torch.tensor(u_expert[5]).float().to(device)\n",
    "\n",
    "# Load the pre-trained model from Lab 5A\n",
    "# We've given you a parameterization but you can also save the model from your previous lab and load it here.\n",
    "A = torch.Tensor(np.load(\"./weights/A.npy\")).to(device)\n",
    "B = torch.Tensor(np.load(\"./weights/B.npy\")).to(device)\n",
    "timesteps = x_trajectory.shape[0]"
   ],
   "id": "122604658c56bd8e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:40.255544Z",
     "start_time": "2024-05-31T06:51:40.252147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_vs_expert(x, x_label: str, x_expert, track: Track):\n",
    "    \"\"\"\n",
    "    Plot a given trajectory and expert trajectory on the same plot.\n",
    "\n",
    "    Args:\n",
    "        x: The trajectory to plot.\n",
    "        x_label: The label for the trajectory.\n",
    "        x_expert: The expert trajectory.\n",
    "        track: The track to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    track.plot()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"x (m)\")\n",
    "    plt.ylabel(\"y (m)\")\n",
    "    plt.plot(x_expert[:, 0], x_expert[:, 1], \"--\", label=\"Expert\")\n",
    "    plt.plot(x[:, 0], x[:, 1], \"-\", label=x_label)\n",
    "    plt.legend(loc=\"upper right\", framealpha=1.0)"
   ],
   "id": "a25673509029fe4b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:40.757192Z",
     "start_time": "2024-05-31T06:51:40.754880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reward_function(state, target):\n",
    "    return torch.sum(- 0.5 * (state - target) ** 2)"
   ],
   "id": "d7589b108b94a81a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:41.176170Z",
     "start_time": "2024-05-31T06:51:41.172681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rollout(A, B, x_trajectory, model_policy, use_simulator=False):\n",
    "    predicted_trajectory = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1, timesteps):\n",
    "        # using x_trajectory[0] compute trajectory using policy\n",
    "        # compute q for each state, acceleration pair\n",
    "        # (subtract from real q estimation)**2 * 0.5\n",
    "        # sum all to get loss\n",
    "        if t == 1:\n",
    "            state = x_trajectory[0]\n",
    "        else:\n",
    "            state = predicted_trajectory[-1]\n",
    "        action = model_policy(state)\n",
    "        actions.append(action)\n",
    "        if use_simulator:\n",
    "            next_state = sim.step(state.detach(), action.detach()).float()\n",
    "        else:\n",
    "            next_state = A @ state + B @ action\n",
    "        predicted_trajectory.append(next_state)\n",
    "        rewards.append(reward_function(predicted_trajectory[-1], x_trajectory[-1]))\n",
    "        total_reward += rewards[-1]\n",
    "    predicted_trajectory = torch.stack(predicted_trajectory)\n",
    "    actions = torch.stack(actions)\n",
    "    rewards = torch.stack(rewards)\n",
    "    \n",
    "\n",
    "    return predicted_trajectory, actions, rewards"
   ],
   "id": "49aac30aec6df39c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:41.895593Z",
     "start_time": "2024-05-31T06:51:41.891260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QHat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QHat, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        init.uniform_(self.fc1.weight, a=-0.5, b=0.5)\n",
    "        init.uniform_(self.fc2.weight, a=-0.5, b=0.5)\n",
    "        init.uniform_(self.fc3.weight, a=-0.5, b=0.5)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value"
   ],
   "id": "3579470f027d63ca",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:51:42.183970Z",
     "start_time": "2024-05-31T06:51:42.179788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        init.uniform_(self.fc1.weight, a=-0.05, b=0.05)\n",
    "        init.uniform_(self.fc2.weight, a=-0.05, b=0.05)\n",
    "        init.uniform_(self.fc3.weight, a=-0.05, b=0.05)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.tanh(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = 5 * F.tanh(self.fc3(x))\n",
    "        return x\n"
   ],
   "id": "4d59f99dd5a604c7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T14:16:15.815547Z",
     "start_time": "2024-05-31T14:16:15.810282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_Q = QHat().to(device)\n",
    "optimizer_Q = torch.optim.Adam(model_Q.parameters(), lr=1e-3)\n",
    "model_policy = Policy().to(device)\n",
    "optimizer_policy = torch.optim.Adam(model_policy.parameters(), lr=0.001)"
   ],
   "id": "256e123efe063df0",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-31T14:16:16.140168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_printoptions(precision=3)\n",
    "epochs = 100\n",
    "gamma = 0.5\n",
    "q_train_iterations = 500\n",
    "policy_train_iterations = 500\n",
    "compute_Q_loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1) Sample trajectory\n",
    "    model_policy.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_trajectory, actions, rewards = rollout(A, B, x_trajectory, model_policy)\n",
    "        total_reward = torch.sum(rewards)\n",
    "\n",
    "    # 2) Update Q model until residual is small\n",
    "    for i in range(q_train_iterations):\n",
    "\n",
    "        optimizer_Q.zero_grad()\n",
    "\n",
    "        Q_hat = model_Q(predicted_trajectory, actions).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q = rewards + gamma * Q_hat\n",
    "\n",
    "        loss_Q = compute_Q_loss(Q, Q_hat)\n",
    "        loss_Q.backward()\n",
    "        \n",
    "        optimizer_Q.step()\n",
    "\n",
    "    model_Q.eval()\n",
    "    model_policy.train()\n",
    "    for i in range(policy_train_iterations):\n",
    "        optimizer_policy.zero_grad()\n",
    "\n",
    "        predicted_trajectory, actions, rewards = rollout(A, B, x_trajectory, model_policy)\n",
    "\n",
    "        Q = model_Q(predicted_trajectory, actions)\n",
    "\n",
    "        loss_policy = -1 * torch.sum(Q)\n",
    "        loss_policy.backward()\n",
    "        optimizer_policy.step()\n",
    "\n",
    "    print(f'Iteration {epoch+1}\\t Total Reward: {total_reward}\\t Policy Loss: {loss_policy}\\t Q Loss: {loss_Q}')"
   ],
   "id": "d3dd09caae898510",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\t Total Reward: -135859.59375\t Policy Loss: -7891.02685546875\t Q Loss: 616133.125\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model_policy.eval()\n",
    "predicted_trajectory, actions, rewards = rollout(None, None, x_trajectory, model_policy, use_simulator=True)\n",
    "\n",
    "print(\"Total Reward: \", torch.sum(rewards))"
   ],
   "id": "b0c458955e7c50e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "plot_vs_expert(predicted_trajectory.detach().cpu().numpy(), 'RL', x_trajectory.detach().cpu().numpy() , track)",
   "id": "3eb26749d92e6054",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4dcbb3b5966f0930"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
