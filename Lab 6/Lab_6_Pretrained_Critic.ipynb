{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lab 6: Pretrained Critic",
   "id": "3c0734f589520705"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Starter Code",
   "id": "a717d0fd7315a35f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "!git clone https://github.com/Damowerko/ese2000-dynamical-systems.git\n",
    "import sys\n",
    "if sys.path[-1] != \"./ese2000-dynamical-systems/\":\n",
    "    sys.path.append('./ese2000-dynamical-systems/')\n",
    "from ese2000_dynamical.config import Config\n",
    "from ese2000_dynamical.track import load_track, Track\n",
    "from ese2000_dynamical.simulator import Simulator, dynamics_ca\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.style\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from IPython.core.display_functions import clear_output\n",
    "import os\n",
    "\n",
    "matplotlib.style.use(\"seaborn-v0_8-colorblind\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "data_path = Path(\"./ese2000-dynamical-systems/data\")\n",
    "figure_path = Path(\"figures\")\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "expert_sample = torch.from_numpy(np.load(\"./lerp_expert_sample.npy\")).float().to(device)\n",
    "simulator = Simulator()\n",
    "sim_weight_A = torch.Tensor(simulator.A).to(device)\n",
    "sim_weight_B = torch.Tensor(simulator.B).to(device)\n",
    "track = load_track(data_path / \"track.npz\")\n",
    "\n",
    "def plot_vs_expert(x, x_label: str, x_expert, track: Track):\n",
    "    plt.figure()\n",
    "    track.plot()\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"x (m)\")\n",
    "    plt.ylabel(\"y (m)\")\n",
    "    plt.plot(x_expert[:, 0], x_expert[:, 1], \"--\", label=\"Expert\")\n",
    "    plt.plot(x[:, 0], x[:, 1], \"-\", label=x_label)\n",
    "    plt.legend(loc=\"upper right\", framealpha=1.0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Please inspect the `Actor_Critic.py` file.\n",
    "# TODO: Look at how the policy model is encapsulated with the `Actor` class as well as all the global variables that are defined at the top.\n",
    "# TODO: Look at the `Critic` class to see how the pretrained model is loaded and what inputs it accepts\n",
    "from Actor_Critic import *"
   ],
   "id": "76c34a2c7b1c486e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load pretrained critic model",
   "id": "46b9b149446aba59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pretrained_critic_model_path = \"./models/q_model.pth\"\n",
    "critic = Critic(pretrained_critic_model_path, device)"
   ],
   "id": "f77d0eb5c461d051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Policy",
   "id": "92188664a5daf93d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Your policy must accept tensors of dimension `(batch_size, STATE_LOOK_AHEAD_DIM)`**\n",
   "id": "abadad2c4c6d5479"
  },
  {
   "cell_type": "code",
   "id": "3579470f027d63ca",
   "metadata": {},
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_LOOK_AHEAD_DIM, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 2)\n",
    "        init.uniform_(self.fc4.weight, a=-1e-6, b=1e-6)\n",
    "\n",
    "    def forward(self, normalized_state_look_ahead):\n",
    "        x = normalized_state_look_ahead\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "256e123efe063df0",
   "metadata": {},
   "source": [
    "if os.path.exists(\"./models/policy_model.pth\"):\n",
    "    policy  = torch.jit.load(\"./models/policy_model.pth\").to(device)\n",
    "    print(\"Model already trained and loaded from file 'models/policy_model.pth'\")\n",
    "    print(\"If you would like to retrain your model, delete './models' and run this cell again.\")\n",
    "else:\n",
    "    policy = Policy().to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**We encapsulate the policy model with the Actor class to abstract some functions that make learning easier**. Feel free to inspect `Actor_Critic.py` for more details.",
   "id": "ad632f0e0c2db2f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "actor = Actor(policy, device, expert_sample)",
   "id": "98c3867133f6d494",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Rollout Policy Function",
   "id": "8bbabd37912d8c1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define a function to iteratively evaluate a policy based on its current state till it diverges `truncate_distance` from the track. This gives us a trajectory for our actor/policy.",
   "id": "550bfdbeef7b149f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rollout(actor, expert_sample, truncate_distance=1, max_timesteps=250, random_start=True):\n",
    "    \n",
    "    # The true trajectory (that can be plotted) of the actor/policy\n",
    "    predicted_trajectory = []\n",
    "    # normalized policy states with look ahead\n",
    "    policy_states = []\n",
    "    # normalized policy actions\n",
    "    policy_actions = []\n",
    "    state = expert_sample[0].unsqueeze(0)\n",
    "    if random_start:\n",
    "        state = expert_sample[\n",
    "            int(torch.rand(1).item() * expert_sample.shape[0])].unsqueeze(0)\n",
    "    predicted_trajectory.append(state)\n",
    "    for t in range(max_timesteps - 1):\n",
    "    \n",
    "        normalized_action, state_with_look_ahead_normalized = actor.act(state)\n",
    "        action = normalized_action * ACTION_SCALE\n",
    "        \n",
    "        policy_states.append(state_with_look_ahead_normalized)\n",
    "        policy_actions.append(normalized_action)\n",
    "        \n",
    "        next_state = state.detach() @ sim_weight_A.T + action.detach() @ sim_weight_B.T\n",
    "        predicted_trajectory.append(next_state)\n",
    "        \n",
    "        if get_closest_point(state, expert_sample)[0] > truncate_distance:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    policy_states = torch.stack(policy_states).squeeze(1)\n",
    "    policy_actions = torch.stack(policy_actions).squeeze(1)\n",
    "    predicted_trajectory = torch.stack(predicted_trajectory).squeeze(1).detach()\n",
    "    \n",
    "    return policy_states, policy_actions, predicted_trajectory"
   ],
   "id": "4a1fd83c3a9f18d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "55bca575deef3f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import trange\n",
    "torch.set_printoptions(precision=3)\n",
    "episodes = 10_000\n",
    "gamma = 0.99\n",
    "policy_loss = 0\n",
    "policy_opt = torch.optim.Adam(policy.parameters(), lr=2e-5, maximize=True)\n",
    "policy.train()\n",
    "for episode in trange(episodes):\n",
    "\n",
    "    policy_opt.zero_grad()\n",
    "\n",
    "    (policy_states,\n",
    "     policy_actions,\n",
    "     predicted_trajectory) = rollout(actor, expert_sample)\n",
    "\n",
    "    policy_loss = critic.criticize(policy_states, policy_actions)\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        clear_output()\n",
    "        print(f'Episode {episode+1}\\t Policy loss: {policy_loss}\\t Trajectory size: {predicted_trajectory.shape[0]}')\n",
    "        plot_vs_expert(predicted_trajectory.cpu().numpy(), 'RL', expert_sample.detach().cpu().numpy() , track)\n",
    "        plt.show()"
   ],
   "id": "1f6e15b281b90ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate",
   "id": "4f41d52ef7e83aae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, _, predicted_trajectory = rollout(actor, expert_sample, random_start=False)\n",
    "plot_vs_expert(predicted_trajectory.cpu().numpy(), 'RL', expert_sample.detach().cpu().numpy() , track)\n",
    "plt.show()"
   ],
   "id": "2ab6ee43eb97172e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5215a95d99fdadc7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
