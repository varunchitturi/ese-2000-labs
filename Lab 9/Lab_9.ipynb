{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211d69dfabea3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1720988611829,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1211d69dfabea3c5",
    "outputId": "96a0a628-f9a8-42cf-d6c3-9463ede66687"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [],
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    decoded = \" \".join([itos[i] for i in int_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "sample_words = text[:36]\n",
    "print(\"Original text: {}\\n\".format(sample_words))\n",
    "print(\"Encoded text: {}\\n\".format(words_to_tokens(split_to_words(sample_words))))\n",
    "print(\"Decoded text: {}\\n\".format(tokens_to_words(words_to_tokens(split_to_words(sample_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22463c10a95801e",
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c3e73672a0d716",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988621247,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "82c3e73672a0d716"
   },
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329672eb8116e436",
   "metadata": {
    "id": "329672eb8116e436"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f4e2e10b103e95",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988622421,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "31f4e2e10b103e95"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx:idx + self.T],  self.text[idx + 1:idx + self.T + 1]\n",
    "\n",
    "train_set = TextDataset(train, T)\n",
    "test_set = TextDataset(test, T)\n",
    "batch_size = 300\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [],
   "source": [
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix X is a VxV (V is our vocab size) symmetric matrix where X_ij is how many times the ith word appears within W words away from the jth word.\n",
    "W = 10\n",
    "X = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "for t_idx in trange(len(tokenized_text)):\n",
    "    left_bound = max(t_idx-W//2,0)\n",
    "    right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "    context_words = tokenized_text[left_bound : right_bound]\n",
    "    for u_idx in range(left_bound, right_bound):\n",
    "        t = tokenized_text[t_idx]\n",
    "        u = tokenized_text[u_idx]\n",
    "        X[t, u] += 1.0\n",
    "X = X.to(device)\n",
    "# X should be a symmetric matrix\n",
    "torch.isclose(X, X.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isclose(X, X.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "Z = X - X.mean(dim=0, keepdim=True)\n",
    "Z /= Z.std(dim=0, keepdim=True)\n",
    "cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "L, Q = torch.linalg.eigh(cov)\n",
    "principal_eigv = Q[:, -n:].T\n",
    "\n",
    "# PCA embeddings for training\n",
    "embeddings = Z @ principal_eigv.T # (vocab_size, n)\n",
    "# Full embeddings if we need them to visualize\n",
    "# In vector form would be Q.T @ x_n\n",
    "full_embeddings = Z @ Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "# Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "## Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=14295\n",
    "#average_coefficients = full_embeddings.mean(axis=0)\n",
    "sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# Compute the expectation of the absolute value of the norm of each component.\n",
    "average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "data = average_coefficients[:K]\n",
    "\n",
    "# Reverse the tensor:\n",
    "data = data\n",
    "\n",
    "# Normalize by sum?\n",
    "#data = data / data.sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(f\"Average Coefficients (k={K})\")\n",
    "fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "## Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=64\n",
    "L_plot = L[-K:]/L.sum()\n",
    "L_plot,_ = L_plot.sort(descending=True)\n",
    "L_plot = L_plot.cpu().numpy()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Principal eigenvalues (k=64)\")\n",
    "markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "## Co ocurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words\n",
    "top_10_words = X.sum(axis=0).sort(descending=True).indices[:10]\n",
    "top_10_words = [vocab[i] for i in top_10_words]\n",
    "print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Remove all the punctations and stop words from the matrix for visualization\n",
    "X_viz = X.clone()\n",
    "words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# top 20 words not including stop words\n",
    "top_100_words = X.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# Create a custom colormap\n",
    "cmap = plt.cm.get_cmap('viridis').copy()\n",
    "cmap.set_over('green')\n",
    "\n",
    "# Plot the image with the custom colormap\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# Add colorbar with custom settings\n",
    "cbar = plt.colorbar(extend='max')\n",
    "cbar.set_label('Value')\n",
    "\n",
    "plt.title('Co-occurrence Matrix')\n",
    "plt.show()\n",
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tx60HzRzvef",
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3gCca0eqy91t",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1720988709140,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "3gCca0eqy91t"
   },
   "outputs": [],
   "source": [
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n is the embedding dimension in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead attention layer, analogous to the one in the \n",
    "        AttentionLayer class. The main difference is that we need to make sure that the \n",
    "        matrix multiplications account for the new head dimenison.\n",
    "        Args:\n",
    "            X (torch.Tensor): The input sequence.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        self.norm1(X.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        # The unsqueeze is used to add the head dimension to the matrices,\n",
    "        # because they are of shape (H, m, n), and we need to multiply them\n",
    "        # with X_expanded of shape (B, 1, n, T)\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "\n",
    "        # Transpose QX for multiplication\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B_matrix = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # Compute attention weights A per head\n",
    "        A = F.softmax(B_matrix, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute Z per head\n",
    "        Z = torch.matmul(VX, A)  # (B, H, m, T)\n",
    "\n",
    "        # Average over the heads\n",
    "        Z = Z.sum(dim=1)\n",
    "\n",
    "        # Continue with feed-forward network\n",
    "        Y_l = torch.matmul(self.W, Z)  # (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        Y_l = self.norm2(Y_l.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y_l)  # (B, n, T)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        X_l = self.dropout(X_l)\n",
    "\n",
    "        return X_l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "RYUNfNqx0TSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720988711543,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "RYUNfNqx0TSw"
   },
   "outputs": [],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, num_blocks, m, H):\n",
    "        super(LLM, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[MultiHeadLayer(m, n, H) for _ in range(num_blocks)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        token_emb = self.token_embedding[tokens]\n",
    "        pos_emb = self.position_embedding(torch.arange(tokens.shape[1], device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        # We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.decoder_layers(x)\n",
    "        return self.norm(x.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "    def generate(self, input_tokens, max_generate_tokens=500):\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = self(input_tokens[: , -T:])\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tokens = torch.cat([input_tokens, next_token], dim=1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5de84fbe5d8ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1720993361314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ca5de84fbe5d8ec",
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18"
   },
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(\"./model.pt\"):\n",
    "    model = torch.load(\"./model.pt\", map_location=device)\n",
    "    print(\"Loaded existing model\")\n",
    "else:\n",
    "    num_blocks = 3\n",
    "    num_heads_per_block = 8\n",
    "    model = LLM(num_blocks, num_heads_per_block).to(device)\n",
    "    lr = 1e-4\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "    num_epochs = 20\n",
    "    model.eval()\n",
    "    num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "    # wandb.config.update({\"lr\": lr, \n",
    "    #                     \"num_blocks\": num_blocks, \n",
    "    #                     \"num_heads_per_block\": num_heads_per_block,\n",
    "    #                     \"context_size\": T,\n",
    "    #                     \"num_epochs\": num_epochs,\n",
    "    #                     \"model_summary\": str(model),\n",
    "    #                     \"num_parameters\": num_parameters_str})\n",
    "    print(\"Created new model with {}\".format(num_parameters_str))\n",
    "    train_loss_evolution = []\n",
    "    for epoch in trange(num_epochs):\n",
    "        train_loss = 0\n",
    "        for t_idx, (x, y) in enumerate(train_loader):\n",
    "            logits = model(x)\n",
    "            batch_size, _, _ = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss_evolution.append(train_loss/len(train_loader))\n",
    "        clear_output()\n",
    "        print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "        run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "        wandb.config.update({\"num_epochs\": epoch+1})\n",
    "        plt.plot(train_loss_evolution)\n",
    "        plt.show()\n",
    "    torch.save(model, \"./model.pt\")\n",
    "    wandb.save('./model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for t_idx, (x, y) in enumerate(test_loader):\n",
    "        logits = model(x)\n",
    "        batch_size, _, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = \"\".join(tokens_to_words(model.generate(initial, max_generate_tokens=1000).squeeze().tolist()))\n",
    "with open(\"output.txt\", \"w\") as text_file:\n",
    "    text_file.write(generated_text)\n",
    "\n",
    "wandb.save(\"output.txt\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c68a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fI8MSAifK33",
   "metadata": {
    "id": "6fI8MSAifK33"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91497c7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
