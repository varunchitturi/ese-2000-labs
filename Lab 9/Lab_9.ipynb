{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675875",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4bb49",
   "metadata": {},
   "source": [
    "### Functions to encode and decode words into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [13326, 3653, 11563, 4707, 1062, 2388, 3015, 7313]\n",
      "\n",
      "Recovered text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "def words_to_tokens(words):\n",
    "    \"\"\"\n",
    "    Convert a list of words to a list of tokens\n",
    "    \"\"\"\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(index_list):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a list of words\n",
    "    \"\"\"\n",
    "    decoded = \" \".join([itos[i] for i in index_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "# Checking that the word to token and back conversion works\n",
    "sample_words = text[:36]\n",
    "token_ids = words_to_tokens(split_to_words(sample_words))\n",
    "recovered_words = tokens_to_words(token_ids)\n",
    "print(f\"Original text: {sample_words}\\n\")\n",
    "print(f\"Encoded text: {token_ids}\\n\")\n",
    "print(f\"Recovered text: {recovered_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e9b9f",
   "metadata": {},
   "source": [
    "### Converting dataset into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [13326, 3653, 11563, 4707, 1062, 2388, 3015, 7313, 2879, 5692]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([292072])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "\n",
    "# The works of Shakespeare are now a sequence of integers representing the words in the text. Sorry, William.\n",
    "tokenized_text = torch.tensor(tokenized_text)\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Task 2: Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134424/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## Task 3: PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134424/1436626230.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=1, keepdim=True)\n",
    "#     Z /= Z.std(dim=1, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "embeddings = torch.load(\"embeddings.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "## Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "### Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=8192\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "### Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Top k eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "### Co ocurrence matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [],
   "source": [
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for t_idx, (E, y) in enumerate(test_loader):\n",
    "#         logits = model(E)\n",
    "#         B, _ = logits.shape\n",
    "#         loss = F.cross_entropy(logits, y)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "# print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [],
   "source": [
    "# initial = test[132:132+T].unsqueeze(0)\n",
    "# generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68a8c",
   "metadata": {},
   "source": [
    "## Task 4: Language Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b2798",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "\n",
    "# Splitting into train and test sets\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca499d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c22c604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "class WordIndexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Dataset class takes and encoded tensor of word indices and returns a tensor of context windows of size T.\n",
    "    The tensors returned by this dataset are not yet one-hot encoded.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single context window of size T. \n",
    "        The context window is a sequence of T words.\n",
    "\n",
    "        During training, we will predict the next token of every word in the context window,\n",
    "        so Y_item is the next word for every word in the context window.\n",
    "        \"\"\"\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = WordIndexDataset(train, T)\n",
    "test_dataset = WordIndexDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ba46f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [-inf, 1., 1., 1., 1.],\n",
       "        [-inf, -inf, 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=torch.ones(5,5)\n",
    "dddmask=torch.tril_indices(5,5, -1)\n",
    "dd[dddmask[0], dddmask[1]] = float('-inf')\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d6630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6574d8c0",
   "metadata": {},
   "source": [
    "## MultiheadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f02d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([1, 5, 32])\n",
      "out.shape: torch.Size([1, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "class MultiheadLayer(nn.Module):\n",
    "    def __init__(self, H, n=n):\n",
    "        super(MultiheadLayer, self).__init__()\n",
    "        \n",
    "        assert n % H == 0, \"n must be divisible by num_heads\"\n",
    "        self.H = H\n",
    "        self.n = n\n",
    "        #self.attn_heads = nn.ModuleList([HeadAttn(m = n // H, n=n) for _ in range(H)])\n",
    "        self.m = n // H\n",
    "        self.Q = nn.Parameter(torch.empty(H, n, self.m))\n",
    "        self.K = nn.Parameter(torch.empty(H, n, self.m))\n",
    "        self.V = nn.Parameter(torch.empty(H, n, self.m))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, self.m, n))\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.empty(n, 4 * n))\n",
    "        self.W2 = nn.Parameter(torch.empty(4 * n, n))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.W1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W2, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X0):\n",
    "        B, T, n = X0.shape\n",
    "        X = self.norm1(X0) # (B, T, n)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        #heads = torch.stack([attn(X, use_mask=True) for attn in self.attn_heads])\n",
    "\n",
    "        X = X.unsqueeze(1) # (B, 1, T, n)\n",
    "\n",
    "        QX = torch.matmul(X, self.Q) # (B, H, T, m)\n",
    "        KX = torch.matmul(X, self.K) # (B, H, T, m)\n",
    "        VX = torch.matmul(X, self.V) # (B, H, T, m)     \n",
    "        \n",
    "        B =  QX @ KX.transpose(-2, -1) * (self.m ** -0.5) # (B, H, T, T)\n",
    "\n",
    "        mask = torch.tril_indices(T,T, -1)\n",
    "        B[:,:, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        #A = F.softmax(B, dim=-1)\n",
    "        A = F.softmax(B, dim=-1) \n",
    "        \n",
    "        AVX = torch.matmul(A, VX) # (B, H, T, m)\n",
    "        Y = torch.matmul(AVX, self.W) # (B, T, n)\n",
    "\n",
    "        X = torch.sum(Y, dim=1) # (B, T, n)\n",
    "        \n",
    "        X = self.norm2(X)\n",
    "        X2 = X0 + F.relu(X)\n",
    "        return X2\n",
    "    \n",
    "    \n",
    "# Test\n",
    "model = MultiheadLayer(H=4, n=32).to(device)\n",
    "#E = next(iter(train_loader))[0]\n",
    "#X = embeddings[E].transpose(2,1)\n",
    "X = torch.randn(1,5, 32).to(device)\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "X = X.to(device)\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04363f4",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eae4e678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readout.shape: torch.Size([1, 64, 14295])\n"
     ]
    }
   ],
   "source": [
    "class LLM(nn.Module):   \n",
    "    def __init__(self, L, H, n):\n",
    "        super(LLM, self).__init__()\n",
    "\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "        # this is the PCA embeddings \n",
    "        self.token_embedding = embeddings\n",
    "\n",
    "        self.decoder_layers = nn.Sequential(*[MultiheadLayer(H, n) for _ in range(L)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        \n",
    "        #self.readout_weight = nn.Parameter(torch.empty(c, n))\n",
    "        # row wise\n",
    "        self.readout_weight = nn.Parameter(torch.empty(n, c))\n",
    "        nn.init.kaiming_uniform_(self.readout_weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, E):\n",
    "        B, T = E.shape\n",
    "\n",
    "        X = self.token_embedding[E]#.transpose(1,2) # (B, n, T)\n",
    "        P = self.position_embedding(torch.arange(T, device=device))#.transpose(0,1) # (n, T)\n",
    "        \n",
    "        X = X + P\n",
    "\n",
    "        X = self.decoder_layers(X) # (B, T, n)\n",
    "        #X = self.norm(X.transpose(1,2)).transpose(1,2)\n",
    "        X = self.norm(X)\n",
    "\n",
    "        #Y = torch.matmul(self.readout_weight, X) # (B, T, c)\n",
    "        # row wise\n",
    "        Y = torch.matmul(X, self.readout_weight) \n",
    "        \n",
    "        return Y\n",
    "\n",
    "# Test\n",
    "model = LLM(L=2, H=2, n=n).to(device)\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "readout = model(initial)\n",
    "print(f\"readout.shape: {readout.shape}\")\n",
    "# generated_text = generate(model, initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e9687f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician hanging players trumpet Pay easeful Mann'd winked broiled vast lion inevitable Fine unconstrain'd Mariners prattling glistering Bunch contemn Successively cipher Praise Italy bestow'd chidest civil Crassus Accountant hours grievances needless laugh MARGARET dally High ho According Allowing hypocrisy thither inquire depose taunt pirate drowsy unwillingly helpful Brakenbury prevailing violentest Hour spirit brothers misbehaved doers painter infer fowling largess conspires cymbals enter tale's pillow Ethiope's wait relent ends compell'd move Framed Among shamefast desolation resemble beadsmen crosses parson's taunted signs chapless sceptre backing casque heading wounded attorneyed pastures dauntless cobbled permit bribe withdrawn moveth secretly wasteful Brandon report napping Clear Maiden\n"
     ]
    }
   ],
   "source": [
    "def generate_reversed(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context = input_tokens.clone()\n",
    "        generated_sequence = input_tokens.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(context)\n",
    "            \n",
    "            last_token_logits = logits[:,-1,:]\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Slide context window\n",
    "\n",
    "            context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "            generated_sequence.append(next_token.item())  # Add new token to generated sequence\n",
    "        generated_words = tokens_to_words(generated_sequence)\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string\n",
    "# Test generate\n",
    "model = LLM(L=2, H=4, n=n).to(device)\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "print(generate_reversed(model, initial, max_generate_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fdb7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4d6255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qq7ipllo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Varun's version</strong> at: <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/qq7ipllo' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/qq7ipllo</a><br/> View project at: <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_200355-qq7ipllo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qq7ipllo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241021_200922-vjjsx3wp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/vjjsx3wp' target=\"_blank\">Varun's version</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/vjjsx3wp' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/vjjsx3wp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "L = 6\n",
    "H = 8\n",
    "model = LLM(L, H, n).to(device)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    name=\"Varun's version\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4910a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "num_parameters = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {} parameters\".format(num_parameters))\n",
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for t_idx, (X, y) in enumerate(train_loader):\n",
    "        logits = model(X)\n",
    "        B, _, _ = logits.shape\n",
    "        logits = logits.view(B * T, -1)\n",
    "        y = y.view(B * T, -1).squeeze()\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician aside welcome; I would now now now your Hail tis that that so being prerogative storm murderer, therefore put EDWARD therefore good down harm \n",
      " You lie love absolute \n",
      " \n",
      " office \n",
      " \n",
      " \n",
      " \n",
      " Give alive! \n",
      " Let scale spared from how and and and at blind the accept for for member,, how advise thyself own the the the the, which Their oracle content why else not of sake has it it look mock'd to die gold whence, \n",
      " AUTOLYCUS. \n",
      " PETRUCHIO on on \n",
      " midst son \n",
      " Harp censure\n"
     ]
    }
   ],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate_reversed(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
