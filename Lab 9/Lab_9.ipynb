{
 "cells": [
  {
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "id": "4c56f9ac2b2ce56"
  },
  {
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "cell_type": "markdown",
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ],
   "id": "d8ff74f15f78e2e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt",
   "id": "1211d69dfabea3c5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:43:40.509547Z",
     "start_time": "2024-07-12T23:43:38.781450Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import math\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.1450092172177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "2a8eac0f-ac64-4ad9-8a49-9f74b9be0bf8",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:43:45.842210Z",
     "start_time": "2024-07-12T23:43:45.836423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:100])"
   ],
   "id": "58d8918bcd4f0a06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "d5244308b67761a"
   },
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "id": "d5244308b67761a"
  },
  {
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "cell_type": "markdown",
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ],
   "id": "ae55df526e53534b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aba7e30bedd5646",
    "outputId": "2c4f49d3-1569-427b-e8e6-60dd0f6b7e26",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:43:49.116280Z",
     "start_time": "2024-07-12T23:43:48.941218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_to_words(text):\n",
    "    words = []\n",
    "    word = \"\"\n",
    "    for c in text:\n",
    "        if c.isalnum():\n",
    "            word += c\n",
    "        else:\n",
    "            words.append(word)\n",
    "            words.append(c)\n",
    "            word = \"\"\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "words = list(set(split_to_words(text)))\n",
    "vocab_size = len(words)\n",
    "print(\"Number of distinct words in text: {}\".format(vocab_size))"
   ],
   "id": "4aba7e30bedd5646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in text: 13334\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "27134b23-ba14-4b81-ed42-bdea5b404edc",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:43:51.236991Z",
     "start_time": "2024-07-12T23:43:51.101038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(words)}\n",
    "itos = {i:word for i, word in enumerate(words)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    return [itos[i] for i in int_list]\n",
    "\n",
    "sample_words = split_to_words(text)[:10]\n",
    "print(\"Original text: {}\".format(\"\".join(sample_words)))\n",
    "print(\"Encoded text: {}\".format(words_to_tokens(sample_words)))\n",
    "print(\"Decoded text: {}\".format(tokens_to_words(words_to_tokens(sample_words))))"
   ],
   "id": "9cacb2e9ced76d25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we \n",
      "Encoded text: [6541, 6292, 5528, 8716, 0, 5245, 12311, 6292, 432, 6292]\n",
      "Decoded text: ['First', ' ', 'Citizen', ':', '', '\\n', 'Before', ' ', 'we', ' ']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "8d66ef73-4ecc-469f-c7e3-0f3144aff087",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:43:57.220803Z",
     "start_time": "2024-07-12T23:43:56.961152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ],
   "id": "1d146ef59a76b0ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [6541, 6292, 5528, 8716, 0, 5245, 12311, 6292, 432, 6292]\n",
      "['First', ' ', 'Citizen', ':', '', '\\n', 'Before', ' ', 'we', ' ']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "cell_type": "markdown",
   "source": [
    "## Data Split"
   ],
   "id": "a22463c10a95801e"
  },
  {
   "metadata": {
    "id": "82c3e73672a0d716",
    "ExecuteTime": {
     "end_time": "2024-07-12T23:44:00.504226Z",
     "start_time": "2024-07-12T23:44:00.486319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 32\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ],
   "id": "82c3e73672a0d716",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "329672eb8116e436"
   },
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "id": "329672eb8116e436"
  },
  {
   "metadata": {
    "id": "31f4e2e10b103e95",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:42:38.821928Z",
     "start_time": "2024-07-13T05:42:38.816515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, context_size):\n",
    "        self.text = text\n",
    "        self.context_size = context_size\n",
    "        assert self.context_size < len(text), \"context_size must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx:idx + self.context_size],  self.text[idx + 1:idx + self.context_size + 1]\n",
    "\n",
    "train_set = TextDataset(train, context_size)\n",
    "test_set = TextDataset(test, context_size)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ],
   "id": "31f4e2e10b103e95",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "cell_type": "markdown",
   "source": [
    "# Embeddings"
   ],
   "id": "659a4f4edabab2a2"
  },
  {
   "metadata": {
    "id": "1a1790cd0b8bacdd"
   },
   "cell_type": "markdown",
   "source": [
    "We will use PCA to create the token embeddings"
   ],
   "id": "1a1790cd0b8bacdd"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccbafd52bae8f505",
    "outputId": "d7adbdbb-f3e7-4114-fadb-28ec1c31891b",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:43:11.970983Z",
     "start_time": "2024-07-13T05:42:41.171177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix X is a VxV (V is our vocab size) symmetric matrix where X_ij is how many times the ith word appears within W words away from the jth word.\n",
    "W = 10\n",
    "X = torch.stack([torch.zeros(len(words)) for _ in range(len(words))])\n",
    "for i in trange(len(tokenized_text)):\n",
    "    words_to_right = tokenized_text[i+1:i+W+1]\n",
    "    words_to_left = tokenized_text[i-W:i]\n",
    "    X[tokenized_text[i], words_to_right] += 1.0\n",
    "    X[tokenized_text[i], words_to_left] += 1.0\n",
    "X = X.to(device)"
   ],
   "id": "ccbafd52bae8f505",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528579/528579 [00:30<00:00, 17501.61it/s]\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "id": "582e9c67a87949a4",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:52:14.433397Z",
     "start_time": "2024-07-13T05:52:04.611199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "embedding_dim = 256\n",
    "X -= X.mean(dim=1, keepdim=True)\n",
    "X /= X.std(dim=1, keepdim=True)\n",
    "cov = (X @ X.T)/(X.shape[0] - 1)\n",
    "L, Q = torch.linalg.eigh(cov)\n",
    "principle_eigv = Q[:, -embedding_dim:].T\n",
    "embeddings = X @ principle_eigv.T # (vocab_size, embedding_dim)"
   ],
   "id": "582e9c67a87949a4",
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "id": "7tx60HzRzvef"
  },
  {
   "cell_type": "code",
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, num_heads, dqk, dv):\n",
    "        super(Attn, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dqk = dqk\n",
    "        self.dv = dv\n",
    "        self.Wq = nn.Parameter(torch.randn(num_heads, embedding_dim, dqk))\n",
    "        nn.init.kaiming_uniform_(self.Wq, a=math.sqrt(5))\n",
    "        self.Wk = nn.Parameter(torch.randn(num_heads, embedding_dim, dqk))\n",
    "        nn.init.kaiming_uniform_(self.Wk, a=math.sqrt(5))\n",
    "        self.Wv = nn.Parameter(torch.randn(num_heads, embedding_dim, dv))\n",
    "        nn.init.kaiming_uniform_(self.Wv, a=math.sqrt(5))\n",
    "        self.Wo = nn.Parameter(torch.randn(num_heads * dv, embedding_dim))\n",
    "        nn.init.kaiming_uniform_(self.Wo, a=math.sqrt(5))\n",
    "    def forward(self, x, use_mask=False):\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        B, N, D = x.shape\n",
    "        x = x.unsqueeze(1)\n",
    "        q = x @ self.Wq.unsqueeze(0)\n",
    "        k = x @ self.Wk.unsqueeze(0)\n",
    "        v = x @ self.Wv.unsqueeze(0)\n",
    "        qk =  q @ k.transpose(-2, -1) * (self.dqk ** -0.5)\n",
    "\n",
    "        if use_mask:\n",
    "            mask = torch.tril_indices(qk.shape[-2], qk.shape[-1], -1)\n",
    "            qk[:, :, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        softmax_qk = F.softmax(qk, dim=-1)\n",
    "        qkv = softmax_qk @ v\n",
    "        concat_qkv = qkv.permute(0, 2, 1, 3).reshape(B, N, self.num_heads * self.dv)\n",
    "        out = concat_qkv @ self.Wo.unsqueeze(0)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "id": "3gCca0eqy91t",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:52:17.204400Z",
     "start_time": "2024-07-13T05:52:17.195913Z"
    }
   },
   "id": "3gCca0eqy91t",
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, dqk=embedding_dim, dv=embedding_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_attn = Attn(num_heads, dqk, dv)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.linear2 = nn.Linear(3 * embedding_dim, embedding_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.masked_attn(self.norm1(x)) + x\n",
    "        x = self.linear2(F.relu(self.linear1(self.norm2(x)))) + x\n",
    "        return x"
   ],
   "metadata": {
    "id": "2zgrRoCY04CP",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:52:17.530789Z",
     "start_time": "2024-07-13T05:52:17.525987Z"
    }
   },
   "id": "2zgrRoCY04CP",
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "source": [
    "class LLM(nn.Module):\n",
    "  def __init__(self, num_blocks, num_heads_per_block, key_query_dim=embedding_dim, value_dim=embedding_dim):\n",
    "    super(LLM, self).__init__()\n",
    "    self.num_blocks = num_blocks\n",
    "    self.attn = Attn(num_heads_per_block, key_query_dim, value_dim)\n",
    "    self.position_embedding = nn.Embedding(context_size, embedding_dim)\n",
    "    self.token_embedding = embeddings\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(num_heads_per_block, key_query_dim, value_dim) for _ in range(num_blocks)])\n",
    "    self.norm = nn.LayerNorm(embedding_dim)\n",
    "    self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "  def forward(self, tokens):\n",
    "    token_emb = self.token_embedding[tokens]\n",
    "    pos_emb = self.position_embedding(torch.arange(tokens.shape[1], device=device))\n",
    "    x = token_emb + pos_emb\n",
    "    for layer in self.decoder_layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    return self.out(self.norm(x))\n",
    "\n",
    "  def generate(self, input_tokens, max_generate_tokens=500):\n",
    "    for _ in range(max_generate_tokens):\n",
    "      print(input_tokens)\n",
    "      logits = self(input_tokens[: , -context_size:])\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      input_tokens = torch.cat([input_tokens, next_token], dim=1)\n",
    "    return input_tokens"
   ],
   "metadata": {
    "id": "RYUNfNqx0TSw",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:52:17.887830Z",
     "start_time": "2024-07-13T05:52:17.881018Z"
    }
   },
   "id": "RYUNfNqx0TSw",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T05:52:18.638582Z",
     "start_time": "2024-07-13T05:52:18.386045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_blocks = 6\n",
    "num_heads_per_block = 8\n",
    "model = LLM(num_blocks, num_heads_per_block).to(device)\n",
    "lr = 1e-4\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "num_epochs = 100"
   ],
   "id": "ca5de84fbe5d8ec",
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "source": [
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        logits = model(x)\n",
    "        batch_size, _, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(batch_size * context_size, -1), y.view(batch_size * context_size, -1).squeeze())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {train_loss/len(train_loader)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIlPL9cr6YxE",
    "outputId": "9cedb567-3ad4-4f7a-8ae1-94dcf37b0964",
    "ExecuteTime": {
     "end_time": "2024-07-13T05:53:10.271704Z",
     "start_time": "2024-07-13T05:52:21.266225Z"
    }
   },
   "id": "FIlPL9cr6YxE",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[80], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     10\u001B[0m     opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 11\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        logits = model(x)\n",
    "        batch_size, _, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(batch_size * context_size, -1), y.view(batch_size * context_size, -1).squeeze())\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ],
   "id": "4e45c455cd8bcd29",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "initial = torch.tensor(words_to_tokens(split_to_words(\"Romeo:\\n\")), device=device).int().unsqueeze(0)\n",
    "print(\"\".join(tokens_to_words(model.generate(initial, max_generate_tokens=1000).squeeze().tolist())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "Jhoh1INhBePM",
    "outputId": "7889c3ba-cfb5-4a1a-d1e6-c04bcf13eb99"
   },
   "id": "Jhoh1INhBePM",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
