{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "device = \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps:0\"\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "431bddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # deleteme\n",
    "# dd=torch.ones(5,5)\n",
    "# mask = torch.tril_indices(5,5, -1)\n",
    "# dd[mask[0], mask[1]] = float('-inf')\n",
    "# dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4bb49",
   "metadata": {},
   "source": [
    "### Functions to encode and decode words into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [11164, 13785, 101, 178, 3687, 1787, 6423, 844]\n",
      "\n",
      "Recovered text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "def words_to_tokens(words):\n",
    "    \"\"\"\n",
    "    Convert a list of words to a list of tokens\n",
    "    \"\"\"\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(index_list):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a list of words\n",
    "    \"\"\"\n",
    "    decoded = \" \".join([itos[i] for i in index_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "# Checking that the word to token and back conversion works\n",
    "sample_words = text[:36]\n",
    "token_ids = words_to_tokens(split_to_words(sample_words))\n",
    "recovered_words = tokens_to_words(token_ids)\n",
    "print(f\"Original text: {sample_words}\\n\")\n",
    "print(f\"Encoded text: {token_ids}\\n\")\n",
    "print(f\"Recovered text: {recovered_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e9b9f",
   "metadata": {},
   "source": [
    "### Converting dataset into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [11164, 13785, 101, 178, 3687, 1787, 6423, 844, 9168, 6637]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([292072])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "\n",
    "# The works of Shakespeare are now a sequence of integers representing the words in the text. Sorry, William.\n",
    "tokenized_text = torch.tensor(tokenized_text)\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Task 2: Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199231/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## Task 3: PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199231/3551890280.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pca_embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([14295, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=1, keepdim=True)\n",
    "#     Z /= Z.std(dim=1, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     pca_embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full pca_embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "pca_embeddings = torch.load(\"embeddings.pt\").to(device)\n",
    "pca_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "## Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "### Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=8192\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "### Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Top k eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "### Co ocurrence matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [],
   "source": [
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for t_idx, (E, y) in enumerate(test_loader):\n",
    "#         logits = model(E)\n",
    "#         B, _ = logits.shape\n",
    "#         loss = F.cross_entropy(logits, y)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "# print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [],
   "source": [
    "# initial = test[132:132+T].unsqueeze(0)\n",
    "# generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68a8c",
   "metadata": {},
   "source": [
    "## Task 4: Language Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dca834",
   "metadata": {},
   "source": [
    "### MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ea7ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "# This is the same as the MultiheadLayer in the lab 6 notebook. It corresponds to the equations in Section 3 of this lab's writeup.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "    \n",
    "    \n",
    "model = MultiHeadLayer(m=32, n=256, H=8).to(device)\n",
    "X_tilde = torch.randn(1,256,64).to(device)\n",
    "out = model(X_tilde)\n",
    "\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52a20d",
   "metadata": {},
   "source": [
    "### LanguageTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6529c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Mutlihead Transformer, analogous to the Transformer class, in the single head case.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "        # Word embedding table. This is the only change from the previous lab's code. We have \n",
    "        # PCA embeddings to convert word indices to embeddings.\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead transformer, stacks L multihead layers.\n",
    "        This class is essentially the same as the Transformer class, but using the \n",
    "        MultiHeadLayer class instead of the AttentionLayer class.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            X_L^{T-1} (torch.Tensor): The last vector of the output of the transformer.\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings. We need to transpose the result to get the shape (B, n, T).\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "        B, n, T = X.shape\n",
    "\n",
    "        # Compute the mean token to append to the sequence.\n",
    "        X_tilde = X.mean(dim=2, keepdim=True) # mean over the time dimension\n",
    "        X_tilde = torch.cat((X, X_tilde), dim=-1)\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "        \n",
    "        # Output the last vector.\n",
    "        return X_l[:,:,-1]\n",
    "\n",
    "# Test\n",
    "model = LanguageTransformer(L=2, H=2, m=32, n=256).to(device)\n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "out = model(E)\n",
    "print(f\"output.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b2798",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c28745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "    \n",
    "# Splitting into train and test sets\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca499d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c22c604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "class WordIndexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Dataset class takes and encoded tensor of word indices and returns a tensor of context windows of size T.\n",
    "    The tensors returned by this dataset are not yet one-hot encoded.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single context window of size T. \n",
    "        The context window is a sequence of T words.\n",
    "\n",
    "        During training, we will predict the next token of every word in the context window,\n",
    "        so Y_item is the next word for every word in the context window.\n",
    "        \"\"\"\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = WordIndexDataset(train, T)\n",
    "test_dataset = WordIndexDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173015c",
   "metadata": {},
   "source": [
    "## Task 5: Training on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4d27124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jporras/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([256, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 13/4107 [00:03<19:07,  3.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Gradient reset to indicate where the backward computation stops.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Call the neural network. In this case, we will take the average of the output of the\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# transformer as the prediction.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m cross_entropy_value \u001b[38;5;241m=\u001b[39m cross_entropy_loss(y_hat,Y_embeddings)\n\u001b[1;32m     36\u001b[0m cross_entropy_value\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Compute gradients moving backwards untit the gradient reset.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[63], line 42\u001b[0m, in \u001b[0;36mLanguageTransformer.forward\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m     40\u001b[0m X_l \u001b[38;5;241m=\u001b[39m X_tilde\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 42\u001b[0m     X_l \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Output the last vector.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_l[:,:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[62], line 54\u001b[0m, in \u001b[0;36mMultiHeadLayer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Compute QX, KX, VX for each head\u001b[39;00m\n\u001b[1;32m     53\u001b[0m QX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), X_expanded)  \u001b[38;5;66;03m# (B, H, m, T)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m KX \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_expanded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, H, m, T)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m VX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), X_expanded)  \u001b[38;5;66;03m# (B, H, m, T)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m QX_t \u001b[38;5;241m=\u001b[39m QX\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, H, T, m)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_epochs = 10\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = T\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformer(m, n, L, H).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "cross_entropy_loss = nn.MSELoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # Load the embeddings for the target word\n",
    "        # We want to predict the last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        Y_embeddings = pca_embeddings[y_word_to_predict].transpose(0,1).to(device) # (B, n)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch).mean(dim=-1)\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,Y_embeddings)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "print(train_loss[-1]) # Print training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84e2b8",
   "metadata": {},
   "source": [
    "## Task 6: Adding the readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae465fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 14295, 5])\n"
     ]
    }
   ],
   "source": [
    "class LanguageTransformerWithReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    A slight modification of the LanguageTransformer class of Task 4.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "        c (int): The vocabulary size.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadout, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "\n",
    "        B, n, T = X.shape\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be29cd",
   "metadata": {},
   "source": [
    "## Task 7: Training for word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f05b002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/4107 [00:07<48:01,  1.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m estimator(x_batch)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# When using cross entropy loss, we need to pass the target as a 1D tensor of class indices.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# The softmax function is applied internally to the transformer's output y_hat.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m cross_entropy_value \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m cross_entropy_value\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Compute gradients moving backwards untit the gradient reset.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# (Step iii) Update parameters by taking an SGD (or other optimizer) step.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_epochs = 5\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = T\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformerWithReadout(m, n, L, H, c).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "# We use the Cross Entropy loss for estimating the probabilities of the next word.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # We want to predict tha last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        # Load the embeddings for the words.\n",
    "        X_embeddings = pca_embeddings[x_batch].transpose(1,2) # (B, n, T)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch)\n",
    "\n",
    "        # When using cross entropy loss, we need to pass the target as a 1D tensor of class indices.\n",
    "        # The softmax function is applied internally to the transformer's output y_hat.\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_batch)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "print(train_loss[-1]) # Print training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf06d6",
   "metadata": {},
   "source": [
    "## Task 8: Sampling from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42322234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probabilities.shape: torch.Size([1, 14295, 64])\n",
      "Input text: afeard of you. \n",
      " \n",
      " Widow: \n",
      " He that is giddy thinks the world turns round. \n",
      " \n",
      " PETRUCHIO: \n",
      " Roundly replied. \n",
      " \n",
      " KATHARINA: \n",
      " Mistress, how mean you that? \n",
      " \n",
      " Widow: \n",
      " Thus I conceive by him. \n",
      " \n",
      " PETRUCHIO: \n",
      " Conceives by me! How likes Hortensio that?\n",
      "\n",
      "The most likely next word is: Blunt\n",
      "\n",
      "Sampled words according to a multinomial distribution (either could be the next word when using sampling):\n",
      "warning cuff vilely foulest building keep'st starveth chose assist nails "
     ]
    }
   ],
   "source": [
    "# Taking a snippet of the text set to test the model.\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "\n",
    "log_probabilities = model(initial_indices)\n",
    "print(f\"log_probabilities.shape: {log_probabilities.shape}\")\n",
    "last_word_probabilities = log_probabilities[:,:,-1]\n",
    "probabilities = F.softmax(last_word_probabilities, dim=-2)\n",
    "\n",
    "print(f\"Input text: {tokens_to_words(initial_indices.reshape(-1).tolist())}\")\n",
    "print(f\"\\nThe most likely next word is: {tokens_to_words([torch.argmax(probabilities).item()])}\")\n",
    "\n",
    "\n",
    "print(\"\\nSampled words according to a multinomial distribution (either could be the next word when using sampling):\")\n",
    "for _ in range(10):\n",
    "    sampled_word = torch.multinomial(probabilities, num_samples=1).item()\n",
    "    print(f\"{tokens_to_words([sampled_word])}\", end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900490a",
   "metadata": {},
   "source": [
    "## Task 9: Generating language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2bc907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== INPUT TEXT ==========\n",
      "thou shalt think on prating whilst thou livest! \n",
      " I tell thee, I, that thou hast marr'd her gown. \n",
      " \n",
      " Tailor: \n",
      " Your worship is deceived; the gown is made \n",
      " Just as my master had direction: \n",
      " Grumio gave order how it should be done. \n",
      " \n",
      " GRUMIO: \n",
      " I gave him no\n",
      "\n",
      "========== INPUT + GENERATED TEXT ==========\n",
      "thou shalt think on prating whilst thou livest! \n",
      " I tell thee, I, that thou hast marr'd her gown. \n",
      " \n",
      " Tailor: \n",
      " Your worship is deceived; the gown is made \n",
      " Just as my master had direction: \n",
      " Grumio gave order how it should be done. \n",
      " \n",
      " GRUMIO: \n",
      " I gave him no endow'd enter'd cross obscure approved minds sol buckets Wherewith paste hose ass readiest tormentors rise whistle cheated Stephen dependant suppliants Wrath assembled prisoner's attendance cries wreaths burr sternness lilies Mantua's cur stood gotten cracking valiantness rings thankless derivative holiness incorporate policy accomplish'd beaten retail Twould dogg'd gallery Whipp'd amity steely reeking fetch Tincture forceful commons history Carthage scapes unfold islands rectorship packthread womankind dispenses looking clime pipe knowest angry pack wayward We whoever enpierced tackles lungs wits round Any repent Sink Treason heedfully performed reputation empire murder'd richest gently overjoy'd prenzie pathway gross stand'st issues jour plenteous forsooth heal'd apricocks\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, X, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context = X.clone()\n",
    "        generated_sequence = X.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(context)\n",
    "            \n",
    "            last_word_embeddings = logits[:,:,-1]\n",
    "            probs = F.softmax(last_word_embeddings, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Slide context window: remove the first token and append the next token\n",
    "            context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "            generated_sequence.append(next_token.squeeze().item())  # Add new token to generated sequence\n",
    "        generated_words = tokens_to_words(generated_sequence)\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string\n",
    "\n",
    "# Test generate\n",
    "#model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "print(f\"========== INPUT TEXT ==========\")\n",
    "print(f\"{tokens_to_words(initial_indices.reshape(-1).tolist())}\\n\")\n",
    "\n",
    "# This is the model from task 7\n",
    "print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "print(generate_text(model, initial_indices, max_generate_tokens=100))\n",
    "print(f\"====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06d1ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be did Resign repent officers chestnut ladyship cross weeping desperation words steel'd murder'd rebukes worthiness possess'd unloads chair crushing increaseth generation comely lying shivering memory vesture stair graft bow assembled slaughter'd hospitable wooer's You're bluntly flaky piled annual conferring frank esteem carriage began Thracian esteem tending effect glove childish scoffs temper'd unfit Call'd destroyed force nymphs Likely helpless props painter length wield revenge inducement vicar mortality needed pamper horizon frequent Hercules forfend Had dad stinted Post load Humbly spotted circled Wars jay racking Tybalt frosts shoot beguile looking sorrow shoe Nathaniel solemn Turkey Twould whether Conceiving Pudding nonage presage Served wombs\n"
     ]
    }
   ],
   "source": [
    "words = \"To be or not to be\".split(\" \")\n",
    "initial_indices = torch.tensor(words_to_tokens(words)).to(device).unsqueeze(0)\n",
    "print(generate_text(model, initial_indices, max_generate_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5a3b5",
   "metadata": {},
   "source": [
    "## Task 10: Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "068c503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([1, 5])\n",
      "out.shape: torch.Size([1, 14295, 5])\n"
     ]
    }
   ],
   "source": [
    "class LanguageTransformerWithReadoutAndPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Modification of the LanguageTransformerWithReadout class of Task 7 to include positional encoding.\n",
    "    Positional encoding is a learnable matrix that is added to the embeddings of the input tokens.\n",
    "    \n",
    "    Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadoutAndPositionalEncoding, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # Learnable parameters for positional encoding. \n",
    "        # Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        # Adding word embeddings and positional encoding\n",
    "        # Although P is (n,T), this is broadcasted to (B, n, T), which means that the same \n",
    "        # positional encoding is added to every sequence in the batch.\n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformerWithReadoutAndPositionalEncoding(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49a97d",
   "metadata": {},
   "source": [
    "## Task 11: Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c12ea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 5])\n"
     ]
    }
   ],
   "source": [
    "# We now need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68411a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([1, 5])\n",
      "out.shape: torch.Size([1, 14295, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from Task 10 and added layer normalization. This is the final version of this class.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # PCA Word embeddings\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(n)\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        X_l = self.layer_norm(X_l.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        \n",
    "        return Y_hat\n",
    "\n",
    "# testing. \n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformer(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08718c",
   "metadata": {},
   "source": [
    "## Task 12: Future masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41c30e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 5])\n"
     ]
    }
   ],
   "source": [
    "# We need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # FUTURE MASKING: \n",
    "        # To mask attention, we create a matrix that indicates if an entry in B is a word in the future\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "        \n",
    "        # If an entry is in the future, we set it to -inf, \n",
    "        # so that when we apply softmax, the probability of that word is 0, while \n",
    "        # the rest of the words sum to 1.\n",
    "        B = B.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "        # Now when we apply softmax, only the words in the past are have nonzero probability.\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92a0ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([1, 32, 5])\n",
      "out.shape: torch.Size([1, 32, 5])\n"
     ]
    }
   ],
   "source": [
    "# class MultiheadLayer(nn.Module):\n",
    "#     def __init__(self, H, n=n):\n",
    "#         super(MultiheadLayer, self).__init__()\n",
    "        \n",
    "#         assert n % H == 0, \"n must be divisible by num_heads\"\n",
    "#         self.H = H\n",
    "#         self.n = n\n",
    "#         self.m = n // H\n",
    "\n",
    "#         # Column wise\n",
    "#         self.Q = nn.Parameter(torch.empty(H, self.m, n))\n",
    "#         self.K = nn.Parameter(torch.empty(H, self.m, n))\n",
    "#         self.V = nn.Parameter(torch.empty(H, self.m, n))\n",
    "\n",
    "#         self.W = nn.Parameter(torch.empty(H, n, self.m))\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(n)\n",
    "#         self.norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "#         nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "#     def forward(self, X0):\n",
    "#         # Column wise\n",
    "#         B, n, T = X0.shape\n",
    "#         X = self.norm1(X0.transpose(1,2)).transpose(1,2) # (B, T, n)\n",
    "#         X = X0\n",
    "        \n",
    "#         # Multi-head attention\n",
    "\n",
    "#         X = X.unsqueeze(1) # (B, 1, T, n)\n",
    "        \n",
    "#         # Column wise\n",
    "#         QX = torch.matmul(self.Q, X) # (B, H, T, m)\n",
    "#         KX = torch.matmul(self.K, X) # (B, H, T, m)\n",
    "#         VX = torch.matmul(self.V, X) # (B, H, T, m)\n",
    "#         # \n",
    "#         B =  QX.transpose(-2,-1) @ KX * (self.m ** -0.5) # (B, H, T, T)\n",
    "\n",
    "#         mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "#         B = B.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "#         A = F.softmax(B, dim=-1)\n",
    "    \n",
    "#         A_t = A.transpose(-2,-1)\n",
    "#         VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "#         Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "#         X = torch.sum(Y, dim=1) # (B, T, n)\n",
    "\n",
    "#         X = self.norm2(X.transpose(1,2)).transpose(1,2)\n",
    "#         X2 = X0 + F.relu(X)\n",
    "#         return X2\n",
    "    \n",
    "# # Test\n",
    "# model = MultiheadLayer(H=1, n=32).to(device)\n",
    "# #E = next(iter(train_loader))[0]\n",
    "# #X = embeddings[E].transpose(2,1)\n",
    "# # row wise\n",
    "# # X = torch.randn(1,5, 32).to(device)\n",
    "# # column wise\n",
    "# X_tilde = torch.randn(1,5, 32).to(device).transpose(1,2)\n",
    "# print(f\"X.shape: {X_tilde.shape}\")\n",
    "# X_tilde = X_tilde.to(device)\n",
    "# out = model(X_tilde)\n",
    "\n",
    "# print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54ca21",
   "metadata": {},
   "source": [
    "## Final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30f37eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5974 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m LLM(L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n\u001b[38;5;241m=\u001b[39mn)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m initial_indices \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;241m132\u001b[39m:\u001b[38;5;241m132\u001b[39m\u001b[38;5;241m+\u001b[39mT]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# generated_text = generate(model, initial, max_generate_tokens=100)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(\"\\n===INPUT===\\n\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(tokens_to_words(initial.reshape(-1).tolist()))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(\"\\n===GENERATED TEXT===\\n\")\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(generated_text)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[81], line 20\u001b[0m, in \u001b[0;36mLLM.forward\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, E):\n\u001b[1;32m     18\u001b[0m     B, T \u001b[38;5;241m=\u001b[39m E\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 20\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mE\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, n, T)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (n, T)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m+\u001b[39m P\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5974 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# class LLM(nn.Module):   \n",
    "#     def __init__(self, L, H, n):\n",
    "#         super(LLM, self).__init__()\n",
    "\n",
    "#         self.position_embedding = nn.Embedding(T, n)\n",
    "#         # this is the PCA embeddings \n",
    "#         self.token_embedding = pca_embeddings\n",
    "\n",
    "#         self.decoder_layers = nn.Sequential(*[MultiheadLayer(H, n) for _ in range(L)])\n",
    "        \n",
    "#         self.norm = nn.LayerNorm(n)\n",
    "        \n",
    "#         # column wise\n",
    "#         self.readout = nn.Parameter(torch.empty(c, n))\n",
    "#         nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "#     def forward(self, E):\n",
    "#         B, T = E.shape\n",
    "\n",
    "#         X = self.token_embedding[E].transpose(1,2) # (B, n, T)\n",
    "#         P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "#         X = X + P\n",
    "\n",
    "#         X = self.decoder_layers(X) # (B, T, n)\n",
    "#         X = self.norm(X.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "#         # column wise\n",
    "#         Y = torch.matmul(self.readout, X) # (B, T, c)\n",
    "#         # row wise\n",
    "#         # Y = torch.matmul(X, self.readout_weight) \n",
    "        \n",
    "#         return Y\n",
    "\n",
    "# # Test\n",
    "# model = LLM(L=2, H=2, n=n).to(device)\n",
    "# initial_indices = test[132:132+T].unsqueeze(0)\n",
    "# out = model(initial_indices)\n",
    "# print(f\"out.shape: {out.shape}\")\n",
    "# # generated_text = generate(model, initial, max_generate_tokens=100)\n",
    "# # print(\"\\n===INPUT===\\n\")\n",
    "# # print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# # print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9687f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician whelp advanced hawks edges Adam's Aim'd Swills Seely Mock Swills taxes betwitched morning bestrid remembered watching commissions dumbly Mock surmounts masterless jest tougher jest lordship's Mock unlick'd affray edges lessons dumbly outside Aim'd morning wanders whelp whelp taxes Swills inforced wanders lordship's Mock piteous commissions banks sir's advanced GREEN Hannibal doting curious Clare misled separation hawks inforced Alla Hannibal misled masterless curious morning destroy remembered unlick'd wanders edges Aim'd Swills lessons lids destroy jest sir's disdained banks outside visit unlick'd disdained Travelling advanced Aim'd surmounts destroy wanders misled taxes jest notched Adam's piteous unlick'd affray lids separation surmounts GREEN Travelling\n"
     ]
    }
   ],
   "source": [
    "# def generate_text(model, input_tokens, max_generate_tokens=500):\n",
    "#     \"\"\"\n",
    "#     Generate text from a model given an initial input token sequence.\n",
    "#     Args:\n",
    "#         model (nn.Module): The model to use for generation.\n",
    "#         input_tokens (torch.Tensor): The initial input token sequence.\n",
    "#         max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "#     Returns:\n",
    "#         torch.Tensor: The generated token sequence.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         context = input_tokens.clone()\n",
    "#         generated_sequence = input_tokens.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "#         for _ in range(max_generate_tokens):\n",
    "#             logits = model(context)\n",
    "            \n",
    "#             last_token_logits = logits[:,-1,:]\n",
    "#             probs = F.softmax(last_token_logits, dim=-1)\n",
    "#             next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "#             # Slide context window\n",
    "\n",
    "#             context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "#             generated_sequence.append(next_token.item())  # Add new token to generated sequence\n",
    "#         generated_words = tokens_to_words(generated_sequence)\n",
    "#         generation_string = \"\".join(generated_words)\n",
    "#         return generation_string\n",
    "# # Test generate\n",
    "# model = LLM(L=2, H=4, n=n).to(device)\n",
    "# initial_indices = test[132:132+T].unsqueeze(0)\n",
    "# print(generate_text(model, initial_indices, max_generate_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fdb7",
   "metadata": {},
   "source": [
    "## Task 13: Final training\n",
    "This final training loop has a slight difference from the ones used so far: \n",
    "We designed our Dataset to give us as targets all the next words for every word in $X$. This means we can evaluate cross entropy on the readouts of every token in the sequence,\n",
    "comparing it to every element in $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4d6255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 4.91136M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavierporras\u001b[0m (\u001b[33mese-2000\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241022_094306-dtuzpwu2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2' target=\"_blank\">Varun's version</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L = 6\n",
    "H = 8\n",
    "m=32\n",
    "n=256\n",
    "model = LanguageTransformer(m=m, n=n, L=L, H=H, c=c).to(device)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    name=\"Varun's version\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4910a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 4.91136M parameters parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "num_parameters = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {} parameters\".format(num_parameters))\n",
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for t_idx, (X_tilde, y) in enumerate(train_loader):\n",
    "        logits = model(X_tilde)\n",
    "        \n",
    "        B, c, T = logits.shape\n",
    "        \n",
    "        # Reshape logits and y to be able to evaluate cross entropy on \n",
    "        # each token in the sequence.\n",
    "        logits = logits.permute(0,2,1)\n",
    "        logits = logits.reshape(B * T, c)\n",
    "        y = y.view(B * T, -1).squeeze()\n",
    "        cross_entropy_loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        cross_entropy_loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += cross_entropy_loss.item()\n",
    "        wandb.log({\"train_loss\": cross_entropy_loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13b0a9",
   "metadata": {},
   "source": [
    "## Task 14: Final text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician? \n",
      " \n",
      " NORTHUMBERLAND: \n",
      " Believe to me, Buckingham; and see the truth of \n",
      " Is all free to friend. Here \n",
      " With age triumphs, my love still well \n",
      " The heart of my brother's my word, \n",
      " As he would put up. \n",
      " \n",
      " PERDITA: \n",
      " You will not need: \n",
      " Yet, fair upon your highness. \n",
      " \n",
      " First Citizen: \n",
      " You are too bitterly: if the letter I have he: \n",
      " That's his breast that have there have left the king: \n",
      " And\n"
     ]
    }
   ],
   "source": [
    "initial_indices = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate_text(model,initial_indices, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial_indices.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
