{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "device = \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps:0\"\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Word Embeddings\n",
    "\n",
    "A simple approach is to encode words in the index of a long vector. Formally, suppose that we are given a collection of texts that collectively contain a total of $c$ words. We then consider a set of $c$ vectors $\\mathbf{e}_i$ whose length is also $c$. These vectors have all zero entries except for the $i$th entry which we set to 1:\n",
    "\n",
    "$$\n",
    "(\\mathbf{e}_i)_i = 1 \\quad \\text{~and~} \\quad \n",
    "(\\mathbf{e}_i)_j = 0 \\text{~~for~~} i \\neq j.\n",
    "$$\n",
    "\n",
    "We use the vector $\\mathbf{e}_i$ to encode the $i$th word in the corpus.\n",
    "\n",
    "In corpuses used in practice, we have thousands of different words and anywhere between hundreds of thousands to trillions of sentences. In this lab, we work with a subset of Shakespeare's plays which contains $c = 14,295$ different words and a total of 292,072 words in the corpus. But to illustrate ideas, let us work with a corpus made up of just two quotes:\n",
    "\n",
    "> *If by your art, my dearest father, you have put the wild waters in this roar, allay them.*  \n",
    "> *Sir, are not you my father?*\n",
    "\n",
    "In this corpus, we have a total of 24 different words including 3 punctuation marks. We therefore represent the words in the corpus with $c = 24$ vectors of length $c = 24$. Proceeding in the order of the sentence, the vector $\\mathbf{e}_1 = [1, 0, \\ldots, 0]$ represents the word “If,” the vector $\\mathbf{e}_2 = [0, 1, 0, \\ldots, 0]$ represents the word “by,” and so on. The word “father” is the eighth word that appears in the sentence and is therefore represented by the vector $\\mathbf{e}_8$. This vector's value at index 8 is $(\\mathbf{e}_8)_8 = 1$ and all of its other entries are zero.\n",
    "\n",
    "When the same word appears again in the corpus, we encode it with the same vector. For example, when the word “father” appears a second time, we still encode it with the vector $\\mathbf{e}_8$. This also happens with the comma (“,”) which appears three times and is encoded with the vector $\\mathbf{e}_5$ in all three appearances, and with the words “my” and “you” that appear twice and are encoded in the vectors $\\mathbf{e}_6$ and $\\mathbf{e}_9$. So encoded, our corpus becomes:\n",
    "\n",
    "> $\\mathbf{e}_1 \\quad \\mathbf{e}_2 \\quad \\mathbf{e}_3 \\quad \\mathbf{e}_4 \\quad \\mathbf{e}_5 \\quad \\mathbf{e}_6 \\quad \\mathbf{e}_7 \\quad \\mathbf{e}_8 \\quad \\mathbf{e}_5 \\quad \\mathbf{e}_9 \\quad \\mathbf{e}_{10}$  \n",
    "> $\\mathbf{e}_{11} \\quad \\mathbf{e}_{12} \\quad \\mathbf{e}_{13} \\quad \\mathbf{e}_{14} \\quad \\mathbf{e}_{15} \\quad \\mathbf{e}_{16} \\quad \\mathbf{e}_{17} \\quad \\mathbf{e}_5$  \n",
    "> $\\mathbf{e}_{18} \\quad \\mathbf{e}_{19} \\quad \\mathbf{e}_{20}$  \n",
    "> $\\mathbf{e}_{21} \\quad \\mathbf{e}_5 \\quad \\mathbf{e}_{22} \\quad \\mathbf{e}_{23} \\quad \\mathbf{e}_9 \\quad \\mathbf{e}_6 \\quad \\mathbf{e}_8 \\quad \\mathbf{e}_{24}$\n",
    "\n",
    "This is a defilement of Shakespeare's work. However, this representation of the corpus can be processed with numerical techniques.\n",
    "\n",
    "Encoding language with these index vectors is not creative and does not work well. We discuss more interesting and useful word embeddings in the next section.\n"
   ],
   "id": "332da08226eb63ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task 1** \n",
    "\n",
    "Get the data for this lab from [dsd.seas.upenn.edu/lab6](https://dsd.seas.upenn.edu/lab6) and load it into your environment. This is a text file containing around 40,000 lines of dialogue from Shakespeare's plays. Split the text into words, defined here to include punctuation marks and line breaks. We associate words with vectors $\\mathbf{e}_i$ as in equation $(1)$. Since it would be wasteful to store vectors in which all but one entry is 0, we just store the index of the vector that represents each individual word. For example, if “father” is represented by the index vector $\\mathbf{e}_8$, we do not store $\\mathbf{e}_8$ to represent this word; we just store the index $i=8$.\n",
    "\n",
    "Implement a function that turns a word into an index and the inverse function that turns an index into a word. We recommend that you use the code that we provide for this task. It is a somewhat cumbersome and not very enlightening activity.\n"
   ],
   "id": "1548b9369ff09da4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data",
   "id": "2f9c0a7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Data Loading",
   "id": "94606dfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54,
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ],
   "id": "58d8918bcd4f0a06"
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4bb49",
   "metadata": {},
   "source": [
    "### Functions to encode and decode words into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [11164, 13785, 101, 178, 3687, 1787, 6423, 844]\n",
      "\n",
      "Recovered text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "def words_to_tokens(words):\n",
    "    \"\"\"\n",
    "    Convert a list of words to a list of tokens\n",
    "    \"\"\"\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(index_list):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to a list of words\n",
    "    \"\"\"\n",
    "    decoded = \" \".join([itos[i] for i in index_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "# Checking that the word to token and back conversion works\n",
    "sample_words = text[:36]\n",
    "token_ids = words_to_tokens(split_to_words(sample_words))\n",
    "recovered_words = tokens_to_words(token_ids)\n",
    "print(f\"Original text: {sample_words}\\n\")\n",
    "print(f\"Encoded text: {token_ids}\\n\")\n",
    "print(f\"Recovered text: {recovered_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e9b9f",
   "metadata": {},
   "source": [
    "### Converting dataset into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [11164, 13785, 101, 178, 3687, 1787, 6423, 844, 9168, 6637]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([292072])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "\n",
    "# The works of Shakespeare are now a sequence of integers representing the words in the text. Sorry, William.\n",
    "tokenized_text = torch.tensor(tokenized_text)\n",
    "tokenized_text.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cooccurrence Matrices\n",
    "\n",
    "To create richer word embeddings, we leverage the cooccurrence matrix $\\mathbf{C}$. To construct this matrix, we consider a window of length $W + 1$ and scan the corpus for joint occurrences of words $\\mathbf{e}_i$ and $\\mathbf{e}_j$. The cooccurrence $C_{ij}$ is the number of times that $\\mathbf{e}_j$ appears in a window centered at $\\mathbf{e}_i$. If we index the corpus by an index $t$ and use $\\mathbf{w}_t$ to represent the $t$th word in the corpus, we can write cooccurrences as:\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_t \\mathbb{I}(\\mathbf{w}_t = \\mathbf{e}_i) = \\sum_{u = -W/2}^{u = W/2} \\mathbb{I}(\\mathbf{w}_u = \\mathbf{e}_j),\n",
    "$$\n",
    "\n",
    "where we assume that the window is even for simplicity. In the above equation, the first indicator function $\\mathbb{I}(\\mathbf{w}_t = \\mathbf{e}_i) = 1$ only when the window is centered at $\\mathbf{w}_t$ and $\\mathbf{w}_t = \\mathbf{e}_i$. The second indicator function $\\mathbb{I}(\\mathbf{w}_u = \\mathbf{e}_j) = 1$ whenever the word $\\mathbf{e}_j$ appears in the window centered at $\\mathbf{w}_t$. Thus, the second sum counts the number of times that $\\mathbf{e}_j$ appears centered in a window centered at $\\mathbf{w}_t = \\mathbf{e}_i$. The first sum is counting the number of times that $\\mathbf{e}_i$ appears in the corpus.\n",
    "\n",
    "The cooccurrence matrix $\\mathbf{C}$ is relevant because related words tend to appear near each other, and they also tend to appear next to words that indicate their relationships. In an extensive corpus, we expect to find several cooccurrences of the words “birds” and “fly,” indicating that these two words are related. We do not expect to see many cooccurrences of “dogs” and “fly” because dogs do not fly. We also expect to see cooccurrences of the words “bird” and “albatross” and of the words “bird” and “swallow,” indicating that there is some relationship between “albatross” and “swallow.”\n",
    "\n",
    "We highlight that the cooccurrence matrix $\\mathbf{C}$ is symmetric:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{C}^T \n",
    "\\quad \\Leftrightarrow \\quad\n",
    "C_{ij} = C_{ji}\n",
    "$$\n",
    "\n",
    "This is because whenever the word $\\mathbf{e}_j$ appears in a window centered at an occurrence of the word $\\mathbf{e}_i$, these two words are less than $W/2$ words apart. This implies that the word $\\mathbf{e}_i$ must appear in a window centered at an occurrence of the word $\\mathbf{e}_j$.\n"
   ],
   "id": "d8c54bf8c67c79aa"
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "**Task 2** \n",
    "\n",
    "Compute the cooccurrence matrix for the Shakespeare corpus loaded in Task 6.1. Use a window of length $W=10$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199231/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Eigenvector Embeddings\n",
    "\n",
    "A vector $\\mathbf{v}_k$ is said to be an eigenvector of the cooccurrence matrix $\\mathbf{C}$ if there exists a constant $\\lambda_k$ such that \n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\mathbf{v}_k = \\lambda_k \\mathbf{v}_k.\n",
    "$$\n",
    "\n",
    "Eigenvectors are peculiar vectors because the matrix multiplication $\\mathbf{C} \\mathbf{e}$ yields a vector that is, in general, quite different from $\\mathbf{e}$. In the case of an eigenvector, the product $\\mathbf{C} \\mathbf{v}_k = \\lambda_k \\mathbf{v}_k$ is a simple scaling of $\\mathbf{v}_k$. All of the components of $\\mathbf{v}_k$ are multiplied by the same number.\n",
    "\n",
    "It is known that the symmetric matrix $\\mathbf{C} \\in \\mathbb{R}^{c \\times c}$ has $c$ distinct eigenvectors. It is customary to order the corresponding $c$ eigenvalues from largest to smallest so that $\\lambda_k \\geq \\lambda_\\ell$ when $k < \\ell$. Since eigenvector $\\mathbf{v}_k$ is associated with eigenvalue $\\lambda_k$, the eigenvectors inherit this order. When $k < \\ell$, eigenvector $\\mathbf{v}_k$ is associated with an eigenvalue that is not smaller than the eigenvalue associated with eigenvector $\\mathbf{v}_\\ell$ — it is most often larger. We will say that eigenvector $\\mathbf{v}_k$ is not smaller than eigenvector $\\mathbf{v}_\\ell$ or that $\\mathbf{v}_k$ is larger than $\\mathbf{v}_\\ell$ if $\\lambda_k > \\lambda_\\ell$.\n",
    "\n",
    "It is also customary to group eigenvectors in the eigenvector matrix \n",
    "\n",
    "$$\n",
    "\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_{c}],\n",
    "$$\n",
    "\n",
    "in which column $k$ registers the value of eigenvector $\\mathbf{v}_k$. The eigenvector matrix is an $n \\times n$ matrix. It has $c$ columns representing $c$ distinct eigenvectors, which have $c$ rows each.\n",
    "\n",
    "We consider now a number $n \\leq c$ and define the *dimensionality reduction* matrix $\\mathbf{V}_n$ grouping the first $n$ eigenvectors of $\\mathbf{C}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_n = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_{n}].\n",
    "$$\n",
    "\n",
    "This is a tall matrix because it has $c$ rows but only $n$ columns. These columns coincide with the first $n$ columns of $\\mathbf{V}$. Instead of storing all eigenvectors, we are storing only the $n$ largest eigenvectors of $\\mathbf{C}$.\n",
    "\n",
    "We use the dimensionality reduction matrix $\\mathbf{V}_n$ to construct representations of vectors $\\mathbf{e} \\in \\mathbb{R}^{c}$ in a space of dimensionality $n$. These representations are\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{V}_n^T \\mathbf{e}.\n",
    "$$\n",
    "\n",
    "We say that this is a dimensionality reduction because $\\mathbf{x} \\in \\mathbb{R}^{n}$ is a vector with $n$ components, which is (much) smaller than the number of components $c$ of the vector $\\mathbf{e} \\in \\mathbb{R}^{c}$.\n",
    "\n",
    "We use dimensionality reduction to compute word embeddings. Given the collection of words $\\mathbf{e}_i$, we transform them into the collection of embeddings\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = \\mathbf{V}_n^T \\mathbf{e}_i.\n",
    "$$\n",
    "\n",
    "Representations $\\mathbf{x}_i$ are preferable to representations $\\mathbf{e}_i$ because they have smaller dimensionality. They also turn out to capture some semantic properties in the sense that vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$ that are close represent similar words. This is different from the index embeddings $\\mathbf{e}_i$ in which comparisons between different vectors $\\mathbf{e}_i$ and $\\mathbf{e}_j$ have no meaning."
   ],
   "id": "aebbb4db0fa212de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Task 3** \n",
    "\n",
    "Compute the first $n = 256$ eigenvectors of the cooccurrence matrix computed in Task 6.2. Use these eigenvectors to compute the eigenvector embeddings of all of the $c$ words in the corpus loaded in Task 6.1. Store the corpus using these eigenvector embeddings. This is the time series with which we will work in subsequent tasks."
   ],
   "id": "f0dc7ca82d2bf4d1"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199231/3551890280.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pca_embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([14295, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=1, keepdim=True)\n",
    "#     Z /= Z.std(dim=1, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     pca_embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full pca_embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "pca_embeddings = torch.load(\"embeddings.pt\").to(device)\n",
    "pca_embeddings.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The principal component analysis (PCA) transform of a vector $\\mathbf{e}$ is its projection in the eigenvector space of $\\mathbf{C}$, \n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{V}^T \\mathbf{e}.\n",
    "$$\n",
    "\n",
    "This is similar to the dimensionality reduction operation in \n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{V}_n^T \\mathbf{e} \n",
    "$$\n",
    "\n",
    "except that we are using all $c$ eigenvectors instead of the largest $n$.\n",
    "\n",
    "The PCA representation has the important property that it can be undone by multiplication with the eigenvector matrix. I.e., given the PCA transform $\\mathbf{y}$, we can recover the original vector $\\mathbf{e}$ as\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\mathbf{V} \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "The combination of the PCA transform and its inverse indicates that $\\mathbf{e}$ and $\\mathbf{y}$ are equivalent representations of the same information. Given $\\mathbf{e}$ we can compute $\\mathbf{y}$ and given $\\mathbf{y}$ we can compute $\\mathbf{e}$.\n",
    "\n",
    "The same is not true of the dimensionality reduction transformation. When going from $\\mathbf{e}$ to $\\mathbf{x}$ we lose information precisely because we are reducing dimensionality. In this context, it is interesting to implement the dimensionality recovery operation,\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{e}} = \\mathbf{V}_n \\mathbf{x} = \\mathbf{V}_n \\left( \\mathbf{V}_n^T \\mathbf{e} \\right),\n",
    "$$\n",
    "\n",
    "and ask how close the recovered vector $\\mathbf{e}$ is to the original $\\tilde{\\mathbf{e}}$. The answer is that for word vectors $\\mathbf{e}_i$, the error is small. That is, for most word vectors,\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{e}}_i = \\mathbf{V}_n \\mathbf{x}_i = \\mathbf{V}_n \\left( \\mathbf{V}_n^T \\mathbf{e}_i \\right) \\approx \\mathbf{e}_i.\n",
    "$$\n",
    "\n"
   ],
   "id": "8c95ca8c83e9fc81"
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "## Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "### Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=8192\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "### Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Top k eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "### Co ocurrence matrix plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [],
   "source": [
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for t_idx, (E, y) in enumerate(test_loader):\n",
    "#         logits = model(E)\n",
    "#         B, _ = logits.shape\n",
    "#         loss = F.cross_entropy(logits, y)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "# print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [],
   "source": [
    "# initial = test[132:132+T].unsqueeze(0)\n",
    "# generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Language Transformers\n",
    "We use here a softmax attention transformer with multiple heads to process language sequences. For reference, a transformer with multiple heads is defined by the recursion,\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_\\ell^h = \\text{sm} \\left( (\\mathbf{Q}^h_\\ell \\mathbf{X}_{\\ell-1})^T (\\mathbf{K}^h_\\ell \\mathbf{X}_{\\ell-1}) \\right), \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}_\\ell^h = \\mathbf{W}_\\ell^h{}^T \\mathbf{V}^h_\\ell \\mathbf{X}_{\\ell-1} \\mathbf{A}_\\ell^h{}^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_\\ell = \\mathbf{X}_{\\ell-1} + \\sigma \\left( \\sum_{h=1}^H \\mathbf{Y}_\\ell^h \\right).\n",
    "$$\n",
    "\n",
    "The input to the transformer is a sequence of $T$ eigenvector word embeddings $\\mathbf{X}_0 = \\mathbf{X}$ and its output $\\mathbf{X}_L = \\Phi (\\mathbf{X}, \\mathcal{A})$ is another sequence of $T$ eigenvector word embeddings. The trainable tensor $\\mathcal{A} = \\{\\mathbf{Q}^h_\\ell, \\mathbf{K}^h_\\ell, \\mathbf{V}^h_\\ell, \\mathbf{W}^h_\\ell\\}$ contains all of the query, key, value, and dimension recovery matrices of all heads and layers. We use the output sequence to predict the next word, $\\mathbf{x}_T$, in the sequence in Section \\ref{sec_language_readout}.\n",
    "\n",
    "Equation \n",
    "\n",
    "$$\n",
    "\\mathbf{A}_\\ell^h = \\text{sm} \\left( (\\mathbf{Q}^h_\\ell \\mathbf{X}_{\\ell-1})^T (\\mathbf{K}^h_\\ell \\mathbf{X}_{\\ell-1}) \\right)\n",
    "$$\n",
    "\n",
    "is the computation of softmax attention coefficients $\\mathbf{A}_\\ell^h$ for Layer $\\ell$ and Head $h$. We use these attention coefficients to create contextual representations $\\mathbf{Y}_\\ell^h$ in \n",
    "\n",
    "$$\n",
    "\\mathbf{Y}_\\ell^h = \\mathbf{W}_\\ell^h{}^T \\mathbf{V}^h_\\ell \\mathbf{X}_{\\ell-1} \\mathbf{A}_\\ell^h{}^T.\n",
    "$$\n",
    "\n",
    "The output of Layer $\\ell$ is computed by summing all heads and passing the output through a nonlinear operation in \n",
    "\n",
    "$$\n",
    "\\mathbf{X}_\\ell = \\mathbf{X}_{\\ell-1} + \\sigma \\left( \\sum_{h=1}^H \\mathbf{Y}_\\ell^h \\right).\n",
    "$$\n",
    "\n",
    "We also add the skip connection $\\mathbf{X}_{\\ell-1}$ to the output of Layer $\\ell$ of the transformer.\n",
    "\n",
    "Recall that in the attention and representation equations we create the intermediate representations $\\mathbf{Q}^h_\\ell \\mathbf{X}_{\\ell-1}$ (queries), $\\mathbf{K}^h_\\ell \\mathbf{X}_{\\ell-1}$ (keys), and $\\mathbf{V}^h_\\ell \\mathbf{X}_{\\ell-1}$ (values) which are of dimension $m \\ll n$. In this lab and in language models in general, the reduction of dimension is aggressive. We have here that $n = 256$ at the input and choose $m = 2$ for intermediate representations."
   ],
   "id": "2d97d0aeb7585ac9"
  },
  {
   "cell_type": "markdown",
   "id": "33c68a8c",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "Code a Pytorch module to implement the language transformer as specified by $\\text{(13)-(15)}$.\n",
    "This transformer takes sequences of length $T$ and dimension $n$ as inputs and produces sequences of length $T$ and dimension $n$ as outputs. Make the number of layers $L$ and the number of heads $H$ parameters of the transformer. Queries, keys, and values are of dimension $m$, which is also a parameter of the transformer. Use ReLU nonlinearities at each layer.\n",
    "\n",
    "This is the same transformer of Lab 5. It is a copy and paste task. That the time series represents language is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dca834",
   "metadata": {},
   "source": [
    "### MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ea7ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "# This is the same as the MultiheadLayer in the lab 6 notebook. It corresponds to the equations in Section 3 of this lab's writeup.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "    \n",
    "    \n",
    "model = MultiHeadLayer(m=32, n=256, H=8).to(device)\n",
    "X_tilde = torch.randn(1,256,64).to(device)\n",
    "out = model(X_tilde)\n",
    "\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52a20d",
   "metadata": {},
   "source": [
    "### LanguageTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6529c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Mutlihead Transformer, analogous to the Transformer class, in the single head case.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "        # Word embedding table. This is the only change from the previous lab's code. We have \n",
    "        # PCA embeddings to convert word indices to embeddings.\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead transformer, stacks L multihead layers.\n",
    "        This class is essentially the same as the Transformer class, but using the \n",
    "        MultiHeadLayer class instead of the AttentionLayer class.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            X_L^{T-1} (torch.Tensor): The last vector of the output of the transformer.\n",
    "        \"\"\"\n",
    "        # Convert word indices to embeddings. We need to transpose the result to get the shape (B, n, T).\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "        B, n, T = X.shape\n",
    "\n",
    "        # Compute the mean token to append to the sequence.\n",
    "        X_tilde = X.mean(dim=2, keepdim=True) # mean over the time dimension\n",
    "        X_tilde = torch.cat((X, X_tilde), dim=-1)\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "        \n",
    "        # Output the last vector.\n",
    "        return X_l[:,:,-1]\n",
    "\n",
    "# Test\n",
    "model = LanguageTransformer(L=2, H=2, m=32, n=256).to(device)\n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "out = model(E)\n",
    "print(f\"output.shape: {out.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Next Word Prediction\n",
    "\n",
    "To predict word $\\mathbf{x}_T$, we read the output $\\mathbf{X}_L = \\Phi (\\mathbf{X}, \\mathcal{A})$ of the transformer. A possible approach is to take the average across time. To set up this readout strategy, let $\\mathbf{X}_u$ denote a sequence of $T$ words—in the form of eigenvector embeddings—starting at time $u$,\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_u = \\big[\\, \\mathbf{x}_u, \\mathbf{x}_{u+1}, \\ldots, \\mathbf{x}_{T + u -1} \\,\\big] = \\mathbf{x}_{u: u + T-1} .\n",
    "$$\n",
    "\n",
    "This is a recorded history of the language sequence. Our goal is to predict the next word $\\mathbf{x}_{u+T}$ using this recorded history. We do that using the average of the output of the transformer,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_{u+T} =  \\Big[ \\, \\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\Big] \\mathbf{1}.\n",
    "$$\n",
    "\n",
    "We then train the tensor $\\mathcal{A} = \\{\\mathbf{Q}^h_\\ell, \\mathbf{K}^h_\\ell, \\mathbf{V}^h_\\ell, \\mathbf{W}^h_\\ell\\}$ to maximize prediction accuracy over the corpus. Utilizing a mean squared error (MSE), the prediction task reduces to\n",
    "\n",
    "$$\n",
    "\\mathcal{A}^* = \\arg\\min_\\mathcal{A ~} \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\Big\\| \\, \\Phi \\big(\\, \\mathbf{X}_{u}, \\, \\mathcal{A} \\, \\big) \\mathbf{1} - \\mathbf{x}_{u+T} \\,\\Big \\|^2 \\, .\n",
    "$$\n",
    "\n",
    "In this equation, we compare the estimate $\\hat{\\mathbf{x}}_{u+T}$ read out from the transformer's output with the true next word $\\mathbf{x}_{u+T}$. We average the resulting MSE loss over the corpus and seek the tensor $\\mathcal{A}^*$ that minimizes it. Notice that to simplify notation, we sum over the whole corpus. In reality, we can't predict the last $T$ words because we are using histories $\\mathbf{X}_u$ of length $T$. In fact, we have several other limitations in the construction of the training dataset. We may, e.g., want to avoid running over the end of a play or the end of an act. We choose to ignore these practicalities as they have little effect."
   ],
   "id": "307b3af066e53750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5\n",
    "\n",
    "Split the corpus loaded in Task 6.3 into a training set containing 90% of the total number of words and a test set containing 10% of the words. Recall that this is a time series of word embeddings. Use this training set to train a transformer that predicts the next word embedding using the loss in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^* = \\arg\\min_\\mathcal{A ~} \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\Big\\| \\, \\Phi \\big(\\, \\mathbf{X}_{u}, \\, \\mathcal{A} \\, \\big) \\mathbf{1} - \\mathbf{x}_{u+T} \\,\\Big \\|^2 \\, .\n",
    "$$\n",
    "\n",
    "Use $ T = 32 $ for the length of the history $ \\mathbf{X}_u $. Transformer parameters are your choice. If you need a recommendation, use $ L = 6 $, $ H = 8 $, and $ m = 64 $.\n",
    "\n",
    "Evaluate the test MSE and compare it to the train MSE. Both of these MSE values indicate good prediction. However, this does not mean that we are making good predictions of the next word in the sequence. Explain."
   ],
   "id": "ff802eead6b88047"
  },
  {
   "cell_type": "markdown",
   "id": "d05b2798",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c28745b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "    \n",
    "# Splitting into train and test sets\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca499d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c22c604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "class WordIndexDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Dataset class takes and encoded tensor of word indices and returns a tensor of context windows of size T.\n",
    "    The tensors returned by this dataset are not yet one-hot encoded.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single context window of size T. \n",
    "        The context window is a sequence of T words.\n",
    "\n",
    "        During training, we will predict the next token of every word in the context window,\n",
    "        so Y_item is the next word for every word in the context window.\n",
    "        \"\"\"\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = WordIndexDataset(train, T)\n",
    "test_dataset = WordIndexDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4d27124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jporras/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([256, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 13/4107 [00:03<19:07,  3.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 33\u001B[0m\n\u001B[1;32m     29\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad() \u001B[38;5;66;03m# Gradient reset to indicate where the backward computation stops.\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Call the neural network. In this case, we will take the average of the output of the\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# transformer as the prediction.\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mestimator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     34\u001B[0m cross_entropy_value \u001B[38;5;241m=\u001B[39m cross_entropy_loss(y_hat,Y_embeddings)\n\u001B[1;32m     36\u001B[0m cross_entropy_value\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# Compute gradients moving backwards untit the gradient reset.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[63], line 42\u001B[0m, in \u001B[0;36mLanguageTransformer.forward\u001B[0;34m(self, E)\u001B[0m\n\u001B[1;32m     40\u001B[0m X_l \u001B[38;5;241m=\u001B[39m X_tilde\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 42\u001B[0m     X_l \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_l\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Output the last vector.\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_l[:,:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[62], line 54\u001B[0m, in \u001B[0;36mMultiHeadLayer.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Compute QX, KX, VX for each head\u001B[39;00m\n\u001B[1;32m     53\u001B[0m QX \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQ\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), X_expanded)  \u001B[38;5;66;03m# (B, H, m, T)\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m KX \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mK\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_expanded\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (B, H, m, T)\u001B[39;00m\n\u001B[1;32m     55\u001B[0m VX \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mV\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), X_expanded)  \u001B[38;5;66;03m# (B, H, m, T)\u001B[39;00m\n\u001B[1;32m     57\u001B[0m QX_t \u001B[38;5;241m=\u001B[39m QX\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (B, H, T, m)\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_epochs = 10\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = T\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformer(m, n, L, H).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "cross_entropy_loss = nn.MSELoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # Load the embeddings for the target word\n",
    "        # We want to predict the last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        Y_embeddings = pca_embeddings[y_word_to_predict].transpose(0,1).to(device) # (B, n)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch).mean(dim=-1)\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,Y_embeddings)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "print(train_loss[-1]) # Print training loss."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Probability Readout\n",
    "\n",
    "The predictions $\\hat{\\mathbf{x}}_{u+T}$ in \n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_{u+T} = \\Big[ \\, \\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\Big] \\mathbf{1}\n",
    "$$ \n",
    "\n",
    "may have a small MSE when compared to the observed words $\\mathbf{x}_{u+T}$ but they are not a good strategy for estimating the next word. This is because $\\hat{\\mathbf{x}}_{T}$ need not be a valid word. Indeed, it most likely will not be a valid word.\n",
    "\n",
    "Word $\\mathbf{e}_i$ is represented by the eigenvector encoding $\\mathbf{x}_i = \\mathbf{V}_n^T \\mathbf{e}_i$ as stated in \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = \\mathbf{V}_n^T \\mathbf{e}_i.\n",
    "$$ \n",
    "\n",
    "Since there are a total of $c$ words in our corpus, there are a total of $c$ vectors $\\mathbf{x}_i$ that represent valid words. The vectors at the output of the transformer are most unlikely to be one of these vectors, and the estimate $\\hat{\\mathbf{x}}_{T}$ is just as unlikely unless we manage to drive the train and test MSEs to zero.\n",
    "\n",
    "To solve this problem, we must force the readout to be a valid word. We do that with a readout layer whose output is a vector of $\\tilde{d}_n$ probabilities for each of the $\\tilde{d}_n$ words in the corpus. This readout layer is a softmax applied to the output of a fully connected layer that acts on the output of the transformer,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi} (\\mathbf{X}) = \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big( \\Phi (\\mathbf{X}, \\mathcal{A})\\big) \\, \\Big] .\n",
    "$$ \n",
    "\n",
    "The matrix $\\mathbf{A}$ is a trainable parameter with $nT$ columns and $c$ rows. After applying the softmax normalization, the entries of the output $\\boldsymbol{\\pi}(\\mathbf{X})$ add up to one and can be interpreted as a set of probabilities that dictate the likelihood of the next word in the sequence. The $i$th entry $\\boldsymbol{\\pi}_i(\\mathbf{X})$ is the predicted probability that the next word is $\\mathbf{e}_i$.\n",
    "\n",
    "We refer to the probabilities in \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi} (\\mathbf{X}) \n",
    "$$ \n",
    "\n",
    "as a policy. To train this policy, we minimize the cross-entropy loss between the true word at time $u+T$ and the probabilities $\\boldsymbol{\\pi}(\\mathbf{X})$,\n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\big(\\mathbf{e}_{u+T}\\big)^T \\big( \\log \\boldsymbol{\\pi}(\\mathbf{X}_u) \\big) .\n",
    "$$ \n",
    "\n",
    "Notice that in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\big(\\mathbf{e}_{u+T}\\big)^T \\big( \\log \\boldsymbol{\\pi}(\\mathbf{X}_u) \\big) .\n",
    "$$ \n",
    "\n",
    "the vector $\\mathbf{e}_{u+T}$ is the index encoding of the word at time $u+T$. This is a vector with all zeros except that it has a 1 at the entry that corresponds to the index of the word that is observed at time $u+T$. It is therefore a valid probability index that we can incorporate into a cross-entropy comparison.\n",
    "\n",
    "Further notice that the optimization is joint over the trainable parameters $\\mathcal{A}$ of the transformer and the readout matrix $\\mathbf{A}$. These two parameters are implicit in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\big(\\mathbf{e}_{u+T}\\big)^T \\big( \\log \\boldsymbol{\\pi}(\\mathbf{X}_u) \\big) .\n",
    "$$ \n",
    "\n",
    "They appear because $\\boldsymbol{\\pi} (\\mathbf{X}_u)$ depends on $\\mathbf{A}$ and $\\mathcal{A}$. In the hope that it is revealing to make this dependence explicit, we instantiate $\\mathbf{X} = \\mathbf{X}_u$ in \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi} (\\mathbf{X}) = \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big( \\Phi (\\mathbf{X}, \\mathcal{A})\\big) \\, \\Big]\n",
    "$$ \n",
    "\n",
    "and substitute the result in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\big(\\mathbf{e}_{u+T}\\big)^T \\big( \\log \\boldsymbol{\\pi}(\\mathbf{X}_u) \\big) .\n",
    "$$ \n",
    "\n",
    "to write\n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\Big[\\mathbf{e}_{u+T}\\Big]^T \n",
    "\\bigg[ \\log \\text{sm} \n",
    "\\Big[\\, \\mathbf{A} \\, \n",
    "\\text{vec} \\big(\\, \n",
    "\\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\big) \\, \\Big]\\, \\bigg] .\n",
    "$$ \n",
    "\n",
    "We solve this empirical risk minimization (ERM) problem to predict the next word in a sequence of text. This prediction is based on observing a history of length $T$ that is processed by a transformer\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_\\ell = \\mathbf{X}_{\\ell-1} + \\sigma\\bigg( \\sum_{h=1}^H \\mathbf{Y}_\\ell^h  \\,\\bigg) .\n",
    "$$ \n",
    "\n",
    "with a probability readout layer \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi} (\\mathbf{X}) = \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big( \\Phi (\\mathbf{X}, \\mathcal{A})\\big) \\, \\Big] .\n",
    "$$ \n",
    "\n",
    "Different from the readout strategy in \n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_{u+T} = \\Big[ \\, \\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\Big] \\mathbf{1} \n",
    "$$ \n",
    "\n",
    "and the training procedure in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^* = \\arg\\min_{\\mathcal{A}} \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\Big\\| \\, \\Phi \\big(\\, \\mathbf{X}_{u}, \\, \\mathcal{A} \\, \\big) \\mathbf{1} - \\mathbf{x}_{u+T} \\,\\Big \\|^2 \\, .\n",
    "$$ \n",
    "\n",
    "the ERM problem in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A},\\, \\mathbf{A}} ~ \\frac{1}{C}~\\sum_{u=0}^{C-1} ~ \\big(\\mathbf{e}_{u+T}\\big)^T \\big( \\log \\boldsymbol{\\pi}(\\mathbf{X}_u) \\big) .\n",
    "$$ \n",
    "\n",
    "produces parameters $\\mathcal{A}^*$ and $\\mathbf{A}^*$ that map directly to predictions of actual words."
   ],
   "id": "4f42b6b1bc429b1"
  },
  {
   "cell_type": "markdown",
   "id": "3a84e2b8",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Modify the transformer of Task 6.4 to add the readout layer in \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi} (\\mathbf{X}) = \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big( \\Phi (\\mathbf{X}, \\mathcal{A})\\big) \\, \\Big].\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae465fd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:54:58.831221Z",
     "start_time": "2024-10-22T14:54:58.748010Z"
    }
   },
   "source": [
    "class LanguageTransformerWithReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    A slight modification of the LanguageTransformer class of Task 4.\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n=12 in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        L (int): The number of layers.\n",
    "        H (int): The number of heads.\n",
    "        c (int): The vocabulary size.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadout, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        X = self.embedding_table[E].transpose(1,2)\n",
    "\n",
    "        B, n, T = X.shape\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "28be29cd",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Split the corpuses loaded in Task 6.1 and Task 6.3 into a training set containing 90% of the total number of words and a test set containing 10% of the words. Recall that these two are equivalent time series except that the information is encoded differently. In Task 6.1, we store words using index encodings, and in Task 6.3, we store words using eigenvector embeddings. We are loading both here because the eigenvector encodings are the input to the transformer, and the index encodings are needed for the crossentropy comparison in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A}, \\, \\mathbf{A}} \\frac{1}{C} \\sum_{u=0}^{C-1} \\Big[\\mathbf{e}_{u+T}\\Big]^T \\bigg[ \\log \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big(\\, \\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\big) \\, \\Big]\\, \\bigg].\n",
    "$$ \n",
    "\n",
    "Make sure that time indexes match in your data.\n",
    "\n",
    "Use the training set to train a transformer that predicts next word probabilities using the transformer with readout of Task 6.6. Use $T=32$ for the length of the history $\\mathbf{X}_u$. Transformer parameters are your choice. If you need a recommendation, use $L=??$, $H=??$, and $m=??$.\n",
    "\n",
    "Evaluate the crossentropy loss in the test set and compare it to the crossentropy loss in the training set."
   ]
  },
  {
   "cell_type": "code",
   "id": "f05b002a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:57:56.920255Z",
     "start_time": "2024-10-22T14:57:56.827176Z"
    }
   },
   "source": [
    "# Training\n",
    "n_epochs = 5\n",
    "m = 32\n",
    "n = 256\n",
    "L = 6\n",
    "T = T\n",
    "H = 8\n",
    "\n",
    "estimator = LanguageTransformerWithReadout(m, n, L, H, c).float().to(device)\n",
    "optimizer = torch.optim.SGD(estimator.parameters(), lr=1e-5)\n",
    "\n",
    "# We use the Cross Entropy loss for estimating the probabilities of the next word.\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "estimator.train()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(n_epochs): # Iterate over n_epochs epochs\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_loader): # Iterate over all batches in the dataset \n",
    "        # We want to predict tha last word of the context window for this exercise.\n",
    "        y_word_to_predict = y_batch[:,-1]\n",
    "        # Load the embeddings for the words.\n",
    "        X_embeddings = pca_embeddings[x_batch].transpose(1,2) # (B, n, T)\n",
    "        \n",
    "        # (Step i) Load the data. These commands send the data to the GPU memory.\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # (Step ii) Compute the gradients. We use automated differentiation.\n",
    "        optimizer.zero_grad() # Gradient reset to indicate where the backward computation stops.\n",
    "\n",
    "        # Call the neural network. In this case, we will take the average of the output of the\n",
    "        # transformer as the prediction.\n",
    "        y_hat = estimator(x_batch)\n",
    "\n",
    "        # When using cross entropy loss, we need to pass the target as a 1D tensor of class indices.\n",
    "        # The softmax function is applied internally to the transformer's output y_hat.\n",
    "        cross_entropy_value = cross_entropy_loss(y_hat,y_batch)\n",
    "\n",
    "        cross_entropy_value.backward() # Compute gradients moving backwards untit the gradient reset.\n",
    "\n",
    "        # (Step iii) Update parameters by taking an SGD (or other optimizer) step.\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(cross_entropy_value.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} Loss: {train_loss[-1]}\")\n",
    "\n",
    "    # End of batch loop.\n",
    "\n",
    "print(train_loss[-1]) # Print training loss."
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m\n\u001B[1;32m      5\u001B[0m L \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m6\u001B[39m\n\u001B[0;32m----> 6\u001B[0m T \u001B[38;5;241m=\u001B[39m \u001B[43mT\u001B[49m\n\u001B[1;32m      7\u001B[0m H \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n\u001B[1;32m      9\u001B[0m estimator \u001B[38;5;241m=\u001B[39m LanguageTransformerWithReadout(m, n, L, H, c)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'T' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Sampling\n",
    "\n",
    "After solving the empirical risk minimization (ERM) problem in \n",
    "\n",
    "$$\n",
    "\\mathcal{A}^*, \\mathbf{A}^* = \\arg\\min_{\\mathcal{A}, \\, \\mathbf{A}} \\frac{1}{C} \\sum_{u=0}^{C-1} \\Big[\\mathbf{e}_{u+T}\\Big]^T \\bigg[ \\log \\text{sm} \\Big[\\, \\mathbf{A} \\, \\text{vec} \\big(\\, \\Phi (\\mathbf{X}_u, \\mathcal{A}) \\, \\big) \\, \\Big]\\, \\bigg],\n",
    "$$ \n",
    "\n",
    "we have trained values $\\mathcal{A}^*$ for the transformer and $\\mathbf{A}^*$ for the probability readout layer. With these trained values, we can execute \n",
    "\n",
    "$$\n",
    "\\mathbf{\\pi}^* (\\mathbf{X}) = \\text{sm} \\Big[\\, \\mathbf{A}^* \\, \\text{vec} \\big( \\Phi (\\mathbf{X}, \\mathcal{A}^*)\\big) \\, \\Big]\n",
    "$$ \n",
    "\n",
    "for any given text sequence $\\mathbf{X}$ of length $T$. The result is the (optimal) vector of probabilities.\n",
    "\n",
    "This is not yet a word; it is a vector of probabilities that assigns probabilities to each of the $c$ words in the corpus. To generate a word, we need to implement a *sampling* strategy.\n",
    "\n",
    "Let us denote as $\\pi^* (\\mathbf{e}_i | \\mathbf{X})$ the probability of choosing word $\\mathbf{e}_i$. This is the $i$th entry of the vector of probabilities $\\mathbf{\\pi}$. A possible sampling strategy is to sample the word $\\mathbf{e}_i$ with the highest probability:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{e}} = \\arg\\max_{\\mathbf{e}_i} \\pi^* (\\mathbf{e}_i | \\mathbf{X}).\n",
    "$$ \n",
    "\n",
    "Alternatively, we can sample predictions randomly by choosing different words according to their corresponding probabilities. We write\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{e}} = \\mathbf{e}_i \\sim \\pi^* (\\mathbf{e}_i | \\mathbf{X})\n",
    "$$ \n",
    "\n",
    "to signify that we choose $\\hat{\\mathbf{e}} = \\mathbf{e}_i$ with probability $\\pi^* (\\mathbf{e}_i | \\mathbf{X})$.\n",
    "\n",
    "Sampling according to the largest probability (as in the first equation) is a good strategy if we want to predict the next word in the sequence. However, sampling randomly according to word probabilities (as in the second equation) is a better strategy for generating text. Random sampling better imitates the natural variability of human language, and we will use random sampling."
   ],
   "id": "c805494e18d14d66"
  },
  {
   "cell_type": "markdown",
   "id": "e2bf06d6",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Given trained parameters $\\mathcal{A}^*$ and $\\mathbf{A}^*$, implement the following:\n",
    "\n",
    "(a) A transformer with parameters \\$(\\mathcal{A}^*$ that takes language sequences $\\mathbf{X}$ of length $T$ as inputs.\n",
    "\n",
    "(b) A readout layer that postprocesses the output of the transformer to yield a vector of probabilities $\\mathbf{\\pi}^* (\\mathbf{X})$.\n",
    "\n",
    "(c) A sampler that takes probabilities $\\mathbf{\\pi}^*(\\mathbf{X})$ as inputs and returns words $\\hat{\\mathbf{e}}$ sampled according to \n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{e}} = \\mathbf{e}_i \\sim \\pi^* (\\mathbf{e}_i | \\mathbf{X}).\n",
    "$$ \n",
    "\n",
    "The transformer and readout implementations are just instances of the transformer and readout modules from Tasks 6.4 and 6.6. The only new piece here is the sampler.\n",
    "\n",
    "Try your sampler for a few input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42322234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probabilities.shape: torch.Size([1, 14295, 64])\n",
      "Input text: afeard of you. \n",
      " \n",
      " Widow: \n",
      " He that is giddy thinks the world turns round. \n",
      " \n",
      " PETRUCHIO: \n",
      " Roundly replied. \n",
      " \n",
      " KATHARINA: \n",
      " Mistress, how mean you that? \n",
      " \n",
      " Widow: \n",
      " Thus I conceive by him. \n",
      " \n",
      " PETRUCHIO: \n",
      " Conceives by me! How likes Hortensio that?\n",
      "\n",
      "The most likely next word is: Blunt\n",
      "\n",
      "Sampled words according to a multinomial distribution (either could be the next word when using sampling):\n",
      "warning cuff vilely foulest building keep'st starveth chose assist nails "
     ]
    }
   ],
   "source": [
    "# Taking a snippet of the text set to test the model.\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "\n",
    "log_probabilities = model(initial_indices)\n",
    "print(f\"log_probabilities.shape: {log_probabilities.shape}\")\n",
    "last_word_probabilities = log_probabilities[:,:,-1]\n",
    "probabilities = F.softmax(last_word_probabilities, dim=-2)\n",
    "\n",
    "print(f\"Input text: {tokens_to_words(initial_indices.reshape(-1).tolist())}\")\n",
    "print(f\"\\nThe most likely next word is: {tokens_to_words([torch.argmax(probabilities).item()])}\")\n",
    "\n",
    "\n",
    "print(\"\\nSampled words according to a multinomial distribution (either could be the next word when using sampling):\")\n",
    "for _ in range(10):\n",
    "    sampled_word = torch.multinomial(probabilities, num_samples=1).item()\n",
    "    print(f\"{tokens_to_words([sampled_word])}\", end=\" \")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Language Generation\n",
    "\n",
    "In the Language Generation section we adopted a transformer to predict the next word of a sequence of length $ T $. We adapt this model to language generation with a rolling execution.\n",
    "\n",
    "Begin with a language sequence entered by a user, which we call a prompt. From the prompt we construct a time series $ \\mathbf{X}_0 $ with the eigenvector encodings of its words\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_0 = [\\mathbf{x}_0, \\ldots, \\mathbf{x}_{T-1}].\n",
    "$$\n",
    "\n",
    "We assume, for simplicity, that this prompt has length $ T $. Using this prompt we predict the next word in the sequence using the policy $ \\boldsymbol{\\pi}^* $,\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_T \\sim \\boldsymbol{\\pi}^*(\\mathbf{X}_0).\n",
    "$$\n",
    "\n",
    "Although the input $ \\mathbf{x}_T $ has been *generated* by the policy $ \\boldsymbol{\\pi}^* $, we reinterpret it as a *given* word. We then roll the prompt backward and append the generated word $ \\mathbf{x}_T $ to construct the series\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_1 = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_{T-1}, \\mathbf{x}_{T}].\n",
    "$$\n",
    "\n",
    "In this sequence the first $ T-1 $ entries are part of the user prompt. The last one, $ \\mathbf{x}_{T} $, has been generated. We ignore this distinction and proceed to estimate word $ T+1 $ as \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{T+1} \\sim \\boldsymbol{\\pi}^*(\\mathbf{X}_1).\n",
    "$$\n",
    "\n",
    "We then proceed to append this generated word to the time series in the previous equation and roll the series backward. This procedure yields the time series,\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_2 = [\\mathbf{x}_2, \\ldots, \\mathbf{x}_{T-1}, \\mathbf{x}_{T}, \\mathbf{x}_{T+1}].\n",
    "$$\n",
    "\n",
    "In this time series we have the last $ T-2 $ words of the user prompt and two words that have been generated by policy $ \\boldsymbol{\\pi}^* $. These are the words $ \\mathbf{x}_{T} $ and $ \\mathbf{x}_{T+1} $ generated in the previous equations. We again ignore this distinction and generate the next word as,\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{T+2} \\sim \\boldsymbol{\\pi}^*(\\mathbf{X}_2).\n",
    "$$\n",
    "\n",
    "We append word $ \\mathbf{x}_{T+2} $ to the time series, roll the time series backward, and use the updated series to predict the next word in the sequence. In general, at generative step $ u $ we take as an input the time series\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_u = [\\mathbf{x}_u, \\ldots, \\mathbf{x}_{T-1+u}],\n",
    "$$\n",
    "\n",
    "in which the last $ u $ samples have been generated—it can be that all of the samples are generated if $ u \\geq T $. From this time series we generate the word in position $ T+u $ as,\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{T+u} \\sim \\boldsymbol{\\pi}^*(\\mathbf{X}_u).\n",
    "$$\n",
    "\n",
    "The output of the generative language model is the string of text $[\\mathbf{x}_T, \\ldots, \\mathbf{x}_{T_{\\max}}]$ where $ T_{\\max} $ is a prespecified limit for the length of the language sequence to be generated. Of course, rather than returning the eigenvector embeddings $[\\mathbf{x}_T, \\ldots, \\mathbf{x}_{T_{\\max}}]$ we return the sequence of corresponding words."
   ],
   "id": "da10b8c4d70e7fd2"
  },
  {
   "cell_type": "markdown",
   "id": "1900490a",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Implement the generative language model as specified by the recursion $\\text{(31)-(32)}$\n",
    "\n",
    "Take prompts of length $ T=64 $ as inputs and generate language sequences of length $ T_{\\max}=500 $. To make this task more interesting, modify your implementation to accept prompts of any length $ T' \\leq T $. This is not difficult because absent words in the prompt can be set to zero.\n",
    "\n",
    "Try your generative model for some prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2bc907c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== INPUT TEXT ==========\n",
      "thou shalt think on prating whilst thou livest! \n",
      " I tell thee, I, that thou hast marr'd her gown. \n",
      " \n",
      " Tailor: \n",
      " Your worship is deceived; the gown is made \n",
      " Just as my master had direction: \n",
      " Grumio gave order how it should be done. \n",
      " \n",
      " GRUMIO: \n",
      " I gave him no\n",
      "\n",
      "========== INPUT + GENERATED TEXT ==========\n",
      "thou shalt think on prating whilst thou livest! \n",
      " I tell thee, I, that thou hast marr'd her gown. \n",
      " \n",
      " Tailor: \n",
      " Your worship is deceived; the gown is made \n",
      " Just as my master had direction: \n",
      " Grumio gave order how it should be done. \n",
      " \n",
      " GRUMIO: \n",
      " I gave him no endow'd enter'd cross obscure approved minds sol buckets Wherewith paste hose ass readiest tormentors rise whistle cheated Stephen dependant suppliants Wrath assembled prisoner's attendance cries wreaths burr sternness lilies Mantua's cur stood gotten cracking valiantness rings thankless derivative holiness incorporate policy accomplish'd beaten retail Twould dogg'd gallery Whipp'd amity steely reeking fetch Tincture forceful commons history Carthage scapes unfold islands rectorship packthread womankind dispenses looking clime pipe knowest angry pack wayward We whoever enpierced tackles lungs wits round Any repent Sink Treason heedfully performed reputation empire murder'd richest gently overjoy'd prenzie pathway gross stand'st issues jour plenteous forsooth heal'd apricocks\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, X, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context = X.clone()\n",
    "        generated_sequence = X.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(context)\n",
    "            \n",
    "            last_word_embeddings = logits[:,:,-1]\n",
    "            probs = F.softmax(last_word_embeddings, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Slide context window: remove the first token and append the next token\n",
    "            context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "            generated_sequence.append(next_token.squeeze().item())  # Add new token to generated sequence\n",
    "        generated_words = tokens_to_words(generated_sequence)\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string\n",
    "\n",
    "# Test generate\n",
    "#model = LanguageTransformerWithReadout(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "starting_point = torch.randint(0, len(test)-T, (1,))\n",
    "initial_indices = test[starting_point:starting_point+T].unsqueeze(0)\n",
    "print(f\"========== INPUT TEXT ==========\")\n",
    "print(f\"{tokens_to_words(initial_indices.reshape(-1).tolist())}\\n\")\n",
    "\n",
    "# This is the model from task 7\n",
    "print(f\"========== INPUT + GENERATED TEXT ==========\")\n",
    "print(generate_text(model, initial_indices, max_generate_tokens=100))\n",
    "print(f\"====================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06d1ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be did Resign repent officers chestnut ladyship cross weeping desperation words steel'd murder'd rebukes worthiness possess'd unloads chair crushing increaseth generation comely lying shivering memory vesture stair graft bow assembled slaughter'd hospitable wooer's You're bluntly flaky piled annual conferring frank esteem carriage began Thracian esteem tending effect glove childish scoffs temper'd unfit Call'd destroyed force nymphs Likely helpless props painter length wield revenge inducement vicar mortality needed pamper horizon frequent Hercules forfend Had dad stinted Post load Humbly spotted circled Wars jay racking Tybalt frosts shoot beguile looking sorrow shoe Nathaniel solemn Turkey Twould whether Conceiving Pudding nonage presage Served wombs\n"
     ]
    }
   ],
   "source": [
    "words = \"To be or not to be\".split(\" \")\n",
    "initial_indices = torch.tensor(words_to_tokens(words)).to(device).unsqueeze(0)\n",
    "print(generate_text(model, initial_indices, max_generate_tokens=100))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding\n",
    "\n",
    "The output of a transformer is equivariant to a permutation of the entries of the time series. If we exchange the positions of $\\mathbf{x}_t$ and $\\mathbf{x}_u$, the output of the transformer remains the same, except that $[\\Phi(\\mathbf{X}_u, \\mathcal{A})]_u$ and $[\\Phi(\\mathbf{X}_t, \\mathcal{A})]_u$ also exchange places. Positional encoding is a strategy to break this symmetry so that words can have different effects depending on their positions.\n",
    "\n",
    "We use oscillations to define positional encodings. For a time series made up of vectors $\\mathbf{x}_t$ with $n$ entries, we define $n/2$ frequencies $\\alpha_i$. For each of these frequencies, we define a time series $\\mathbf{P}$ in which the values for time $t$ and index $i$ are given by\n",
    "\n",
    "$$\n",
    "p_{ti} = \n",
    "\\begin{cases}\n",
    "\\cos \\left(2\\pi\\, \\alpha_{(i+1)/2} \\, \\frac{t}{T}\\right), & \\text{if } i \\text{ is odd} \\\\\n",
    "\\sin \\left(2\\pi\\, \\alpha_{i/2} \\, \\frac{t}{T}\\right), & \\text{if } i \\text{ is even}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As per the equation above, positional encoding includes sines and cosines of different frequencies in different rows of the positional encoding time series. Odd rows of $\\mathbf{P}$ are cosines of frequency $\\alpha_{(i+1)/2}$. Even rows of $\\mathbf{P}$ are sines of frequency $\\alpha_{i/2}$.\n",
    "\n",
    "The use of sines and cosines in this context is motivated by the Fourier basis, which has intimate connections with convolution. This is a story for another day.\n",
    "\n",
    "**[Javier, please finish description. How do we add?]**"
   ],
   "id": "565193652098aed"
  },
  {
   "cell_type": "markdown",
   "id": "3bd5a3b5",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Modify the transformer of Task 6.6 to incorporate positional encoding. Use frequencies $\\alpha_i = ??$ for $i=1,2,\\ldots, n/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "068c503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([1, 5])\n",
      "out.shape: torch.Size([1, 14295, 5])\n"
     ]
    }
   ],
   "source": [
    "class LanguageTransformerWithReadoutAndPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Modification of the LanguageTransformerWithReadout class of Task 7 to include positional encoding.\n",
    "    Positional encoding is a learnable matrix that is added to the embeddings of the input tokens.\n",
    "    \n",
    "    Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformerWithReadoutAndPositionalEncoding, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # Learnable parameters for positional encoding. \n",
    "        # Each entry in the positional encoding matrix is is a vector of size n that represents a position in the sequence.\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        self.embedding_table = pca_embeddings\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        We change the forward pass from the previous Transformer.\n",
    "        Instead of concatenating a vector to the sequence, we now output a vector of probabilities for each word in the sequence.\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        # Adding word embeddings and positional encoding\n",
    "        # Although P is (n,T), this is broadcasted to (B, n, T), which means that the same \n",
    "        # positional encoding is added to every sequence in the batch.\n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        # Notice, we don't apply the softmax here, because we keep the probabilities unnormalized until \n",
    "        # we call the loss function, for numerical stability.\n",
    "        return Y_hat\n",
    "\n",
    "# testing. Now the transformer outputs a vector of probabilities for each word in the sequence.\n",
    "E = torch.randint(0, len(vocab), (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformerWithReadoutAndPositionalEncoding(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Practical Considerations",
   "id": "c0fe3b7f074f6048"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Layer Normalization\n",
    "\n",
    "In order to improve training stability and convergence, one common implementation trick is to normalize the output vectors of a layer to have zero mean and unit variance. This is commonly referred to as *layer normalization*. If $\\mathbf{X} = \\mathbf{W}^\\ell \\mathbf{X}_\\ell$ is the output of layer $\\ell$, the normalized output $\\hat{\\mathbf{X}}$ at layer $\\ell$ is computed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{X}}_{ij} = \\gamma_i \\cdot \\frac{\\mathbf{X}_{ij} - \\mu_i}{\\sqrt{\\sigma_{i}^2 + \\epsilon}} + \\beta_i\n",
    "$$\n",
    "\n",
    "Here, $\\epsilon > 0$ is a small number to avoid dividing by zero, and $\\mu$ and $\\sigma^2$ are the row-wise mean and variance of the elements $\\mathbf{X}_{ij}$ at layer $\\ell$:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\frac{1}{n} \\sum_{j=1}^n \\mathbf{X}_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_i^2 = \\frac{1}{n} \\sum_{j=1}^n (\\mathbf{X}_{ij} - \\mu_i)^2\n",
    "$$\n",
    "\n",
    "The learnable parameters $\\gamma_i$ and $\\beta_i$ play the role of recovering the mean and the variance. This might seem like we didn't do anything, but now the learnable parameters do not depend on the computation of $\\mathbf{X}$. This results in more stable training.\n",
    "\n",
    "By normalizing each hidden vector, layer normalization helps to mitigate internal covariate shift and ensures more stable gradients during training."
   ],
   "id": "25acc044e0b109cc"
  },
  {
   "cell_type": "markdown",
   "id": "dd49a97d",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "**Modify the transformer of Task 6.10 to incorporate layer normalization at the output of each transformer layer. Use the PyTorch function `nn.LayerNorm`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c12ea124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 5])\n"
     ]
    }
   ],
   "source": [
    "# We now need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68411a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([1, 5])\n",
      "out.shape: torch.Size([1, 14295, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LanguageTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from Task 10 and added layer normalization. This is the final version of this class.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, L, H, c):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadLayer(m, n, H) for _ in range(L)\n",
    "        ])\n",
    "\n",
    "        # PCA Word embeddings\n",
    "        self.embedding_table = pca_embeddings\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(n)\n",
    "\n",
    "        # Adding readout layer\n",
    "        self.readout = nn.Parameter(torch.empty(c, n).to(device))\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            E (torch.Tensor): The input word indices.\n",
    "        Returns:\n",
    "            Y_hat (torch.Tensor): The output of the transformer, passed through the readout layer.\n",
    "        \"\"\"\n",
    "        B, T = E.shape\n",
    "\n",
    "        # Word embeddings\n",
    "        X = self.embedding_table[E].transpose(1,2) # (B, n, T)\n",
    "\n",
    "        # To create positional encodings, we need to create a vector for each position in the sequence.\n",
    "        P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "        X_tilde = X + P\n",
    "        \n",
    "        # X_l has shape (B, n, T+1)\n",
    "        X_l = X_tilde\n",
    "        for layer in self.layers:\n",
    "            X_l = layer(X_l)\n",
    "\n",
    "        X_l = self.layer_norm(X_l.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # We implement the readout layer as a linear mapping on each word in the sequence.\n",
    "        Y_hat = torch.matmul(self.readout, X_l) # (B, c, T)\n",
    "\n",
    "        \n",
    "        return Y_hat\n",
    "\n",
    "# testing. \n",
    "E = torch.randint(0, pca_embeddings.shape[0], (1,5)).to(device).long()\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "model = LanguageTransformer(m=32, n=256, L=6, H=8, c=c).to(device)\n",
    "out = model(E)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Future Masking\n",
    "\n",
    "Notice that in Equation $\\text{(13)}$, we have attention coefficients for each pair of words in a sequence. This means that our model can potentially learn to have $\\mathbf{A}_{ij}$ will have nonzero attention even if the word $\\mathbf{w}_j$ is ahead of the word $\\mathbf{w}_i$. For tasks such as word generation, this is undesirable: we want to ensure that our attention coefficients only focus on past words, so that we can effectively predict the next one better.\n",
    "\n",
    "We can apply *future masking* to ensure this. The idea is to reweight $\\mathbf{a}_t$ so that the attention weight is zero for all the words beyond $t$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_t = [a_{1t}\\ a_{2t}\\ \\dots a_{tt}\\ 0\\ \\dots\\ 0],\n",
    "$$\n",
    "\n",
    "while making sure that the nonzero attention coefficients sum to 1.\n",
    "\n",
    "An implementation trick to achieve this is to manually set the coefficients to $-\\infty$ before passing them to softmax:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{B}_{ij} = [(\\mathbf{Q}\\mathbf{X})^T(\\mathbf{K}\\mathbf{X})]_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{B}}_{ij} = \n",
    "\\begin{cases}\n",
    "\\mathbf{B}_{ij} & \\text{if } j > i \\\\\n",
    "-\\infty & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So for each head in $\\text{(13)}$, $\\mathbf{A}^h_\\ell$ is replaced by \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{A}}^h_\\ell = \\text{sm}(\\tilde{\\mathbf{B}}^h_\\ell).\n",
    "$$"
   ],
   "id": "9802dbcca73be1b1"
  },
  {
   "cell_type": "markdown",
   "id": "8e08718c",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "**Modify the transformer of Task 6.11 to incorporate future masking at all layers.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "41c30e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([1, 256, 5])\n"
     ]
    }
   ],
   "source": [
    "# We need to modify both MultiHeadLayer and the LanguageTransformer class to include layer normalization.\n",
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified version of the MultiHeadLayer class with layer normalization.\n",
    "    It will have two normalization layers, one after the multi-head attention and one after the nonlinearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        # First layer normalization object.\n",
    "        # Layernorm will average over the n dimensions of each element in the sequence.\n",
    "        self.layer_norm1 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        \n",
    "        # Second layer normalization object.\n",
    "        self.layer_norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass of the multihead attention layer with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input embeddings.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "\n",
    "        # First layer normalization.\n",
    "        # An annoying Pytorch detail: layer norm function expects the normalization to be over the last dimension.\n",
    "        # Therefore, we need to transpose the last two dimensions of the input to shape (B, T, n) each time we normalize, then transpose back.\n",
    "        # (X.transpose(-2,-1) means that we are transposing over the last two dimensions)\n",
    "        X = self.layer_norm1(X.transpose(-2,-1)).transpose(-2,-1)\n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # FUTURE MASKING: \n",
    "        # To mask attention, we create a matrix that indicates if an entry in B is a word in the future\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "        \n",
    "        # If an entry is in the future, we set it to -inf, \n",
    "        # so that when we apply softmax, the probability of that word is 0, while \n",
    "        # the rest of the words sum to 1.\n",
    "        B = B.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "        # Now when we apply softmax, only the words in the past are have nonzero probability.\n",
    "        A = F.softmax(B, dim=-1)\n",
    "    \n",
    "        A_t = A.transpose(-2,-1)\n",
    "        VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "        Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "\n",
    "        # Second layer normalization. Transpose over the last two dimensions\n",
    "        Y = self.layer_norm2(Y.transpose(-2,-1)).transpose(-2,-1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y.sum(dim=1))\n",
    "\n",
    "        return X_l\n",
    "\n",
    "# Testing the change\n",
    "model = MultiHeadLayer(m=32, n=256, H=2).to(device)\n",
    "X = torch.randn(1,256,5).to(device)\n",
    "out = model(X)\n",
    "print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92a0ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([1, 32, 5])\n",
      "out.shape: torch.Size([1, 32, 5])\n"
     ]
    }
   ],
   "source": [
    "# class MultiheadLayer(nn.Module):\n",
    "#     def __init__(self, H, n=n):\n",
    "#         super(MultiheadLayer, self).__init__()\n",
    "        \n",
    "#         assert n % H == 0, \"n must be divisible by num_heads\"\n",
    "#         self.H = H\n",
    "#         self.n = n\n",
    "#         self.m = n // H\n",
    "\n",
    "#         # Column wise\n",
    "#         self.Q = nn.Parameter(torch.empty(H, self.m, n))\n",
    "#         self.K = nn.Parameter(torch.empty(H, self.m, n))\n",
    "#         self.V = nn.Parameter(torch.empty(H, self.m, n))\n",
    "\n",
    "#         self.W = nn.Parameter(torch.empty(H, n, self.m))\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(n)\n",
    "#         self.norm2 = nn.LayerNorm(n)\n",
    "        \n",
    "#         nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "#         nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "#     def forward(self, X0):\n",
    "#         # Column wise\n",
    "#         B, n, T = X0.shape\n",
    "#         X = self.norm1(X0.transpose(1,2)).transpose(1,2) # (B, T, n)\n",
    "#         X = X0\n",
    "        \n",
    "#         # Multi-head attention\n",
    "\n",
    "#         X = X.unsqueeze(1) # (B, 1, T, n)\n",
    "        \n",
    "#         # Column wise\n",
    "#         QX = torch.matmul(self.Q, X) # (B, H, T, m)\n",
    "#         KX = torch.matmul(self.K, X) # (B, H, T, m)\n",
    "#         VX = torch.matmul(self.V, X) # (B, H, T, m)\n",
    "#         # \n",
    "#         B =  QX.transpose(-2,-1) @ KX * (self.m ** -0.5) # (B, H, T, T)\n",
    "\n",
    "#         mask = torch.triu(torch.ones(T, T), diagonal=1).to(device)\n",
    "#         B = B.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "#         A = F.softmax(B, dim=-1)\n",
    "    \n",
    "#         A_t = A.transpose(-2,-1)\n",
    "#         VXA_t = torch.matmul(VX, A_t) # (B, H, m, T)\n",
    "#         Y = torch.matmul(self.W, VXA_t) # (B, H, T, n)\n",
    "#         X = torch.sum(Y, dim=1) # (B, T, n)\n",
    "\n",
    "#         X = self.norm2(X.transpose(1,2)).transpose(1,2)\n",
    "#         X2 = X0 + F.relu(X)\n",
    "#         return X2\n",
    "    \n",
    "# # Test\n",
    "# model = MultiheadLayer(H=1, n=32).to(device)\n",
    "# #E = next(iter(train_loader))[0]\n",
    "# #X = embeddings[E].transpose(2,1)\n",
    "# # row wise\n",
    "# # X = torch.randn(1,5, 32).to(device)\n",
    "# # column wise\n",
    "# X_tilde = torch.randn(1,5, 32).to(device).transpose(1,2)\n",
    "# print(f\"X.shape: {X_tilde.shape}\")\n",
    "# X_tilde = X_tilde.to(device)\n",
    "# out = model(X_tilde)\n",
    "\n",
    "# print(f\"out.shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a54ca21",
   "metadata": {},
   "source": [
    "## Final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30f37eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5974 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m model \u001B[38;5;241m=\u001B[39m LLM(L\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, H\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, n\u001B[38;5;241m=\u001B[39mn)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     37\u001B[0m initial_indices \u001B[38;5;241m=\u001B[39m test[\u001B[38;5;241m132\u001B[39m:\u001B[38;5;241m132\u001B[39m\u001B[38;5;241m+\u001B[39mT]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout.shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# generated_text = generate(model, initial, max_generate_tokens=100)\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# print(\"\\n===INPUT===\\n\")\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# print(tokens_to_words(initial.reshape(-1).tolist()))\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# print(\"\\n===GENERATED TEXT===\\n\")\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# print(generated_text)\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[81], line 20\u001B[0m, in \u001B[0;36mLLM.forward\u001B[0;34m(self, E)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, E):\n\u001B[1;32m     18\u001B[0m     B, T \u001B[38;5;241m=\u001B[39m E\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m---> 20\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken_embedding\u001B[49m\u001B[43m[\u001B[49m\u001B[43mE\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m) \u001B[38;5;66;03m# (B, n, T)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     P \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding(torch\u001B[38;5;241m.\u001B[39marange(T, device\u001B[38;5;241m=\u001B[39mdevice))\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# (n, T)\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     X \u001B[38;5;241m=\u001B[39m X \u001B[38;5;241m+\u001B[39m P\n",
      "\u001B[0;31mIndexError\u001B[0m: index 5974 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# class LLM(nn.Module):   \n",
    "#     def __init__(self, L, H, n):\n",
    "#         super(LLM, self).__init__()\n",
    "\n",
    "#         self.position_embedding = nn.Embedding(T, n)\n",
    "#         # this is the PCA embeddings \n",
    "#         self.token_embedding = pca_embeddings\n",
    "\n",
    "#         self.decoder_layers = nn.Sequential(*[MultiheadLayer(H, n) for _ in range(L)])\n",
    "        \n",
    "#         self.norm = nn.LayerNorm(n)\n",
    "        \n",
    "#         # column wise\n",
    "#         self.readout = nn.Parameter(torch.empty(c, n))\n",
    "#         nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "#     def forward(self, E):\n",
    "#         B, T = E.shape\n",
    "\n",
    "#         X = self.token_embedding[E].transpose(1,2) # (B, n, T)\n",
    "#         P = self.position_embedding(torch.arange(T, device=device)).transpose(0,1) # (n, T)\n",
    "        \n",
    "#         X = X + P\n",
    "\n",
    "#         X = self.decoder_layers(X) # (B, T, n)\n",
    "#         X = self.norm(X.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "#         # column wise\n",
    "#         Y = torch.matmul(self.readout, X) # (B, T, c)\n",
    "#         # row wise\n",
    "#         # Y = torch.matmul(X, self.readout_weight) \n",
    "        \n",
    "#         return Y\n",
    "\n",
    "# # Test\n",
    "# model = LLM(L=2, H=2, n=n).to(device)\n",
    "# initial_indices = test[132:132+T].unsqueeze(0)\n",
    "# out = model(initial_indices)\n",
    "# print(f\"out.shape: {out.shape}\")\n",
    "# # generated_text = generate(model, initial, max_generate_tokens=100)\n",
    "# # print(\"\\n===INPUT===\\n\")\n",
    "# # print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# # print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9687f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician whelp advanced hawks edges Adam's Aim'd Swills Seely Mock Swills taxes betwitched morning bestrid remembered watching commissions dumbly Mock surmounts masterless jest tougher jest lordship's Mock unlick'd affray edges lessons dumbly outside Aim'd morning wanders whelp whelp taxes Swills inforced wanders lordship's Mock piteous commissions banks sir's advanced GREEN Hannibal doting curious Clare misled separation hawks inforced Alla Hannibal misled masterless curious morning destroy remembered unlick'd wanders edges Aim'd Swills lessons lids destroy jest sir's disdained banks outside visit unlick'd disdained Travelling advanced Aim'd surmounts destroy wanders misled taxes jest notched Adam's piteous unlick'd affray lids separation surmounts GREEN Travelling\n"
     ]
    }
   ],
   "source": [
    "# def generate_text(model, input_tokens, max_generate_tokens=500):\n",
    "#     \"\"\"\n",
    "#     Generate text from a model given an initial input token sequence.\n",
    "#     Args:\n",
    "#         model (nn.Module): The model to use for generation.\n",
    "#         input_tokens (torch.Tensor): The initial input token sequence.\n",
    "#         max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "#     Returns:\n",
    "#         torch.Tensor: The generated token sequence.\n",
    "#     \"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         context = input_tokens.clone()\n",
    "#         generated_sequence = input_tokens.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "#         for _ in range(max_generate_tokens):\n",
    "#             logits = model(context)\n",
    "            \n",
    "#             last_token_logits = logits[:,-1,:]\n",
    "#             probs = F.softmax(last_token_logits, dim=-1)\n",
    "#             next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "#             # Slide context window\n",
    "\n",
    "#             context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "#             generated_sequence.append(next_token.item())  # Add new token to generated sequence\n",
    "#         generated_words = tokens_to_words(generated_sequence)\n",
    "#         generation_string = \"\".join(generated_words)\n",
    "#         return generation_string\n",
    "# # Test generate\n",
    "# model = LLM(L=2, H=4, n=n).to(device)\n",
    "# initial_indices = test[132:132+T].unsqueeze(0)\n",
    "# print(generate_text(model, initial_indices, max_generate_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fdb7",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "**Modify the transformer in Task 6.12 to incorporate future masking at all layers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4d6255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 4.91136M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mjavierporras\u001B[0m (\u001B[33mese-2000\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241022_094306-dtuzpwu2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2' target=\"_blank\">Varun's version</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/dtuzpwu2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L = 6\n",
    "H = 8\n",
    "m=32\n",
    "n=256\n",
    "model = LanguageTransformer(m=m, n=n, L=L, H=H, c=c).to(device)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    name=\"Varun's version\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4910a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 4.91136M parameters parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "num_parameters = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {} parameters\".format(num_parameters))\n",
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for t_idx, (X_tilde, y) in enumerate(train_loader):\n",
    "        logits = model(X_tilde)\n",
    "        \n",
    "        B, c, T = logits.shape\n",
    "        \n",
    "        # Reshape logits and y to be able to evaluate cross entropy on \n",
    "        # each token in the sequence.\n",
    "        logits = logits.permute(0,2,1)\n",
    "        logits = logits.reshape(B * T, c)\n",
    "        y = y.view(B * T, -1).squeeze()\n",
    "        cross_entropy_loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        cross_entropy_loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += cross_entropy_loss.item()\n",
    "        wandb.log({\"train_loss\": cross_entropy_loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13b0a9",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "**Repeat the generative exercise in Task 6.9 using the transformer trained in Task 6.13.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician? \n",
      " \n",
      " NORTHUMBERLAND: \n",
      " Believe to me, Buckingham; and see the truth of \n",
      " Is all free to friend. Here \n",
      " With age triumphs, my love still well \n",
      " The heart of my brother's my word, \n",
      " As he would put up. \n",
      " \n",
      " PERDITA: \n",
      " You will not need: \n",
      " Yet, fair upon your highness. \n",
      " \n",
      " First Citizen: \n",
      " You are too bitterly: if the letter I have he: \n",
      " That's his breast that have there have left the king: \n",
      " And\n"
     ]
    }
   ],
   "source": [
    "initial_indices = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate_text(model,initial_indices, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial_indices.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
