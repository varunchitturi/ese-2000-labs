{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [11809, 7893, 13505, 10777, 6189, 8455, 11153, 5160]\n",
      "\n",
      "Decoded text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    decoded = \" \".join([itos[i] for i in int_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "sample_words = text[:36]\n",
    "print(\"Original text: {}\\n\".format(sample_words))\n",
    "print(\"Encoded text: {}\\n\".format(words_to_tokens(split_to_words(sample_words))))\n",
    "print(\"Decoded text: {}\\n\".format(tokens_to_words(words_to_tokens(split_to_words(sample_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [11809, 7893, 13505, 10777, 6189, 8455, 11153, 5160, 3503, 5572]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22463c10a95801e",
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82c3e73672a0d716",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988621247,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "82c3e73672a0d716"
   },
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329672eb8116e436",
   "metadata": {
    "id": "329672eb8116e436"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f4e2e10b103e95",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988622421,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "31f4e2e10b103e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "        # single item \n",
    "        #Y_item = self.text[idx + self.T]\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = TextDataset(train, T)\n",
    "test_dataset = TextDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a43618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4755,  7979,  6149,  ..., 10777,  1695,  3441], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get growing sequences from a batch? \n",
    "E.reshape(B*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99588/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99588/1436626230.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=1, keepdim=True)\n",
    "#     Z /= Z.std(dim=1, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "embeddings = torch.load(\"embeddings.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "# Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "## Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=8192\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "## Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Top k eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "## Co ocurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tx60HzRzvef",
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9621a80",
   "metadata": {},
   "source": [
    "## MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3gCca0eqy91t",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1720988709140,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "3gCca0eqy91t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E shape: torch.Size([64, 256, 64])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'embeddings shape: torch.Size([14295, 256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 256, 64])\n",
      "out shape: torch.Size([64, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n is the embedding dimension in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, n, m))\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.empty(n*4, n))\n",
    "        self.W2 = nn.Parameter(torch.empty(n, n*4))\n",
    "\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W2, a=math.sqrt(5))\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead attention layer, analogous to the one in the \n",
    "        AttentionLayer class. The main difference is that we need to make sure that the \n",
    "        matrix multiplications account for the new head dimenison.\n",
    "        Args:\n",
    "            X (torch.Tensor): The input sequence.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        self.norm1(X.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        # The unsqueeze is used to add the head dimension to the matrices,\n",
    "        # because they are of shape (H, m, n), and we need to multiply them\n",
    "        # with X_expanded of shape (B, 1, n, T)\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "\n",
    "        # Transpose QX for multiplication\n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B_matrix = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # Mask lower triangular part of B_matrix\n",
    "\n",
    "        # Normalize by sqrt(m)\n",
    "        B_matrix /= math.sqrt(self.m)\n",
    "\n",
    "        #mask = torch.tril(torch.ones_like(B_matrix), diagonal=-1)\n",
    "        mask = torch.tril_indices(T,T, -1)\n",
    "        B_matrix[:,:, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        # Compute attention weights A per head\n",
    "        A = F.softmax(B_matrix, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute Z per head\n",
    "        Z = torch.matmul(VX, A)  # (B, H, m, T)\n",
    "\n",
    "        Y_l = torch.matmul(self.W, Z) # (B, H, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        Y_l = self.norm2(Y_l.permute(0,1,3,2)).permute(0,1,3,2)\n",
    "        \n",
    "        X_l = X + self.W2 @ self.nonlinearity(self.W1 @ Y_l.mean(dim=1))  # (B, n, T)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        #X_l = self.dropout(X_l)\n",
    "\n",
    "        return X_l\n",
    "# Test\n",
    "E = next(iter(train_loader))[0]\n",
    "B, T = E.shape\n",
    "E = E.reshape(-1)\n",
    "E = embeddings[E]   \n",
    "E = E.reshape(B, -1 ,T)\n",
    "display(f\"E shape: {E.shape}\")\n",
    "display(f\"embeddings shape: {embeddings.shape}\")\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "\n",
    "model = MultiHeadLayer(m=3, n=256, H=2).to(device)\n",
    "readout = model(E)\n",
    "print(f\"out shape: {readout.shape}\") #(B,T,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05fb71",
   "metadata": {},
   "source": [
    "## LLM (todo rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "RYUNfNqx0TSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720988711543,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "RYUNfNqx0TSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=64\n",
      "E.shape=torch.Size([64, 64])\n",
      "Number of parameters: 5270784\n",
      "logits:  [2, 6, 11, 10, 63, 62, 23, 49, 51, 51, 10, 10, 6, 40, 2, 55, 51, 6, 36, 40, 40, 10, 24, 19, 43, 2, 0, 10, 9, 19, 9, 0, 18, 12, 18, 10, 51, 2, 2, 12, 2, 10, 10, 2, 12, 2, 12, 12, 51, 6, 2, 12, 62, 2, 9, 9, 55, 6, 9, 9, 19, 26, 47, 51]\n",
      "text prediction:  pates meanly cope like demands hot hale guides Shrug'st Shrug'st like like meanly whatever pates flower Shrug'st meanly friendships whatever whatever like touch perceive nobleness pates twere like spots perceive spots twere lays leg lays like Shrug'st pates pates leg pates like like pates leg pates leg leg Shrug'st meanly pates leg hot pates spots spots flower meanly spots spots perceive complices necessity Shrug'st\n"
     ]
    }
   ],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, L, m, n, H):\n",
    "        super(LLM, self).__init__()\n",
    "        self.num_blocks = L\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[MultiHeadLayer(m, n, H) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout = nn.Parameter(torch.empty(c, n))\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, E):\n",
    "        X = self.token_embedding[E] # (B, T, n)\n",
    "        P = self.position_embedding(torch.arange(E.shape[1], device=device)) # (T, n)\n",
    "\n",
    "        X = X + P \n",
    "\n",
    "        # We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\n",
    "        X = X.permute(0,2,1) # (B, n, T)\n",
    "\n",
    "        X_L = self.decoder_layers(X) \n",
    "\n",
    "        # Average over the context size and readout\n",
    "        \n",
    "        X_L = self.norm(X_L.permute(0,2,1)).permute(0,2,1) # (B, n, T)\n",
    "\n",
    "        Y = torch.matmul(self.readout,X_L) # (B, c, T)\n",
    "\n",
    "        return Y\n",
    "\n",
    "print(f\"T={T}\")\n",
    "E = next(iter(train_loader))[0]\n",
    "print(f\"E.shape={E.shape}\")\n",
    "model = LLM(L=3, m=3, n=256, H=2).to(device)\n",
    "logits = model(E)\n",
    "last_token_logits = logits[:,-1,:]\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "#print(f\"Output shape: {out.shape}\") #(B,T,n)\n",
    "#print(f\"Output sample: {out[0,0:1,:]}\")\n",
    "print(\"logits: \", (last_token_logits.argmax(dim=-1).tolist()))\n",
    "print(\"text prediction: \", tokens_to_words(last_token_logits.argmax(dim=-1).tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0174c",
   "metadata": {},
   "source": [
    "## Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1df6bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = input_tokens.clone()\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(input_tokens)\n",
    "            last_token_logits = logits[:,-1,:]\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n",
    "        generated_words = tokens_to_words(generated_sequence.reshape(-1).tolist())\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862ff72",
   "metadata": {},
   "source": [
    "Even with a good chunk of context, the generated text is gibberish at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e7ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician friendships leg pates perceive outside leg lights frown'd tires heart's like Finds Russia Perpetual pates seems intends perceive perceive meanly friendships Shrug'st Russia Finds leg\n"
     ]
    }
   ],
   "source": [
    "# Testing code generation method on \n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    generated_words = generate(model,initial, max_generate_tokens=25)\n",
    "    print(\"\\n===INPUT===\\n\")\n",
    "    print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "    print(\"\\n===GENERATED TEXT===\\n\")\n",
    "    print(\"\".join(generated_words[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea6300",
   "metadata": {},
   "source": [
    "## Train (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2b2f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([64, 64])\n",
      "Y.shape: torch.Size([64, 64])\n",
      "logits.shape: torch.Size([64, 14295, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.7049, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E,Y = next(iter(train_loader))\n",
    "B, T = E.shape\n",
    "logits = model(E)\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "print(f\"Y.shape: {Y.shape}\")\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "# reshaped \n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3256464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.position_embedding.weight.data.numel()\n",
    "#model.token_embedding.weight.data.numel()\n",
    "model.decoder_layers[0].Q.data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e04c79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3659520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([14295, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(model.position_embedding.weight.data.numel())\n",
    "display(model.readout.data.numel())\n",
    "model.readout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3002b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavierporras\u001b[0m (\u001b[33mese-2000\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241021_120817-3wpktoow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/3wpktoow' target=\"_blank\">vital-surf-78</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/3wpktoow' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/3wpktoow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "L = 6\n",
    "H = 8\n",
    "n = n//H\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "#num_epochs = 1\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "model = LLM(L, n, n, H).to(device)\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"m\": n,\n",
    "        \"n\": n,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "568448d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 354/8213 [02:32<56:17,  2.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m B, _, T \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m logits_reshaped \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m T,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m y_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#loss = F.cross_entropy(logits.reshape(B * T, -1), y.reshape(B * T, -1).squeeze())\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits_reshaped, y_reshaped)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "train_loss_evolution = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for E, y in tqdm(train_loader):\n",
    "        logits = model(E)\n",
    "        B, _, T = logits.shape\n",
    "        logits_reshaped = logits.reshape(B * T,-1)\n",
    "        y_reshaped = y.reshape(B * T)\n",
    "        #loss = F.cross_entropy(logits.reshape(B * T, -1), y.reshape(B * T, -1).squeeze())\n",
    "        loss = F.cross_entropy(logits_reshaped, y_reshaped)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)},commit=False)\n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n",
    "\n",
    "# Testing code generation\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))\n",
    "\n",
    "#finish run \n",
    "run.finish()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fdfba564",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 11.72 GiB memory in use. Of the allocated memory 11.40 GiB is allocated by PyTorch, and 17.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m initial \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;241m132\u001b[39m:\u001b[38;5;241m132\u001b[39m\u001b[38;5;241m+\u001b[39mT]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_generate_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===INPUT===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens_to_words(initial\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn[120], line 14\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, input_tokens, max_generate_tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m input_tokens\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_generate_tokens):\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     last_token_logits \u001b[38;5;241m=\u001b[39m logits[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m     16\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(last_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[119], line 22\u001b[0m, in \u001b[0;36mLLM.forward\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m X_L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Average over the context size and readout\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#y_hat = torch.matmul(x.mean(dim=-1), self.readout)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout,X_L) \u001b[38;5;66;03m# (B, c, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[118], line 61\u001b[0m, in \u001b[0;36mMultiHeadLayer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m B, n, T \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# X: (B, n, T)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Normalize embedding dim. \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# We permute because nn.LayerNorm always normalizes the last dimension.\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Expand X to include the head dimension\u001b[39;00m\n\u001b[1;32m     65\u001b[0m X_expanded \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 1, n, T)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2892\u001b[0m         layer_norm,\n\u001b[1;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 3.19 MiB is free. Including non-PyTorch memory, this process has 11.72 GiB memory in use. Of the allocated memory 11.40 GiB is allocated by PyTorch, and 17.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814f842",
   "metadata": {},
   "source": [
    "## Train (actual loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5de84fbe5d8ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1720993361314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ca5de84fbe5d8ec",
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 0.247296M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m     31\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if os.path.exists(\"./model.pt\"):\n",
    "#     model = torch.load(\"./model.pt\", map_location=device)\n",
    "#     print(\"Loaded existing model\")\n",
    "# else:\n",
    "#     L = 1\n",
    "#     H = 2\n",
    "#     m = n//H\n",
    "#     model = LLM(L, m, H).to(device)\n",
    "#     lr = 1e-4\n",
    "#     opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "#     #num_epochs = 20\n",
    "#     num_epochs = 1\n",
    "#     model.eval()\n",
    "#     num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "#     # wandb.config.update({\"lr\": lr, \n",
    "#     #                     \"num_blocks\": num_blocks, \n",
    "#     #                     \"num_heads_per_block\": num_heads_per_block,\n",
    "#     #                     \"context_size\": T,\n",
    "#     #                     \"num_epochs\": num_epochs,\n",
    "#     #                     \"model_summary\": str(model),\n",
    "#     #                     \"num_parameters\": num_parameters_str})\n",
    "#     print(\"Created new model with {}\".format(num_parameters_str))\n",
    "#     train_loss_evolution = []\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         train_loss = 0\n",
    "#         for t_idx, (x, y) in enumerate(train_loader):\n",
    "#             logits = model(x)\n",
    "#             batch_size, _, _ = logits.shape\n",
    "#             #loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "#             opt.zero_grad()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             train_loss += loss.item()\n",
    "#         train_loss_evolution.append(train_loss/len(train_loader))\n",
    "#         clear_output()\n",
    "#         print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "#         run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "#         wandb.config.update({\"num_epochs\": epoch+1})\n",
    "#         plt.plot(train_loss_evolution)\n",
    "#         plt.show()\n",
    "#     torch.save(model, \"./model.pt\")\n",
    "#     wandb.save('./model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  7.485606698046796\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for t_idx, (E, y) in enumerate(test_loader):\n",
    "        logits = model(E)\n",
    "        B, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[\u001b[38;5;241m132\u001b[39m:\u001b[38;5;241m132\u001b[39m\u001b[38;5;241m+\u001b[39mT]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate(model,initial, max_generate_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===INPUT===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68a8c",
   "metadata": {},
   "source": [
    "# Port of Varun's original version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b50f8",
   "metadata": {},
   "source": [
    "## HeadAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137d8cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([64, 64, 256])\n",
      "E.shape: torch.Size([64, 64])\n",
      "out.shape: torch.Size([64, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class HeadAttn(nn.Module):\n",
    "    def __init__(self, m,n):\n",
    "        super(HeadAttn, self).__init__()\n",
    "        self.D = m\n",
    "        self.Q = nn.Parameter(torch.empty(n, m))\n",
    "        self.K = nn.Parameter(torch.empty(n, m))\n",
    "        self.V = nn.Parameter(torch.empty(n, m))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(m, n))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X, use_mask=True):\n",
    "        \n",
    "        if len(X.shape) == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "        \n",
    "        QX = X @ self.Q\n",
    "        KX = X @ self.K\n",
    "        VX = X @ self.V     \n",
    "        \n",
    "        B =  QX @ KX.transpose(-2, -1) * (self.D ** -0.5) \n",
    "\n",
    "        if use_mask:\n",
    "            mask = torch.tril_indices(B.shape[-2], B.shape[-1], -1)\n",
    "            B[:, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        A = F.softmax(B, dim=-1)\n",
    "        AVX = torch.matmul(A, VX)\n",
    "        Y = torch.matmul(AVX, self.W)\n",
    "        return Y\n",
    "\n",
    "# Test\n",
    "model = HeadAttn(m=n//2,n=n).to(device)\n",
    "#x = torch.randn(1, 10, embedding_dim).to(device)\n",
    "E = next(iter(train_loader))[0]\n",
    "X = embeddings[E]\n",
    "print(f\"x.shape: {X.shape}\")\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dea009",
   "metadata": {},
   "source": [
    "## MultiHeadAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59044f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([64, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, H, n):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "        assert n % H == 0, \"D must be divisible by num_heads\"\n",
    "        self.attn = nn.ModuleList([HeadAttn(m = n // H, n=n) for _ in range(H)])\n",
    "        #self.Wo = nn.Parameter(torch.empty(n, n))\n",
    "        #nn.init.kaiming_uniform_(self.Wo, a=math.sqrt(5))\n",
    "    def forward(self, X, use_mask=True):\n",
    "        B, T, _ = X.shape\n",
    "        heads = torch.stack([attn(X, use_mask) for attn in self.attn])\n",
    "        #concat_head = torch.concat(heads, dim=-1)\n",
    "        X = torch.sum(heads,axis=0)\n",
    "        return X\n",
    "\n",
    "# Test\n",
    "model = MultiHeadAttn(H=2,n=n).to(device)\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574d8c0",
   "metadata": {},
   "source": [
    "## DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f02d8fc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[256], expected input with shape [*, 256], but got input of size[32, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m DecoderLayer(H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n\u001b[38;5;241m=\u001b[39mn)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 33\u001b[0m readout \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreadout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, X0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X0):\n\u001b[0;32m---> 17\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# The good\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#X = X0 + self.MHAttn(X)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# The bad\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMHAttn(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2892\u001b[0m         layer_norm,\n\u001b[1;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[256], expected input with shape [*, 256], but got input of size[32, 64]"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, H, n=n):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.MHAttn = MultiHeadAttn(H, n)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.empty(n, 4 * n))\n",
    "        self.W2 = nn.Parameter(torch.empty(4 * n, n))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.W1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W2, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X0):\n",
    "        X = self.norm1(X0)\n",
    "        # The good\n",
    "        #X = X0 + self.MHAttn(X)\n",
    "        # The bad\n",
    "        X = self.MHAttn(X)\n",
    "        X = self.norm2(X)\n",
    "\n",
    "        # X1 = torch.matmul(X, self.W1)\n",
    "        X2 = X0 + F.relu(X)\n",
    "        # X3 = torch.matmul(X2, self.W2)\n",
    "        #X = X + X3\n",
    "\n",
    "        return X2\n",
    "    \n",
    "# Test\n",
    "model = DecoderLayer(H=2, n=n).to(device)\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af1abfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04363f4",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae4e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLM(nn.Module):   \n",
    "    def __init__(self, L, H, n):\n",
    "        super(LLM, self).__init__()\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[DecoderLayer(H, n) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout_weight = nn.Parameter(torch.empty(n, c))\n",
    "        nn.init.kaiming_uniform_(self.readout_weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, E):\n",
    "        X = self.token_embedding[E] # (B, T, n)\n",
    "        P = self.position_embedding(torch.arange(E.shape[1], device=device)) # (T, n)\n",
    "        \n",
    "        X = X + P\n",
    "\n",
    "        X = self.decoder_layers(X) # (B, T, n)\n",
    "\n",
    "        Y = torch.matmul(self.norm(X), self.readout_weight) # (B, T, c)\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    def generate(self, X, max_generate_tokens=500):\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = self(X[: , -T:])\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            X = torch.cat([X, next_token], dim=1)\n",
    "        return X\n",
    "\n",
    "# Test\n",
    "model = LLM(L=2, H=2, n=n).to(device)\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "# generated_text = generate(model, initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fdb7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4d6255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:abomguau) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▃▃▃▃▃▃▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>2.44974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Varun's version</strong> at: <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/abomguau' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/abomguau</a><br/> View project at: <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241021_130307-abomguau/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:abomguau). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241021_130510-vyip80id</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/vyip80id' target=\"_blank\">Varun's version</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/vyip80id' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/vyip80id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "L = 6\n",
    "H = 8\n",
    "model = LLM(L, H, n).to(device)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    name=\"Varun's version\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4910a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss 0.08064873162446105\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [13:56<20:54, 418.11s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "num_parameters = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {} parameters\".format(num_parameters))\n",
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for t_idx, (X, y) in enumerate(train_loader):\n",
    "        logits = model(X)\n",
    "        B, _, _ = logits.shape\n",
    "        logits = logits.view(B * T, -1)\n",
    "        y = y.view(B * T, -1).squeeze()\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fI8MSAifK33",
   "metadata": {
    "id": "6fI8MSAifK33"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91497c7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
