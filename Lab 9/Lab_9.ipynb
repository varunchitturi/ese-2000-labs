{
 "cells": [
  {
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "id": "4c56f9ac2b2ce56"
  },
  {
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "cell_type": "markdown",
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ],
   "id": "d8ff74f15f78e2e9"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1211d69dfabea3c5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988611829,
     "user_tz": 420,
     "elapsed": 1102,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "outputId": "96a0a628-f9a8-42cf-d6c3-9463ede66687",
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:21.828640Z",
     "start_time": "2024-07-14T22:55:20.878073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ],
   "id": "1211d69dfabea3c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-14 15:55:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt’\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  2.92MB/s    in 0.4s    \r\n",
      "\r\n",
      "2024-07-14 15:55:21 (2.92 MB/s) - ‘input.txt’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988615612,
     "user_tz": 420,
     "elapsed": 3314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T23:13:05.384086Z",
     "start_time": "2024-07-14T23:13:05.031862Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988616138,
     "user_tz": 420,
     "elapsed": 528,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:23.859126Z",
     "start_time": "2024-07-14T22:55:23.856154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:100])"
   ],
   "id": "58d8918bcd4f0a06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "d5244308b67761a"
   },
   "cell_type": "markdown",
   "source": [
    "## Tokenization"
   ],
   "id": "d5244308b67761a"
  },
  {
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "cell_type": "markdown",
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ],
   "id": "ae55df526e53534b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988617702,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:26.444902Z",
     "start_time": "2024-07-14T22:55:26.369358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_to_words(text):\n",
    "    words = []\n",
    "    word = \"\"\n",
    "    for c in text:\n",
    "        if c.isalnum():\n",
    "            word += c\n",
    "        else:\n",
    "            words.append(word)\n",
    "            words.append(c)\n",
    "            word = \"\"\n",
    "    words.append(word)\n",
    "    return words\n",
    "\n",
    "words = list(set(split_to_words(text)))\n",
    "vocab_size = len(words)\n",
    "print(\"Number of distinct words in text: {}\".format(vocab_size))"
   ],
   "id": "4aba7e30bedd5646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in text: 13334\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988618989,
     "user_tz": 420,
     "elapsed": 576,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:28.395258Z",
     "start_time": "2024-07-14T22:55:28.315458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(words)}\n",
    "itos = {i:word for i, word in enumerate(words)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    return [itos[i] for i in int_list]\n",
    "\n",
    "sample_words = split_to_words(text)[:10]\n",
    "print(\"Original text: {}\".format(\"\".join(sample_words)))\n",
    "print(\"Encoded text: {}\".format(words_to_tokens(sample_words)))\n",
    "print(\"Decoded text: {}\".format(tokens_to_words(words_to_tokens(sample_words))))"
   ],
   "id": "9cacb2e9ced76d25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we \n",
      "Encoded text: [11026, 12486, 12555, 10887, 0, 8957, 8995, 12486, 11929, 12486]\n",
      "Decoded text: ['First', ' ', 'Citizen', ':', '', '\\n', 'Before', ' ', 'we', ' ']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988619915,
     "user_tz": 420,
     "elapsed": 482,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:30.596507Z",
     "start_time": "2024-07-14T22:55:30.456335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ],
   "id": "1d146ef59a76b0ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [11026, 12486, 12555, 10887, 0, 8957, 8995, 12486, 11929, 12486]\n",
      "['First', ' ', 'Citizen', ':', '', '\\n', 'Before', ' ', 'we', ' ']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "cell_type": "markdown",
   "source": [
    "## Data Split"
   ],
   "id": "a22463c10a95801e"
  },
  {
   "metadata": {
    "id": "82c3e73672a0d716",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988621247,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:31.483388Z",
     "start_time": "2024-07-14T22:55:31.472261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_size = 32\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ],
   "id": "82c3e73672a0d716",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "329672eb8116e436"
   },
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "id": "329672eb8116e436"
  },
  {
   "metadata": {
    "id": "31f4e2e10b103e95",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988622421,
     "user_tz": 420,
     "elapsed": 3,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:32.779958Z",
     "start_time": "2024-07-14T22:55:32.776800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, context_size):\n",
    "        self.text = text\n",
    "        self.context_size = context_size\n",
    "        assert self.context_size < len(text), \"context_size must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx:idx + self.context_size],  self.text[idx + 1:idx + self.context_size + 1]\n",
    "\n",
    "train_set = TextDataset(train, context_size)\n",
    "test_set = TextDataset(test, context_size)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ],
   "id": "31f4e2e10b103e95",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "cell_type": "markdown",
   "source": [
    "# Embeddings"
   ],
   "id": "659a4f4edabab2a2"
  },
  {
   "metadata": {
    "id": "1a1790cd0b8bacdd"
   },
   "cell_type": "markdown",
   "source": [
    "We will use PCA to create the token embeddings"
   ],
   "id": "1a1790cd0b8bacdd"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988654405,
     "user_tz": 420,
     "elapsed": 30529,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:55:59.380084Z",
     "start_time": "2024-07-14T22:55:51.258691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix X is a VxV (V is our vocab size) symmetric matrix where X_ij is how many times the ith word appears within W words away from the jth word.\n",
    "W = 10\n",
    "X = torch.stack([torch.zeros(len(words)) for _ in range(len(words))])\n",
    "for i in trange(len(tokenized_text)):\n",
    "    words_to_right = tokenized_text[i+1:i+W+1]\n",
    "    words_to_left = tokenized_text[i-W:i]\n",
    "    X[tokenized_text[i], words_to_right] += 1.0\n",
    "    X[tokenized_text[i], words_to_left] += 1.0\n",
    "X = X.to(device)"
   ],
   "id": "ccbafd52bae8f505",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 528579/528579 [00:07<00:00, 67490.79it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "582e9c67a87949a4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988658073,
     "user_tz": 420,
     "elapsed": 3672,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:56:02.341324Z",
     "start_time": "2024-07-14T22:56:02.232431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "embedding_dim = 256\n",
    "X -= X.mean(dim=1, keepdim=True)\n",
    "X /= X.std(dim=1, keepdim=True)\n",
    "cov = (X @ X.T)/(X.shape[0] - 1)\n",
    "L, Q = torch.linalg.eigh(cov)\n",
    "principle_eigv = Q[:, -embedding_dim:].T\n",
    "embeddings = X @ principle_eigv.T # (vocab_size, embedding_dim)"
   ],
   "id": "582e9c67a87949a4",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_linalg_eigh.eigenvalues' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m X \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mstd(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m cov \u001B[38;5;241m=\u001B[39m (X \u001B[38;5;241m@\u001B[39m X\u001B[38;5;241m.\u001B[39mT)\u001B[38;5;241m/\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m L, Q \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meigh\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcov\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m principle_eigv \u001B[38;5;241m=\u001B[39m Q[:, \u001B[38;5;241m-\u001B[39membedding_dim:]\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m      8\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m X \u001B[38;5;241m@\u001B[39m principle_eigv\u001B[38;5;241m.\u001B[39mT \u001B[38;5;66;03m# (vocab_size, embedding_dim)\u001B[39;00m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: The operator 'aten::_linalg_eigh.eigenvalues' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "id": "7tx60HzRzvef"
  },
  {
   "cell_type": "code",
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, num_heads, dqk, dv):\n",
    "        super(Attn, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dqk = dqk\n",
    "        self.dv = dv\n",
    "        self.Wq = nn.Parameter(torch.randn(num_heads, embedding_dim, dqk))\n",
    "        nn.init.kaiming_uniform_(self.Wq, a=math.sqrt(5))\n",
    "        self.Wk = nn.Parameter(torch.randn(num_heads, embedding_dim, dqk))\n",
    "        nn.init.kaiming_uniform_(self.Wk, a=math.sqrt(5))\n",
    "        self.Wv = nn.Parameter(torch.randn(num_heads, embedding_dim, dv))\n",
    "        nn.init.kaiming_uniform_(self.Wv, a=math.sqrt(5))\n",
    "        self.Wo = nn.Parameter(torch.randn(num_heads * dv, embedding_dim))\n",
    "        nn.init.kaiming_uniform_(self.Wo, a=math.sqrt(5))\n",
    "    def forward(self, x, use_mask=False):\n",
    "\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        B, N, D = x.shape\n",
    "        x = x.unsqueeze(1)\n",
    "        q = x @ self.Wq.unsqueeze(0)\n",
    "        k = x @ self.Wk.unsqueeze(0)\n",
    "        v = x @ self.Wv.unsqueeze(0)\n",
    "        qk =  q @ k.transpose(-2, -1) * (self.dqk ** -0.5)\n",
    "\n",
    "        if use_mask:\n",
    "            mask = torch.tril_indices(qk.shape[-2], qk.shape[-1], -1)\n",
    "            qk[:, :, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        softmax_qk = F.softmax(qk, dim=-1)\n",
    "        qkv = softmax_qk @ v\n",
    "        concat_qkv = qkv.permute(0, 2, 1, 3).reshape(B, N, self.num_heads * self.dv)\n",
    "        out = concat_qkv @ self.Wo.unsqueeze(0)\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "id": "3gCca0eqy91t",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988709140,
     "user_tz": 420,
     "elapsed": 546,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:56:05.322626Z",
     "start_time": "2024-07-14T22:56:05.314793Z"
    }
   },
   "id": "3gCca0eqy91t",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, dqk=embedding_dim, dv=embedding_dim):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_attn = Attn(num_heads, dqk, dv)\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.linear2 = nn.Linear(3 * embedding_dim, embedding_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.masked_attn(self.norm1(x)) + x\n",
    "        x = self.linear2(F.relu(self.linear1(self.norm2(x)))) + x\n",
    "        return x"
   ],
   "metadata": {
    "id": "2zgrRoCY04CP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988710208,
     "user_tz": 420,
     "elapsed": 551,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:56:06.188010Z",
     "start_time": "2024-07-14T22:56:06.185024Z"
    }
   },
   "id": "2zgrRoCY04CP",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "class LLM(nn.Module):\n",
    "  def __init__(self, num_blocks, num_heads_per_block, key_query_dim=embedding_dim, value_dim=embedding_dim):\n",
    "    super(LLM, self).__init__()\n",
    "    self.num_blocks = num_blocks\n",
    "    self.attn = Attn(num_heads_per_block, key_query_dim, value_dim)\n",
    "    self.position_embedding = nn.Embedding(context_size, embedding_dim)\n",
    "    self.token_embedding = embeddings\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer(num_heads_per_block, key_query_dim, value_dim) for _ in range(num_blocks)])\n",
    "    self.norm = nn.LayerNorm(embedding_dim)\n",
    "    self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "  def forward(self, tokens):\n",
    "    token_emb = self.token_embedding[tokens]\n",
    "    pos_emb = self.position_embedding(torch.arange(tokens.shape[1], device=device))\n",
    "    x = token_emb + pos_emb\n",
    "    for layer in self.decoder_layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    return self.out(self.norm(x))\n",
    "\n",
    "  def generate(self, input_tokens, max_generate_tokens=500):\n",
    "    for _ in range(max_generate_tokens):\n",
    "      logits = self(input_tokens[: , -context_size:])\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      next_token = torch.multinomial(probs, num_samples=1)\n",
    "      input_tokens = torch.cat([input_tokens, next_token], dim=1)\n",
    "    return input_tokens"
   ],
   "metadata": {
    "id": "RYUNfNqx0TSw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988711543,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T22:56:06.812430Z",
     "start_time": "2024-07-14T22:56:06.807648Z"
    }
   },
   "id": "RYUNfNqx0TSw",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca5de84fbe5d8ec",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720993361314,
     "user_tz": 420,
     "elapsed": 3038,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18",
    "ExecuteTime": {
     "end_time": "2024-07-14T23:13:11.753040Z",
     "start_time": "2024-07-14T23:13:11.684121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_blocks = 6\n",
    "num_heads_per_block = 8\n",
    "if os.path.exists(\"./model.pt\"):\n",
    "    model = torch.load(\"./model.pt\", map_location=device)\n",
    "else:\n",
    "    model = LLM(num_blocks, num_heads_per_block).to(device)\n",
    "lr = 1e-4\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "num_epochs = 100\n",
    "model.eval()"
   ],
   "id": "ca5de84fbe5d8ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLM(\n",
       "  (attn): Attn()\n",
       "  (position_embedding): Embedding(32, 256)\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (masked_attn): Attn()\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=256, out_features=768, bias=True)\n",
       "      (linear2): Linear(in_features=768, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=256, out_features=13334, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        logits = model(x)\n",
    "        batch_size, _, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(batch_size * context_size, -1), y.view(batch_size * context_size, -1).squeeze())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch}, Loss {train_loss/len(train_loader)}\")\n",
    "    plt.plot(train_loss_evolution)"
   ],
   "metadata": {
    "id": "FIlPL9cr6YxE",
    "ExecuteTime": {
     "end_time": "2024-07-14T23:27:58.291342Z",
     "start_time": "2024-07-14T23:13:14.597258Z"
    }
   },
   "id": "FIlPL9cr6YxE",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [14:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m     opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m      9\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 10\u001B[0m     \u001B[43mopt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     12\u001B[0m train_loss_evolution\u001B[38;5;241m.\u001B[39mappend(train_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader))\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    389\u001B[0m             )\n\u001B[0;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:188\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    175\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    177\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    178\u001B[0m         group,\n\u001B[1;32m    179\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    185\u001B[0m         state_steps,\n\u001B[1;32m    186\u001B[0m     )\n\u001B[0;32m--> 188\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:340\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    338\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 340\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/adamw.py:416\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[1;32m    413\u001B[0m step_t \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;66;03m# Perform stepweight decay\u001B[39;00m\n\u001B[0;32m--> 416\u001B[0m \u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    419\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "id": "4e45c455cd8bcd29",
    "ExecuteTime": {
     "end_time": "2024-07-14T23:28:59.058819Z",
     "start_time": "2024-07-14T23:28:07.468719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        logits = model(x)\n",
    "        batch_size, _, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(batch_size * context_size, -1), y.view(batch_size * context_size, -1).squeeze())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ],
   "id": "4e45c455cd8bcd29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.12407381848835772\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "initial = test[132:164].unsqueeze(0)\n",
    "print(\"\".join(tokens_to_words(model.generate(initial, max_generate_tokens=1000).squeeze().tolist())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720997577756,
     "user_tz": 420,
     "elapsed": 4770,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-07-14T23:29:16.760947Z",
     "start_time": "2024-07-14T23:29:04.108295Z"
    }
   },
   "id": "Jhoh1INhBePM",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BAPTISTA:\n",
      "After my death the one half of my lands,\n",
      "And in possession twenty and with this-northneitherquarry persons.\n",
      "boss, life good Duke as thy Adieu this gentleman?\n",
      "The King is a high\n",
      "Accursed in the mark of his time-traitor were.'\n",
      "How, arms thou hast well, in him ere kinsman.\n",
      "\n",
      "POLIXENES:\n",
      "Till, but preachment, I would have been took;\n",
      "For 'Let I think'd this to come spell.\n",
      "\n",
      "FRIAR Murderer:\n",
      "My, sir, if in the Unto, sir;\n",
      "Fair foolish, come to the put world of your own?\n",
      "\n",
      "ANGELO:\n",
      "Be without me of his remembrance bold face?\n",
      "And acquit, go helding is so she, which you heard;\n",
      "velvet to plead point of his prince;\n",
      "For favourites, in flatter expiring well-wall, scatter, which I have,\n",
      "Who murder Polixenes tongues brought attend do from;\n",
      "The R northern, they! man well, come.\n",
      "\n",
      "POLIXENES:\n",
      "O is have, some son seem, there thou shalt.\n",
      "\n",
      "ROMEO:\n",
      "Quick, sir, Richmond up?\n",
      "\n",
      "Servant:\n",
      "Those, therefore.\n",
      "\n",
      "GLOUCESTER:\n",
      "mirth my lord?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Ay, when, sir;\n",
      "Let on hope on't brains than yours.\n",
      "\n",
      "GREGORY:\n",
      "Is by a worser,\n",
      "Even spread. But no more on you;\n",
      "For he cords of your Wherein: but Bunch am\n",
      "Which misinterpret should light speak hath therefore?\n",
      "Our good went follow, there furnish it,\n",
      "Ah did maid-repeal'd hand banquet all:\n",
      "Well you be, sir; not their thou wide\n",
      "In Six you both, his arm if,\n",
      "You? by, you a child: it, what shalt first.\n",
      "My scene is rebukes us in a testimonied,\n",
      "flayed I saw have up! a death's too year your lord, as Apollo.\n",
      "\n",
      "ANGELO:\n",
      "There lords, once thou? Fly be penitently,\n",
      "I know no call. Hadst now to guide?\n",
      "O you rode of your suitor! My horse!\n",
      "\n",
      "Clown:\n",
      "The Hermione is lips death! 'None gave't a stabb!\n",
      "\n",
      "Musician:\n",
      "We comes agreed; whose they you;\n",
      "tis an me then: mine, rather succession.\n",
      "Amen this on like; sir. Under in a hazard,\n",
      "My lord ! this, which work well-south withdraw be,\n",
      "Arise my wife Cato kind the high of up.\n",
      ";\n",
      "The celerity act in whose burnt's him,\n",
      "That the death of the whole that and;\n",
      "Those too Swarming, makest the world of westward,\n",
      "he your country, Gloucester you perverse Marry didst.\n",
      "\n",
      "sorrowing:\n",
      "He! See, good escape? but, that?\n",
      "My lord happy praise find, nay\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2lfXYl9x6bu6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720988956306,
     "user_tz": 420,
     "elapsed": 2,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    }
   },
   "id": "2lfXYl9x6bu6",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "57PP7f1Iezeo",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1720920641464,
     "user_tz": 420,
     "elapsed": 5,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     }
    }
   },
   "id": "57PP7f1Iezeo",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "6fI8MSAifK33"
   },
   "id": "6fI8MSAifK33"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
