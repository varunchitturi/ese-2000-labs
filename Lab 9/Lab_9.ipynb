{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps:0\"\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [11410, 14281, 11174, 4130, 13854, 1762, 1043, 2824]\n",
      "\n",
      "Decoded text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    decoded = \" \".join([itos[i] for i in int_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "sample_words = text[:36]\n",
    "print(\"Original text: {}\\n\".format(sample_words))\n",
    "print(\"Encoded text: {}\\n\".format(words_to_tokens(split_to_words(sample_words))))\n",
    "print(\"Decoded text: {}\\n\".format(tokens_to_words(words_to_tokens(split_to_words(sample_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [11410, 14281, 11174, 4130, 13854, 1762, 1043, 2824, 12578, 1934]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22463c10a95801e",
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c3e73672a0d716",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988621247,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "82c3e73672a0d716"
   },
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329672eb8116e436",
   "metadata": {
    "id": "329672eb8116e436"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f4e2e10b103e95",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988622421,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "31f4e2e10b103e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "        # single item \n",
    "        #Y_item = self.text[idx + self.T]\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = TextDataset(train, T)\n",
    "test_dataset = TextDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a43618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12901,  4130,  8804,  ...,  7157, 11239,  3832])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get growing sequences from a batch? \n",
    "E.reshape(B*T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121820/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_121820/1436626230.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=1, keepdim=True)\n",
    "#     Z /= Z.std(dim=1, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "embeddings = torch.load(\"embeddings.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "# Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "## Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=8192\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "## Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Top k eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "## Co ocurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tx60HzRzvef",
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9621a80",
   "metadata": {},
   "source": [
    "## MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3gCca0eqy91t",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1720988709140,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "3gCca0eqy91t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E shape: torch.Size([64, 256, 64])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'embeddings shape: torch.Size([14295, 256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 256, 64])\n",
      "out shape: torch.Size([64, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n is the embedding dimension in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(H, n, m))\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.empty(n*4, n))\n",
    "        self.W2 = nn.Parameter(torch.empty(n, n*4))\n",
    "\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W2, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead attention layer, analogous to the one in the \n",
    "        AttentionLayer class. The main difference is that we need to make sure that the \n",
    "        matrix multiplications account for the new head dimenison.\n",
    "        Args:\n",
    "            X (torch.Tensor): The input sequence.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, n, T = X.shape  # X: (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        self.norm1(X.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        # The unsqueeze is used to add the head dimension to the matrices,\n",
    "        # because they are of shape (H, m, n), and we need to multiply them\n",
    "        # with X_expanded of shape (B, 1, n, T)\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "\n",
    "        # Transpose QX for multiplication\n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B_matrix = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # Mask lower triangular part of B_matrix\n",
    "\n",
    "        # Normalize by sqrt(m)\n",
    "        B_matrix /= math.sqrt(self.m)\n",
    "\n",
    "        #mask = torch.tril(torch.ones_like(B_matrix), diagonal=-1)\n",
    "        mask = torch.tril_indices(T,T, -1)\n",
    "        B_matrix[:,:, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        # Compute attention weights A per head\n",
    "        A = F.softmax(B_matrix, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute Z per head\n",
    "        Z = torch.matmul(VX, A)  # (B, H, m, T)\n",
    "\n",
    "        Y_l = torch.matmul(self.W, Z) # (B, H, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        Y_l = self.norm2(Y_l.permute(0,1,3,2)).permute(0,1,3,2)\n",
    "        \n",
    "        X_l = X + self.W2 @ self.nonlinearity(self.W1 @ Y_l.mean(dim=1))  # (B, n, T)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        #X_l = self.dropout(X_l)\n",
    "\n",
    "        return X_l\n",
    "# Test\n",
    "E = next(iter(train_loader))[0]\n",
    "B, T = E.shape\n",
    "E = E.reshape(-1)\n",
    "E = embeddings[E]   \n",
    "E = E.reshape(B, -1 ,T)\n",
    "display(f\"E shape: {E.shape}\")\n",
    "display(f\"embeddings shape: {embeddings.shape}\")\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "\n",
    "model = MultiHeadLayer(m=3, n=256, H=2).to(device)\n",
    "readout = model(E)\n",
    "print(f\"out shape: {readout.shape}\") #(B,T,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05fb71",
   "metadata": {},
   "source": [
    "## LLM (todo rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "RYUNfNqx0TSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720988711543,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "RYUNfNqx0TSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=64\n",
      "E.shape=torch.Size([64, 64])\n",
      "Number of parameters: 5270784\n",
      "logits:  [15, 44, 47, 12, 35, 44, 12, 3, 44, 16, 15, 47, 44, 26, 44, 44, 26, 29, 24, 3, 35, 12, 26, 15, 20, 20, 35, 13, 15, 15, 47, 44, 26, 24, 15, 3, 26, 47, 44, 63, 63, 15, 47, 15, 44, 47, 12, 15, 26, 12, 26, 29, 24, 57, 26, 44, 44, 13, 63, 44, 34, 15, 20, 15]\n",
      "text prediction:  slay rainy custom hipped Imparts rainy hipped malice rainy carouses slay custom rainy Greeks rainy rainy Greeks shot bloodily malice Imparts hipped Greeks slay Publicola Publicola Imparts weal slay slay custom rainy Greeks bloodily slay malice Greeks custom rainy raising raising slay custom slay rainy custom hipped slay Greeks hipped Greeks shot bloodily dowdy Greeks rainy rainy weal raising rainy they're slay Publicola slay\n"
     ]
    }
   ],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, L, m, n, H):\n",
    "        super(LLM, self).__init__()\n",
    "        self.num_blocks = L\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[MultiHeadLayer(m, n, H) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout = nn.Parameter(torch.empty(c, n))\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, E):\n",
    "        X = self.token_embedding[E] # (B, T, n)\n",
    "        P = self.position_embedding(torch.arange(E.shape[1], device=device)) # (T, n)\n",
    "\n",
    "        X = X + P \n",
    "\n",
    "        # We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\n",
    "        X = X.permute(0,2,1) # (B, n, T)\n",
    "\n",
    "        X_L = self.decoder_layers(X) \n",
    "\n",
    "        # Average over the context size and readout\n",
    "        \n",
    "        X_L = self.norm(X_L.permute(0,2,1)).permute(0,2,1) # (B, n, T)\n",
    "\n",
    "        Y = torch.matmul(self.readout,X_L) # (B, c, T)\n",
    "\n",
    "        return Y\n",
    "\n",
    "print(f\"T={T}\")\n",
    "E = next(iter(train_loader))[0]\n",
    "print(f\"E.shape={E.shape}\")\n",
    "model = LLM(L=3, m=3, n=256, H=2).to(device)\n",
    "logits = model(E)\n",
    "last_token_logits = logits[:,-1,:]\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "#print(f\"Output shape: {out.shape}\") #(B,T,n)\n",
    "#print(f\"Output sample: {out[0,0:1,:]}\")\n",
    "print(\"logits: \", (last_token_logits.argmax(dim=-1).tolist()))\n",
    "print(\"text prediction: \", tokens_to_words(last_token_logits.argmax(dim=-1).tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0174c",
   "metadata": {},
   "source": [
    "## Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df6bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = input_tokens.clone()\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(generated_sequence)\n",
    "            last_token_logits = logits[:,-1,:]\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n",
    "        generated_words = tokens_to_words(generated_sequence.reshape(-1).tolist())\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862ff72",
   "metadata": {},
   "source": [
    "Even with a good chunk of context, the generated text is gibberish at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code generation method on \n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    generated_words = generate(model,initial, max_generate_tokens=25)\n",
    "    print(\"\\n===INPUT===\\n\")\n",
    "    print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "    print(\"\\n===GENERATED TEXT===\\n\")\n",
    "    print(\"\".join(generated_words[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea6300",
   "metadata": {},
   "source": [
    "## Train (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "E,Y = next(iter(train_loader))\n",
    "B, T = E.shape\n",
    "logits = model(E)\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "print(f\"Y.shape: {Y.shape}\")\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "# reshaped \n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3256464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.position_embedding.weight.data.numel()\n",
    "#model.token_embedding.weight.data.numel()\n",
    "model.decoder_layers[0].Q.data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.position_embedding.weight.data.numel())\n",
    "display(model.readout.data.numel())\n",
    "model.readout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 6\n",
    "H = 8\n",
    "n = n//H\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "#num_epochs = 1\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "model = LLM(L, n, n, H).to(device)\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"m\": n,\n",
    "        \"n\": n,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568448d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "train_loss_evolution = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for E, y in tqdm(train_loader):\n",
    "        logits = model(E)\n",
    "        B, _, T = logits.shape\n",
    "        logits_reshaped = logits.reshape(B * T,-1)\n",
    "        y_reshaped = y.reshape(B * T)\n",
    "        #loss = F.cross_entropy(logits.reshape(B * T, -1), y.reshape(B * T, -1).squeeze())\n",
    "        loss = F.cross_entropy(logits_reshaped, y_reshaped)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)},commit=False)\n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n",
    "\n",
    "# Testing code generation\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))\n",
    "\n",
    "#finish run \n",
    "run.finish()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfba564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814f842",
   "metadata": {},
   "source": [
    "## Train (actual loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5de84fbe5d8ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1720993361314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ca5de84fbe5d8ec",
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18"
   },
   "outputs": [],
   "source": [
    "\n",
    "# if os.path.exists(\"./model.pt\"):\n",
    "#     model = torch.load(\"./model.pt\", map_location=device)\n",
    "#     print(\"Loaded existing model\")\n",
    "# else:\n",
    "#     L = 1\n",
    "#     H = 2\n",
    "#     m = n//H\n",
    "#     model = LLM(L, m, H).to(device)\n",
    "#     lr = 1e-4\n",
    "#     opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "#     #num_epochs = 20\n",
    "#     num_epochs = 1\n",
    "#     model.eval()\n",
    "#     num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "#     # wandb.config.update({\"lr\": lr, \n",
    "#     #                     \"num_blocks\": num_blocks, \n",
    "#     #                     \"num_heads_per_block\": num_heads_per_block,\n",
    "#     #                     \"context_size\": T,\n",
    "#     #                     \"num_epochs\": num_epochs,\n",
    "#     #                     \"model_summary\": str(model),\n",
    "#     #                     \"num_parameters\": num_parameters_str})\n",
    "#     print(\"Created new model with {}\".format(num_parameters_str))\n",
    "#     train_loss_evolution = []\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         train_loss = 0\n",
    "#         for t_idx, (x, y) in enumerate(train_loader):\n",
    "#             logits = model(x)\n",
    "#             batch_size, _, _ = logits.shape\n",
    "#             #loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "#             opt.zero_grad()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             train_loss += loss.item()\n",
    "#         train_loss_evolution.append(train_loss/len(train_loader))\n",
    "#         clear_output()\n",
    "#         print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "#         run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "#         wandb.config.update({\"num_epochs\": epoch+1})\n",
    "#         plt.plot(train_loss_evolution)\n",
    "#         plt.show()\n",
    "#     torch.save(model, \"./model.pt\")\n",
    "#     wandb.save('./model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for t_idx, (E, y) in enumerate(test_loader):\n",
    "        logits = model(E)\n",
    "        B, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68a8c",
   "metadata": {},
   "source": [
    "# Port of Varun's original version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b50f8",
   "metadata": {},
   "source": [
    "## HeadAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137d8cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([64, 64, 256])\n",
      "E.shape: torch.Size([64, 64])\n",
      "out.shape: torch.Size([64, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class HeadAttn(nn.Module):\n",
    "    def __init__(self, m,n):\n",
    "        super(HeadAttn, self).__init__()\n",
    "        self.D = m\n",
    "        self.Q = nn.Parameter(torch.empty(n, m))\n",
    "        self.K = nn.Parameter(torch.empty(n, m))\n",
    "        self.V = nn.Parameter(torch.empty(n, m))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(m, n))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X, use_mask=True):\n",
    "        \n",
    "        if len(X.shape) == 2:\n",
    "            X = X.unsqueeze(0)\n",
    "        \n",
    "        QX = X @ self.Q\n",
    "        KX = X @ self.K\n",
    "        VX = X @ self.V     \n",
    "        \n",
    "        B =  QX @ KX.transpose(-2, -1) * (self.D ** -0.5) \n",
    "\n",
    "        if use_mask:\n",
    "            mask = torch.tril_indices(B.shape[-2], B.shape[-1], -1)\n",
    "            B[:, mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        A = F.softmax(B, dim=-1)\n",
    "        AVX = torch.matmul(A, VX)\n",
    "        Y = torch.matmul(AVX, self.W)\n",
    "        return Y\n",
    "\n",
    "# Test\n",
    "model = HeadAttn(m=n//2,n=n).to(device)\n",
    "#x = torch.randn(1, 10, embedding_dim).to(device)\n",
    "E = next(iter(train_loader))[0]\n",
    "X = embeddings[E]\n",
    "print(f\"x.shape: {X.shape}\")\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574d8c0",
   "metadata": {},
   "source": [
    "## DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02d8fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([64, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, H, n=n):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        assert n % H == 0, \"n must be divisible by num_heads\"\n",
    "        self.H = H\n",
    "        self.n = n\n",
    "        self.attn_heads = nn.ModuleList([HeadAttn(m = n // H, n=n) for _ in range(H)])\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.empty(n, 4 * n))\n",
    "        self.W2 = nn.Parameter(torch.empty(4 * n, n))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.W1, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W2, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X0):\n",
    "        X = self.norm1(X0)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        heads = torch.stack([attn(X, use_mask=True) for attn in self.attn_heads])\n",
    "        X = torch.sum(heads, axis=0)\n",
    "        \n",
    "        X = self.norm2(X)\n",
    "        X2 = X0 + F.relu(X)\n",
    "        return X2\n",
    "    \n",
    "# Test\n",
    "model = DecoderLayer(H=4, n=n).to(device)\n",
    "readout = model(X)\n",
    "print(f\"out.shape: {readout.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04363f4",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae4e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(nn.Module):   \n",
    "    def __init__(self, L, H, n):\n",
    "        super(LLM, self).__init__()\n",
    "        self.position_embedding = nn.Embedding(T, n)\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[DecoderLayer(H, n) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout_weight = nn.Parameter(torch.empty(n, c))\n",
    "        nn.init.kaiming_uniform_(self.readout_weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, E):\n",
    "        B, T = E.shape\n",
    "        X = self.token_embedding[E] # (B, T, n)\n",
    "        P = self.position_embedding(torch.arange(T, device=device)) # (T, n)\n",
    "        \n",
    "        X = X + P\n",
    "\n",
    "        X = self.decoder_layers(X) # (B, T, n)\n",
    "\n",
    "        Y = torch.matmul(self.norm(X), self.readout_weight) # (B, T, c)\n",
    "        \n",
    "        return Y\n",
    "\n",
    "# Test\n",
    "model = LLM(L=2, H=2, n=n).to(device)\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "# generated_text = generate(model, initial, max_generate_tokens=100)\n",
    "# print(\"\\n===INPUT===\\n\")\n",
    "# print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "# print(\"\\n===GENERATED TEXT===\\n\")\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9687f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician Began shipping changed scroll believest craft gazed valiantness BRAKENBURY amazedly Before undergo pilgrimage yielding Whisper Sixth choice causes alligator boys banks fellowships boughs becoming envious knee marr'd parentage nobly TYRREL Rage chine cherishing merciful Forsake towards slippery humbly serpigo Abides urging Release vault spheres Give braver home silken do't poll practise boats dares obstacles panting older kinds plucker groom relents hath Thinkest rates fairy told attentiveness Unmeritable pleader foundation cockatrice attainder Wretches Rash faith dirt Youngling Blue misplaced Apothecary liking openly exact scouts Here Gallia Caesar's commandment Lycurguses reserved affects reckoning bleared smote feigned run strongly travel vouches Earl suburbs\n"
     ]
    }
   ],
   "source": [
    "def generate_reversed(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context = input_tokens.clone()\n",
    "        generated_sequence = input_tokens.cpu().squeeze().tolist()  # Ensure it's a 1D list\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(context)\n",
    "            \n",
    "            last_token_logits = logits[:,-1,:]\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Slide context window\n",
    "\n",
    "            context = torch.cat([context[:, 1:], next_token], dim=1)  \n",
    "            generated_sequence.append(next_token.item())  # Add new token to generated sequence\n",
    "        generated_words = tokens_to_words(generated_sequence)\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string\n",
    "# Test generate\n",
    "model = LLM(L=2, H=4, n=n).to(device)\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "print(generate_reversed(model, initial, max_generate_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17fdb7",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4d6255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavierporras\u001b[0m (\u001b[33mese-2000\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241021_173620-89artgni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/89artgni' target=\"_blank\">Varun's version</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/89artgni' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/89artgni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "L = 6\n",
    "H = 8\n",
    "model = LLM(L, H, n).to(device)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "B = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    name=\"Varun's version\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4910a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 8.401152M parameters parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "num_parameters = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {} parameters\".format(num_parameters))\n",
    "train_loss_evolution = []\n",
    "for epoch in trange(num_epochs):\n",
    "    train_loss = 0\n",
    "    for t_idx, (X, y) in enumerate(train_loader):\n",
    "        logits = model(X)\n",
    "        B, _, _ = logits.shape\n",
    "        logits = logits.view(B * T, -1)\n",
    "        y = y.view(B * T, -1).squeeze()\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5880ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician aside welcome; I would now now now your Hail tis that that so being prerogative storm murderer, therefore put EDWARD therefore good down harm \n",
      " You lie love absolute \n",
      " \n",
      " office \n",
      " \n",
      " \n",
      " \n",
      " Give alive! \n",
      " Let scale spared from how and and and at blind the accept for for member,, how advise thyself own the the the the, which Their oracle content why else not of sake has it it look mock'd to die gold whence, \n",
      " AUTOLYCUS. \n",
      " PETRUCHIO on on \n",
      " midst son \n",
      " Harp censure\n"
     ]
    }
   ],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate_reversed(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
