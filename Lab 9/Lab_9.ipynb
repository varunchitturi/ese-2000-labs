{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [213, 12384, 11313, 10436, 12540, 12613, 10482, 7206]\n",
      "\n",
      "Decoded text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    decoded = \" \".join([itos[i] for i in int_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "sample_words = text[:36]\n",
    "print(\"Original text: {}\\n\".format(sample_words))\n",
    "print(\"Encoded text: {}\\n\".format(words_to_tokens(split_to_words(sample_words))))\n",
    "print(\"Decoded text: {}\\n\".format(tokens_to_words(words_to_tokens(split_to_words(sample_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [213, 12384, 11313, 10436, 12540, 12613, 10482, 7206, 12, 4012]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22463c10a95801e",
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c3e73672a0d716",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988621247,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "82c3e73672a0d716"
   },
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329672eb8116e436",
   "metadata": {
    "id": "329672eb8116e436"
   },
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f4e2e10b103e95",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988622421,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "31f4e2e10b103e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([4, 64])\n",
      "y_idx shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        #Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "        # single item \n",
    "        Y_item = self.text[idx + self.T]\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_set = TextDataset(train, T)\n",
    "test_set = TextDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 4\n",
    "train_loader = DataLoader(train_set, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67557/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67557/2854450157.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=0, keepdim=True)\n",
    "#     Z /= Z.std(dim=0, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "embeddings = torch.load(\"embeddings.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "# Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "## Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=14295\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "## Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Principal eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "## Co ocurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tx60HzRzvef",
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9621a80",
   "metadata": {},
   "source": [
    "## MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3gCca0eqy91t",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1720988709140,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "3gCca0eqy91t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E shape: torch.Size([256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'embeddings shape: torch.Size([14295, 256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([4, 256, 64])\n",
      "out shape: torch.Size([4, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n is the embedding dimension in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead attention layer, analogous to the one in the \n",
    "        AttentionLayer class. The main difference is that we need to make sure that the \n",
    "        matrix multiplications account for the new head dimenison.\n",
    "        Args:\n",
    "            X (torch.Tensor): The input sequence.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, T, n = X.shape  # X: (B, T, n)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        self.norm1(X.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        # The unsqueeze is used to add the head dimension to the matrices,\n",
    "        # because they are of shape (H, m, n), and we need to multiply them\n",
    "        # with X_expanded of shape (B, 1, n, T)\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "\n",
    "        # Transpose QX for multiplication\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B_matrix = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # Compute attention weights A per head\n",
    "        A = F.softmax(B_matrix, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute Z per head\n",
    "        Z = torch.matmul(VX, A)  # (B, H, m, T)\n",
    "\n",
    "        # Average over the heads\n",
    "        Z = Z.sum(dim=1)\n",
    "\n",
    "        # Continue with feed-forward network\n",
    "        Y_l = torch.matmul(self.W, Z)  # (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        Y_l = self.norm2(Y_l.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y_l)  # (B, n, T)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        X_l = self.dropout(X_l)\n",
    "\n",
    "        return X_l\n",
    "# Test\n",
    "E = next(iter(train_loader))[0]\n",
    "B, T = E.shape\n",
    "E = E.reshape(-1)\n",
    "X = embeddings[E]\n",
    "X = X.reshape(B, -1 ,T)\n",
    "display(f\"E shape: {E.shape}\")\n",
    "display(f\"embeddings shape: {embeddings.shape}\")\n",
    "print(f\"X_idx shape: {X.shape}\")\n",
    "model = MultiHeadLayer(m=3, n=256, H=2).to(device)\n",
    "out = model(X)\n",
    "print(f\"out shape: {out.shape}\") #(B,T,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05fb71",
   "metadata": {},
   "source": [
    "## LLM (todo rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "RYUNfNqx0TSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720988711543,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "RYUNfNqx0TSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3695616\n",
      "Output shape: torch.Size([4, 256, 64])\n",
      "logits:  [12066, 5139, 5139, 5139]\n",
      "text prediction:  tents hid hid hid\n"
     ]
    }
   ],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, L, m, H):\n",
    "        super(LLM, self).__init__()\n",
    "        self.num_blocks = L\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[MultiHeadLayer(m, n, H) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout = nn.Parameter(torch.empty(n, c))\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        token_emb = self.token_embedding[tokens]\n",
    "        #token_emb = tokens\n",
    "        pos_emb = self.position_embedding(torch.arange(tokens.shape[1], device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        # We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.decoder_layers(x)\n",
    "        # Average over the context size\n",
    "        y_hat = torch.matmul(x.mean(dim=-1), self.readout)\n",
    "        return y_hat\n",
    "\n",
    "E = next(iter(train_loader))[0]\n",
    "model = LLM(L=3, m=3, H=2).to(device)\n",
    "logits = model(E)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Output shape: {out.shape}\") #(B,T,n)\n",
    "#print(f\"Output sample: {out[0,0:1,:]}\")\n",
    "print(\"logits: \", (logits.argmax(dim=-1).tolist()))\n",
    "print(\"text prediction: \", tokens_to_words(logits.argmax(dim=-1).tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0174c",
   "metadata": {},
   "source": [
    "## Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df6bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = input_tokens.clone()\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(input_tokens)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n",
    "        generated_words = tokens_to_words(generated_sequence.reshape(-1).tolist())\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862ff72",
   "metadata": {},
   "source": [
    "Even with a good chunk of context, the generated text is gibberish at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e7ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician bewept divided can't rued festering age revels jump corrupting Patrician Shalt Peering general's Receive lessons brinish do't began prosperity aloof lighter devices valour viewing whipt\n"
     ]
    }
   ],
   "source": [
    "# Testing code generation method on \n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    generated_words = generate(model,initial, max_generate_tokens=25)\n",
    "    print(\"\\n===INPUT===\\n\")\n",
    "    print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "    print(\"\\n===GENERATED TEXT===\\n\")\n",
    "    print(\"\".join(generated_words[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea6300",
   "metadata": {},
   "source": [
    "## Train (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2b2f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([4, 64])\n",
      "Y.shape: torch.Size([4])\n",
      "logits.shape: torch.Size([4, 14295])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.5353, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E,Y = next(iter(train_loader))\n",
    "B, T = E.shape\n",
    "logits = model(E)\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "print(f\"Y.shape: {Y.shape}\")\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "# reshaped \n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3256464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.position_embedding.weight.data.numel()\n",
    "#model.token_embedding.weight.data.numel()\n",
    "model.decoder_layers[0].Q.data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3002b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 4.293888M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjavierporras\u001b[0m (\u001b[33mese-2000\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jporras/sourcecode/ese-2000-labs/Lab 9/wandb/run-20241021_071607-azqps7xc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ese-2000/lab-9-llm/runs/azqps7xc' target=\"_blank\">eager-glade-56</a></strong> to <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/azqps7xc' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/azqps7xc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 3\n",
    "H = 8\n",
    "#m = n//H\n",
    "m = 32\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "#num_epochs = 1\n",
    "\n",
    "model = LLM(L, m, H).to(device)\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"m\": m,\n",
    "        \"n\": n,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "568448d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss 4.913678296748095\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▄▄▂▄▅▂▃▂▃▆█▄▁▅▅▄▂▄▃▄▁▄▅▄▃▃▃▅▃▄▂▃▄▄▁▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_train_loss</td><td>4.91368</td></tr><tr><td>train_loss</td><td>2.57532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-blaze-53</strong> at: <a href='https://wandb.ai/ese-2000/lab-9-llm/runs/hczhebqg' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm/runs/hczhebqg</a><br/> View project at: <a href='https://wandb.ai/ese-2000/lab-9-llm' target=\"_blank\">https://wandb.ai/ese-2000/lab-9-llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241020_193549-hczhebqg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "train_loss_evolution = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for x, y in tqdm(train_loader):\n",
    "        logits = model(x)\n",
    "        B, _ = logits.shape\n",
    "        #loss = F.cross_entropy(logits.reshape(B * T, -1), y.reshape(B * T, -1).squeeze())\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)},commit=False)\n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n",
    "\n",
    "# Testing code generation\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))\n",
    "\n",
    "#finish run\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814f842",
   "metadata": {},
   "source": [
    "## Train (actual loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5de84fbe5d8ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1720993361314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ca5de84fbe5d8ec",
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 0.247296M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m     31\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if os.path.exists(\"./model.pt\"):\n",
    "#     model = torch.load(\"./model.pt\", map_location=device)\n",
    "#     print(\"Loaded existing model\")\n",
    "# else:\n",
    "#     L = 1\n",
    "#     H = 2\n",
    "#     m = n//H\n",
    "#     model = LLM(L, m, H).to(device)\n",
    "#     lr = 1e-4\n",
    "#     opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "#     #num_epochs = 20\n",
    "#     num_epochs = 1\n",
    "#     model.eval()\n",
    "#     num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "#     # wandb.config.update({\"lr\": lr, \n",
    "#     #                     \"num_blocks\": num_blocks, \n",
    "#     #                     \"num_heads_per_block\": num_heads_per_block,\n",
    "#     #                     \"context_size\": T,\n",
    "#     #                     \"num_epochs\": num_epochs,\n",
    "#     #                     \"model_summary\": str(model),\n",
    "#     #                     \"num_parameters\": num_parameters_str})\n",
    "#     print(\"Created new model with {}\".format(num_parameters_str))\n",
    "#     train_loss_evolution = []\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         train_loss = 0\n",
    "#         for t_idx, (x, y) in enumerate(train_loader):\n",
    "#             logits = model(x)\n",
    "#             batch_size, _, _ = logits.shape\n",
    "#             #loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "#             opt.zero_grad()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             train_loss += loss.item()\n",
    "#         train_loss_evolution.append(train_loss/len(train_loader))\n",
    "#         clear_output()\n",
    "#         print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "#         run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "#         wandb.config.update({\"num_epochs\": epoch+1})\n",
    "#         plt.plot(train_loss_evolution)\n",
    "#         plt.show()\n",
    "#     torch.save(model, \"./model.pt\")\n",
    "#     wandb.save('./model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  7.485606698046796\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for t_idx, (x, y) in enumerate(test_loader):\n",
    "        logits = model(x)\n",
    "        B, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[\u001b[38;5;241m132\u001b[39m:\u001b[38;5;241m132\u001b[39m\u001b[38;5;241m+\u001b[39mT]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate(model,initial, max_generate_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===INPUT===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c68a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fI8MSAifK33",
   "metadata": {
    "id": "6fI8MSAifK33"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91497c7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
