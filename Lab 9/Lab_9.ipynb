{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c56f9ac2b2ce56",
   "metadata": {
    "id": "4c56f9ac2b2ce56"
   },
   "source": [
    "# ESE-2000 Lab 6\n",
    "TO DO add intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff74f15f78e2e9",
   "metadata": {
    "id": "d8ff74f15f78e2e9"
   },
   "source": [
    "We download a ~1MB file containing the entirety of Shakespeare's work. This is the dataset we will train our language model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3314,
     "status": "ok",
     "timestamp": 1720988615612,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "initial_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from IPython.core.display_functions import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import wandb\n",
    "import re\n",
    "device = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps:0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\" \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c0a7f",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94606dfb",
   "metadata": {},
   "source": [
    "###  Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d8918bcd4f0a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1720988616138,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "58d8918bcd4f0a06",
    "outputId": "8a342c6e-28ce-4b94-a902-a92cfba530bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Sample Shakespeare----\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "\n",
    "print(\"----Sample Shakespeare----\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5244308b67761a",
   "metadata": {
    "id": "d5244308b67761a"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55df526e53534b",
   "metadata": {
    "id": "ae55df526e53534b"
   },
   "source": [
    "Tokenization converts raw sub-sequences of text (substrings) to sequences of integers. For example, `\"ll.\" -> 208`. We will be developing a character level language model, so we will be converting each individual word into an integer. For example, `\"Hello\" -> 48`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aba7e30bedd5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1720988617702,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "4aba7e30bedd5646",
    "outputId": "4ced07b4-2792-4e7d-c5cb-f351a5a7b73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 292072\n",
      "Number of distinct words in text: 14295\n"
     ]
    }
   ],
   "source": [
    "def split_to_words(text):\n",
    "    return re.findall(r\"\\w+(?:'\\w+)?|[.,!?;:\\\"()\\[\\]{}<>\\\\/\\-—–…]|\\n\", text)\n",
    "\n",
    "vocab = list(set(split_to_words(text)))\n",
    "c = len(vocab)\n",
    "print(\"Number of words: {}\".format(len(split_to_words(text))))\n",
    "print(\"Number of distinct words in text: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cacb2e9ced76d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1720988618989,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "9cacb2e9ced76d25",
    "outputId": "bff09c8f-6079-4f04-e5bf-d4903f546256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: First Citizen:\n",
      "Before we proceed any\n",
      "\n",
      "Encoded text: [213, 12384, 11313, 10436, 12540, 12613, 10482, 7206]\n",
      "\n",
      "Decoded text: First Citizen: \n",
      " Before we proceed any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use index to map words to integer\n",
    "stoi = {word:i for i, word in enumerate(vocab)}\n",
    "itos = {i:word for i, word in enumerate(vocab)}\n",
    "def words_to_tokens(words):\n",
    "    return [stoi[w] for w in words]\n",
    "\n",
    "def tokens_to_words(int_list):\n",
    "    decoded = \" \".join([itos[i] for i in int_list])\n",
    "    return re.sub(r'\\s+([.,!?;:\"(){}\\[\\]<>\\\\/\\-—–…])', r'\\1', decoded)\n",
    "\n",
    "sample_words = text[:36]\n",
    "print(\"Original text: {}\\n\".format(sample_words))\n",
    "print(\"Encoded text: {}\\n\".format(words_to_tokens(split_to_words(sample_words))))\n",
    "print(\"Decoded text: {}\\n\".format(tokens_to_words(words_to_tokens(split_to_words(sample_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d146ef59a76b0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1720988619915,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "1d146ef59a76b0ca",
    "outputId": "163b4b47-a35f-455c-bb96-be8961351615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text sample: [213, 12384, 11313, 10436, 12540, 12613, 10482, 7206, 12, 4012]\n",
      "First Citizen: \n",
      " Before we proceed any further,\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = words_to_tokens(split_to_words(text))\n",
    "print(\"Encoded text sample: {}\".format(tokenized_text[:10]))\n",
    "print(tokens_to_words(tokenized_text[:10]))\n",
    "tokenized_text = torch.tensor(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22463c10a95801e",
   "metadata": {
    "id": "a22463c10a95801e"
   },
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c3e73672a0d716",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988621247,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "82c3e73672a0d716"
   },
   "outputs": [],
   "source": [
    "T = 64 # context size\n",
    "split_factor = 0.9\n",
    "split_index = int(split_factor * len(tokenized_text))\n",
    "train = tokenized_text[:split_index].to(device)\n",
    "test = tokenized_text[split_index:].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329672eb8116e436",
   "metadata": {
    "id": "329672eb8116e436"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31f4e2e10b103e95",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1720988622421,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "31f4e2e10b103e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([64, 64])\n",
      "y_idx shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, T):\n",
    "        self.text = text\n",
    "        self.T = T\n",
    "        assert self.T < len(text), \"context_size (T) must be less than len(text)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.T\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_item = self.text[idx:idx + self.T]\n",
    "        #Y_item = self.text[idx + 1:idx + self.T + 1]\n",
    "        # single item \n",
    "        Y_item = self.text[idx + self.T]\n",
    "        return X_item, Y_item\n",
    "\n",
    "train_dataset = TextDataset(train, T)\n",
    "test_dataset = TextDataset(test, T)\n",
    "\n",
    "\n",
    "# Example of a batch\n",
    "B = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "E, y_idx = next(iter(train_loader))\n",
    "print(f\"X_idx shape: {E.shape}\")\n",
    "print(f\"y_idx shape: {y_idx.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a4f4edabab2a2",
   "metadata": {
    "id": "659a4f4edabab2a2"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbafd52bae8f505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30529,
     "status": "ok",
     "timestamp": 1720988654405,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ccbafd52bae8f505",
    "outputId": "259b60c1-2103-469c-dc26-0f2889185a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67557/504936921.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  C = torch.load(\"C.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "#TODO commented bc its slow\n",
    "# Create co-occurrence matrix\n",
    "# The co-occurrence matrix C is a c x c (c is our vocab size) symmetric matrix where C_ij is how many times the ith word appears within W words away from the jth word.\n",
    "# W = 10\n",
    "# C = torch.stack([torch.zeros(len(vocab)) for _ in range(len(vocab))])\n",
    "# for t_idx in trange(len(tokenized_text)):\n",
    "#     left_bound = max(t_idx-W//2,0)\n",
    "#     right_bound = min(t_idx+W//2+1,len(tokenized_text))\n",
    "#     context_words = tokenized_text[left_bound : right_bound]\n",
    "#     for u_idx in range(left_bound, right_bound):\n",
    "#         t = tokenized_text[t_idx]\n",
    "#         u = tokenized_text[u_idx]\n",
    "#         C[t, u] += 1.0\n",
    "# C = C.to(device)\n",
    "# # X should be a symmetric matrix\n",
    "# torch.isclose(C, C.T, atol=1e-3).all()\n",
    "\n",
    "# # Save C so that we dont have to compute it again\n",
    "#torch.save(C, \"C.pt\")\n",
    "\n",
    "# Load C from storage\n",
    "C = torch.load(\"C.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a788300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of C in GB: numel times 4 bytes per float / 1e9 which is GB\n",
    "C.numel() * 4 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d98a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(C, C.T, atol=1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0fac0",
   "metadata": {},
   "source": [
    "## PCA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "582e9c67a87949a4",
   "metadata": {
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1720988658073,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "582e9c67a87949a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67557/2854450157.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(\"embeddings.pt\").to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch has a bug on mps devices so this won't work on MacBooks\n",
    "n = 256\n",
    "# with torch.no_grad():\n",
    "#     Z = C - C.mean(dim=0, keepdim=True)\n",
    "#     Z /= Z.std(dim=0, keepdim=True)\n",
    "#     cov = (Z @ Z.T)/(Z.shape[0] - 1)\n",
    "#     L, Q = torch.linalg.eigh(cov)\n",
    "#     principal_eigv = Q[:, -n:].T\n",
    "\n",
    "#     # PCA embeddings for training\n",
    "#     embeddings = Z @ principal_eigv.T # (c, n)\n",
    "#     # Full embeddings if we need them to visualize\n",
    "#     # In vector form would be Q.T @ x_n\n",
    "#     full_embeddings = Z @ Q\n",
    "\n",
    "# torch.save(embeddings, \"embeddings.pt\")\n",
    "# Load embeddings\n",
    "embeddings = torch.load(\"embeddings.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b0eec",
   "metadata": {},
   "source": [
    "# Visualize embeddings\n",
    "Decide if this section goes into the notebook or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8765b",
   "metadata": {},
   "source": [
    "## Average coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=14295\n",
    "# #average_coefficients = full_embeddings.mean(axis=0)\n",
    "# sample_embeddings = full_embeddings[torch.randint(0,full_embeddings.shape[0],(1000,))]\n",
    "# # Compute the expectation of the absolute value of the norm of each component.\n",
    "# average_coefficients = sample_embeddings.norm(p=2,dim=0).cpu().numpy()[::-1]\n",
    "# data = average_coefficients[:K]\n",
    "\n",
    "# # Reverse the tensor:\n",
    "# data = data\n",
    "\n",
    "# # Normalize by sum?\n",
    "# #data = data / data.sum()\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(f\"Average Coefficients (k={K})\")\n",
    "# fig= plt.plot(range(K), data,marker='.',linestyle='')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5eff5",
   "metadata": {},
   "source": [
    "## Principal eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K=64\n",
    "# L_plot = L[-K:]/L.sum()\n",
    "# L_plot,_ = L_plot.sort(descending=True)\n",
    "# L_plot = L_plot.cpu().numpy()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Principal eigenvalues (k=64)\")\n",
    "# markerline, stemlines, baseline = plt.stem(range(K), L_plot, linefmt='b-', markerfmt='o', basefmt='k-')\n",
    "# plt.setp(markerline, marker='o', fillstyle='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604f7c0",
   "metadata": {},
   "source": [
    "## Co ocurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top 10 words\n",
    "# top_10_words = C.sum(axis=0).sort(descending=True).indices[:10]\n",
    "# top_10_words = [vocab[i] for i in top_10_words]\n",
    "# print(top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70ee2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Remove all the punctations and stop words from the matrix for visualization\n",
    "# X_viz = C.clone()\n",
    "# words_to_remove = [\",\", \":\", \".\", \"the\", \"I\", \"to\", \"and\", \";\", \"of\", \"you\", \"my\", \"a\", \"?\", \"!\", \"in\", \"that\", \"And\", \"not\", \"-\", \"is\", \"me\", \"be\", \"it\", \"with\", \"your\", \"for\", \"his\", \"have\", \"this\", \"thou\", \"as\", \"thy\", \"will\", \"so\", \"but\", \"The\", \"To\", \"all\", \"her\", \"thee\", \"by\", \"are\", \"our\", \"do\", \"we\"]\n",
    "# vocab_to_remove_indices = set(words_to_tokens(words_to_remove))\n",
    "# idx_to_viz = [i for i, word in enumerate(vocab) if word not in vocab_to_remove_indices]\n",
    "# X_viz = X_viz[idx_to_viz, :][:, idx_to_viz]\n",
    "\n",
    "# # top 20 words not including stop words\n",
    "# top_100_words = C.sum(axis=0).sort(descending=True).indices[:100].cpu().numpy()\n",
    "# top_100_nostop = [word for word in top_100_words if word not in vocab_to_remove_indices]\n",
    "# display(f\"Top 100 words, excluding punctation and most common stop words: {tokens_to_words(top_100_nostop)}\")\n",
    "\n",
    "# # Create a custom colormap\n",
    "# cmap = plt.cm.get_cmap('viridis').copy()\n",
    "# cmap.set_over('green')\n",
    "\n",
    "# # Plot the image with the custom colormap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(X_viz.cpu().numpy(), cmap=cmap, vmax=3)\n",
    "\n",
    "# # Add colorbar with custom settings\n",
    "# cbar = plt.colorbar(extend='max')\n",
    "# cbar.set_label('Value')\n",
    "\n",
    "# plt.title('Co-occurrence Matrix')\n",
    "# plt.show()\n",
    "# # Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tx60HzRzvef",
   "metadata": {
    "id": "7tx60HzRzvef"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9621a80",
   "metadata": {},
   "source": [
    "## MultiHeadLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3gCca0eqy91t",
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1720988709140,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "3gCca0eqy91t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E shape: torch.Size([256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'embeddings shape: torch.Size([14295, 256])'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_idx shape: torch.Size([4, 256, 64])\n",
      "out shape: torch.Size([4, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of the multihead attention layer.\n",
    "    The difference between AttentionLayer and this class is,\n",
    "    now Q,K,V are matrices of shape (H, m, n), and the attention matrix B is of shape (H, T, T)\n",
    "    (one attention feature per head)\n",
    "    Args:\n",
    "        m (int): The dimension of the Q and K matrices.\n",
    "        n (int): The number of features, n is the embedding dimension in our case.\n",
    "        k (int): The dimension of the W matrix.\n",
    "        H (int): The number of heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, H):\n",
    "        super(MultiHeadLayer, self).__init__()\n",
    "        self.m = m\n",
    "        self.H = H\n",
    "\n",
    "        self.Q = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.K = nn.Parameter(torch.empty(H, m, n))\n",
    "        self.V = nn.Parameter(torch.empty(H, m, n))\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(n, m))\n",
    "        \n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(n)\n",
    "        self.norm2 = nn.LayerNorm(n)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the values of the learnable parameter matrices.\n",
    "        Kaiming uniform is just a type of random initialization, you don't need to \n",
    "        worry about it. It is a good default initialization for linear layers.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.Q, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.K, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.V, a=math.sqrt(5))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        The forward pass of the multihead attention layer, analogous to the one in the \n",
    "        AttentionLayer class. The main difference is that we need to make sure that the \n",
    "        matrix multiplications account for the new head dimenison.\n",
    "        Args:\n",
    "            X (torch.Tensor): The input sequence.\n",
    "        Returns:\n",
    "            X_l (torch.Tensor): The output of the multihead attention layer.\n",
    "        \"\"\"\n",
    "        B, T, n = X.shape  # X: (B, T, n)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        self.norm1(X.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "\n",
    "        # Expand X to include the head dimension\n",
    "        X_expanded = X.unsqueeze(1)  # (B, 1, n, T)\n",
    "\n",
    "        # Compute QX, KX, VX for each head\n",
    "        # The unsqueeze is used to add the head dimension to the matrices,\n",
    "        # because they are of shape (H, m, n), and we need to multiply them\n",
    "        # with X_expanded of shape (B, 1, n, T)\n",
    "        QX = torch.matmul(self.Q.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        KX = torch.matmul(self.K.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "        VX = torch.matmul(self.V.unsqueeze(0), X_expanded)  # (B, H, m, T)\n",
    "\n",
    "        # Transpose QX for multiplication\n",
    "        \n",
    "        QX_t = QX.transpose(-2, -1)  # (B, H, T, m)\n",
    "\n",
    "        # Compute attention scores B per head\n",
    "        B_matrix = torch.matmul(QX_t, KX)  # (B, H, T, T)\n",
    "\n",
    "        # Compute attention weights A per head\n",
    "        A = F.softmax(B_matrix, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute Z per head\n",
    "        Z = torch.matmul(VX, A)  # (B, H, m, T)\n",
    "\n",
    "        # Average over the heads\n",
    "        Z = Z.sum(dim=1)\n",
    "\n",
    "        # Continue with feed-forward network\n",
    "        Y_l = torch.matmul(self.W, Z)  # (B, n, T)\n",
    "        \n",
    "        # Normalize embedding dim. \n",
    "        # We permute because nn.LayerNorm always normalizes the last dimension.\n",
    "        Y_l = self.norm2(Y_l.permute(0,2,1)).permute(0,2,1)\n",
    "        \n",
    "        X_l = X + self.nonlinearity(Y_l)  # (B, n, T)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        X_l = self.dropout(X_l)\n",
    "\n",
    "        return X_l\n",
    "# Test\n",
    "E = next(iter(train_loader))[0]\n",
    "B, T = E.shape\n",
    "E = E.reshape(-1)\n",
    "X = embeddings[E]\n",
    "X = X.reshape(B, -1 ,T)\n",
    "display(f\"E shape: {E.shape}\")\n",
    "display(f\"embeddings shape: {embeddings.shape}\")\n",
    "print(f\"X_idx shape: {X.shape}\")\n",
    "model = MultiHeadLayer(m=3, n=256, H=2).to(device)\n",
    "out = model(X)\n",
    "print(f\"out shape: {out.shape}\") #(B,T,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05fb71",
   "metadata": {},
   "source": [
    "## LLM (todo rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "RYUNfNqx0TSw",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1720988711543,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "RYUNfNqx0TSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3695616\n",
      "Output shape: torch.Size([4, 256, 64])\n",
      "logits:  [467, 5157, 467, 467]\n",
      "text prediction:  Autolycus gadding Autolycus Autolycus\n"
     ]
    }
   ],
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, L, m, H):\n",
    "        super(LLM, self).__init__()\n",
    "        self.num_blocks = L\n",
    "        self.position_embedding = nn.Embedding(T, n) #TO DO replace by actual positional embeddings?\n",
    "        self.token_embedding = embeddings\n",
    "        self.decoder_layers = nn.Sequential(*[MultiHeadLayer(m, n, H) for _ in range(L)])\n",
    "        self.norm = nn.LayerNorm(n)\n",
    "        self.readout = nn.Parameter(torch.empty(n, c))\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.readout, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        token_emb = self.token_embedding[tokens]\n",
    "        #token_emb = tokens\n",
    "        pos_emb = self.position_embedding(torch.arange(tokens.shape[1], device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        # We permute to get shape of batch (B) x embed_dim (n) x context_size (T): (B, n, T)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.decoder_layers(x)\n",
    "        # Average over the context size\n",
    "        y_hat = torch.matmul(x.mean(dim=-1), self.readout)\n",
    "        return y_hat\n",
    "\n",
    "E = next(iter(train_loader))[0]\n",
    "model = LLM(L=3, m=3, H=2).to(device)\n",
    "logits = model(E)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Output shape: {out.shape}\") #(B,T,n)\n",
    "#print(f\"Output sample: {out[0,0:1,:]}\")\n",
    "print(\"logits: \", (logits.argmax(dim=-1).tolist()))\n",
    "print(\"text prediction: \", tokens_to_words(logits.argmax(dim=-1).tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "720450d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0174c",
   "metadata": {},
   "source": [
    "## Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df6bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_tokens, max_generate_tokens=500):\n",
    "    \"\"\"\n",
    "    Generate text from a model given an initial input token sequence.\n",
    "    Args:\n",
    "        model (nn.Module): The model to use for generation.\n",
    "        input_tokens (torch.Tensor): The initial input token sequence.\n",
    "        max_generate_tokens (int): The maximum number of tokens to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated token sequence.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        generated_sequence = input_tokens.clone()\n",
    "        for _ in range(max_generate_tokens):\n",
    "            logits = model(input_tokens)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_sequence = torch.cat([generated_sequence, next_token], dim=1)\n",
    "        generated_words = tokens_to_words(generated_sequence.reshape(-1).tolist())\n",
    "        generation_string = \"\".join(generated_words)\n",
    "        return generation_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862ff72",
   "metadata": {},
   "source": [
    "Even with a good chunk of context, the generated text is gibberish at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e7ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===INPUT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician\n",
      "\n",
      "===GENERATED TEXT===\n",
      "\n",
      "mountains are for winds, \n",
      " That shake not, though they blow perpetually. \n",
      " \n",
      " BAPTISTA: \n",
      " How now, my friend! why dost thou look so pale? \n",
      " \n",
      " HORTENSIO: \n",
      " For fear, I promise you, if I look pale. \n",
      " \n",
      " BAPTISTA: \n",
      " What, will my daughter prove a good musician bewept divided can't rued festering age revels jump corrupting Patrician Shalt Peering general's Receive lessons brinish do't began prosperity aloof lighter devices valour viewing whipt\n"
     ]
    }
   ],
   "source": [
    "# Testing code generation method on \n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    generated_words = generate(model,initial, max_generate_tokens=25)\n",
    "    print(\"\\n===INPUT===\\n\")\n",
    "    print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "    print(\"\\n===GENERATED TEXT===\\n\")\n",
    "    print(\"\".join(generated_words[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea6300",
   "metadata": {},
   "source": [
    "## Train (dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2b2f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.shape: torch.Size([4, 64])\n",
      "Y.shape: torch.Size([4])\n",
      "logits.shape: torch.Size([4, 14295])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.5353, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E,Y = next(iter(train_loader))\n",
    "B, T = E.shape\n",
    "logits = model(E)\n",
    "print(f\"E.shape: {E.shape}\")\n",
    "print(f\"Y.shape: {Y.shape}\")\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "# reshaped \n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3256464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.position_embedding.weight.data.numel()\n",
    "#model.token_embedding.weight.data.numel()\n",
    "model.decoder_layers[0].Q.data.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3002b0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 6.14016M parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wu1oedj2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 6\n",
    "H = 8\n",
    "#m = n//H\n",
    "m = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "#num_epochs = 1\n",
    "\n",
    "B = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False)\n",
    "\n",
    "model = LLM(L, m, H).to(device)\n",
    "num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "print(\"Created new model with {}\".format(num_parameters_str))\n",
    "run = wandb.init(\n",
    "    project=\"lab-9-llm\",\n",
    "    entity=\"ese-2000\",\n",
    "    config={\n",
    "        \"architecture\": \"Transformer\",\n",
    "        \"dataset\": \"TinyShakespeare\",\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_blocks\": L,\n",
    "        \"num_heads_per_block\": H,\n",
    "        \"context_size\": T,\n",
    "        \"m\": m,\n",
    "        \"n\": n,\n",
    "        \"model_summary\": str(model),\n",
    "        \"num_parameters\": num_parameters_str\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568448d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss 6.023317226068187\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 259/1027 [00:08<00:24, 31.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 17\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m     19\u001b[0m train_loss_evolution\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "train_loss_evolution = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for x, y in tqdm(train_loader):\n",
    "        logits = model(x)\n",
    "        B, _ = logits.shape\n",
    "        #loss = F.cross_entropy(logits.reshape(B * T, -1), y.reshape(B * T, -1).squeeze())\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "    train_loss_evolution.append(train_loss/len(train_loader))\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "    run.log({\"epoch_train_loss\": train_loss/len(train_loader)},commit=False)\n",
    "    wandb.config.update({\"num_epochs\": epoch+1})\n",
    "    plt.plot(train_loss_evolution)\n",
    "    plt.show()\n",
    "# torch.save(model, \"./model.pt\")\n",
    "# wandb.save('./model.pt')\n",
    "\n",
    "# Testing code generation\n",
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))\n",
    "\n",
    "#finish run\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814f842",
   "metadata": {},
   "source": [
    "## Train (actual loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5de84fbe5d8ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1720993361314,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "ca5de84fbe5d8ec",
    "outputId": "3aeaaffc-cb81-4b68-b795-1f6263c58a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new model with 0.247296M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [56,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [33,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [38,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [47,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [52,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [27,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [32,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [33,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [34,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [35,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [36,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [37,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [38,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [39,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [40,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [41,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [42,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [43,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [44,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [45,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [46,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [47,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [48,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [49,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [50,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [51,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [53,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [54,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [55,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [56,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [57,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [58,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [59,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [60,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [61,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [62,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [23,0,0], thread: [63,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/opt/conda/conda-bld/pytorch_1728945388038/work/aten/src/ATen/native/cuda/NLLLoss2d.cu:106: nll_loss2d_forward_kernel: block: [20,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m     31\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab_5a_trf/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if os.path.exists(\"./model.pt\"):\n",
    "#     model = torch.load(\"./model.pt\", map_location=device)\n",
    "#     print(\"Loaded existing model\")\n",
    "# else:\n",
    "#     L = 1\n",
    "#     H = 2\n",
    "#     m = n//H\n",
    "#     model = LLM(L, m, H).to(device)\n",
    "#     lr = 1e-4\n",
    "#     opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "#     #num_epochs = 20\n",
    "#     num_epochs = 1\n",
    "#     model.eval()\n",
    "#     num_parameters_str = str(sum(p.numel() for p in model.parameters())/1e6,) + 'M parameters'\n",
    "#     # wandb.config.update({\"lr\": lr, \n",
    "#     #                     \"num_blocks\": num_blocks, \n",
    "#     #                     \"num_heads_per_block\": num_heads_per_block,\n",
    "#     #                     \"context_size\": T,\n",
    "#     #                     \"num_epochs\": num_epochs,\n",
    "#     #                     \"model_summary\": str(model),\n",
    "#     #                     \"num_parameters\": num_parameters_str})\n",
    "#     print(\"Created new model with {}\".format(num_parameters_str))\n",
    "#     train_loss_evolution = []\n",
    "#     for epoch in trange(num_epochs):\n",
    "#         train_loss = 0\n",
    "#         for t_idx, (x, y) in enumerate(train_loader):\n",
    "#             logits = model(x)\n",
    "#             batch_size, _, _ = logits.shape\n",
    "#             #loss = F.cross_entropy(logits.view(batch_size * T, -1), y.view(batch_size * T, -1).squeeze())\n",
    "#             loss = F.cross_entropy(logits, y)\n",
    "#             opt.zero_grad()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             train_loss += loss.item()\n",
    "#         train_loss_evolution.append(train_loss/len(train_loader))\n",
    "#         clear_output()\n",
    "#         print(f\"Epoch {epoch+1}, Loss {train_loss/len(train_loader)}\")\n",
    "#         run.log({\"epoch_train_loss\": train_loss/len(train_loader)}) \n",
    "#         wandb.config.update({\"num_epochs\": epoch+1})\n",
    "#         plt.plot(train_loss_evolution)\n",
    "#         plt.show()\n",
    "#     torch.save(model, \"./model.pt\")\n",
    "#     wandb.save('./model.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e45c455cd8bcd29",
   "metadata": {
    "id": "4e45c455cd8bcd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  7.485606698046796\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for t_idx, (x, y) in enumerate(test_loader):\n",
    "        logits = model(x)\n",
    "        B, _ = logits.shape\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(\"Test loss: \", test_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Jhoh1INhBePM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4770,
     "status": "ok",
     "timestamp": 1720997577756,
     "user": {
      "displayName": "Varun Chitturi",
      "userId": "14334239921966396118"
     },
     "user_tz": 420
    },
    "id": "Jhoh1INhBePM",
    "outputId": "88debee9-8b39-46ff-b3e0-d55c5f1fe11f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[\u001b[38;5;241m132\u001b[39m:\u001b[38;5;241m132\u001b[39m\u001b[38;5;241m+\u001b[39mT]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate(model,initial, max_generate_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===INPUT===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "initial = test[132:132+T].unsqueeze(0)\n",
    "generated_text = generate(model,initial, max_generate_tokens=100)\n",
    "print(\"\\n===INPUT===\\n\")\n",
    "print(tokens_to_words(initial.reshape(-1).tolist()))\n",
    "print(\"\\n===GENERATED TEXT===\\n\")\n",
    "print(\"\".join(generated_text[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c68a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fI8MSAifK33",
   "metadata": {
    "id": "6fI8MSAifK33"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91497c7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
